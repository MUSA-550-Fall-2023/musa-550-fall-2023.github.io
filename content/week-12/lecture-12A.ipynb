{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 12A: Predictive modeling with scikit-learn\n",
    "\n",
    "- Nov 19, 2023\n",
    "- Section 401\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last week\n",
    "\n",
    "- A crash course on clustering \n",
    "    - non-spatial: k-means\n",
    "    - spatial: DBSCAN\n",
    "- An introduction to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: clustering is an example of *unsupervised learning*\n",
    "\n",
    "- Clustering tries to detect previously unknown structure in your input dataset\n",
    "- Only requires the data set as input, no associated labels or prediction targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today: an example of *supervised learning*\n",
    "\n",
    "\n",
    "- **Key distinction**: requires a training data set as well as the desired solutions (called *labels*) as inputs to the algorithm\n",
    "- Two main types:\n",
    "    - **Classification**: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. \n",
    "    - **Regression**: predicting a continuous variable from a training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/classification-vs-regression.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples:\n",
    "\n",
    "- **Classification**: a spam filter to classify new emails as spam / not spam based on past examples\n",
    "- **Regression**: predicting housing prices based on property characteristics\n",
    "\n",
    "**Today, we'll walk through an end-to-end regression example to predict Philadelphia's housing prices** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model-based learning\n",
    "\n",
    "- Select a model that can represent the data \n",
    "- Use the training data to identify the best model parameters (by minimizing a *cost function*)\n",
    "- Make predictions on new data — and hope that your model *generalizes* well to new data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning is really just an **optimization problem**\n",
    "\n",
    "Given your training set of data, which model parameters best represent the observed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Choose a model\n",
    "\n",
    "<img src=\"imgs/model-1.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. The model has an associated cost function\n",
    "\n",
    "The *cost function* measures the difference between the model's predictions and the observed data\n",
    "\n",
    "<img src=\"imgs/model-2.png\" width=450></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. \"Learn\" the best model parameters\n",
    "\n",
    "In scikit-learn, you will call the `fit()` method on your algorithm.\n",
    "\n",
    "<img src=\"imgs/model-3.png\" width=600></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: the steps involved\n",
    "\n",
    "1. Wrangle and understand data. \n",
    "1. Select a model that would be best for your dataset.\n",
    "1. Train the model on the training data — the learning algorithm searches for the best model parameters\n",
    "1. Apply the model to new data to make predictions.\n",
    "\n",
    "**Key goal: how we can do this in a way to ensure the model is as generalizable as possible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What could go wrong?\n",
    "\n",
    "\n",
    "<img src=https://media.giphy.com/media/nDxjSaLT9gFig/giphy.gif width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mistake #1: \"bad data\"\n",
    "\n",
    "Or: \"garbage in, garbage out\" \n",
    "\n",
    "**Common issues:**\n",
    "\n",
    "- Not enough training data\n",
    "- Training data isn't representative of the unseen data that you want to make predictions for\n",
    "- Poor quality data — errors, missing data, outliers\n",
    "- Poor features in the training set\n",
    "    - You'll often hear the phrase *feature engineering* to describe the process of improving your input dataset: \n",
    "    - Involves: feature selection, feature extraction, creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mistake #2: \"bad algorithm\"\n",
    "\n",
    "- Overfitting the training data (more on this shortly)\n",
    "    - model performs well, too well in fact, on the training set, and does not generalize well\n",
    "    - model is **too complex** for your training set\n",
    "- Underfitting the training data\n",
    "    - model is **not complex enough**\n",
    "    - predictions will be inacurrate, but adding more model parameters (making it more complex) will help improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization: keeping it simple\n",
    "\n",
    "- We can *regularize* our model to prevent the model from getting too complex and avoid overfitting\n",
    "- Adds a penalty to the cost function that prevents parameters from getting too large\n",
    "- Can effectively think of regularization as forcing some model parameters to be close, not quite, zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/regularization.png\" width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Key question: How do we know if a model will perform well on new data?\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3ohhwjIjLLWBh4EQRW/giphy.gif\" width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note:** Crossing your fingers and hoping for the best is not the recommended strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option #1: a train/test split\n",
    "\n",
    "- Split your data into two sets: the *training set* and the *test set*\n",
    "- Train on the training set and test on the test set!\n",
    "- The accuracy on the test set provides a measure of how well your model generalizes\n",
    "\n",
    "Common to use 80% of data for your training set and 20% for your test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Still run the risk that you've selected the best model parameters for this **specific** training/test combination\n",
    "- For example: \n",
    "    - Does the model work best on a 80/20 split, a 60/40 split, 70/30? How to decide the test/train split?\n",
    "    - If you are using regularization, did your regularization strength parameter work only on this specific training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option #2: *k*-fold cross-validation\n",
    "\n",
    "1. Break the data into a training set and test set\n",
    "1. Split the training set into *k* subsets (or folds), holding out one subset as the test set\n",
    "1. Run the learning algorithm on each combination of subsets, using the average of all of the runs to find the best fitting model parameters\n",
    "\n",
    "::: {.callout-tip}\n",
    "For more information, see the [scikit-learn docs](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/crossval.png\" width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try out a simple example: does money make people happier?\n",
    "\n",
    "We'll load data compiled from two data sources:\n",
    "- The *Better Life Index* from the [OECD's website](https://stats.oecd.org/index.aspx?DataSetCode=BLI)\n",
    "- GDP per capita from the [IMF's website](https://www.imf.org/external/pubs/ft/weo/2016/01/weodata/weorept.aspx?pr.x=32&pr.y=8&sy=2015&ey=2015&scsm=1&ssd=1&sort=country&ds=.&br=1&c=512,668,914,672,612,946,614,137,311,962,213,674,911,676,193,548,122,556,912,678,313,181,419,867,513,682,316,684,913,273,124,868,339,921,638,948,514,943,218,686,963,688,616,518,223,728,516,558,918,138,748,196,618,278,624,692,522,694,622,142,156,449,626,564,628,565,228,283,924,853,233,288,632,293,636,566,634,964,238,182,662,453,960,968,423,922,935,714,128,862,611,135,321,716,243,456,248,722,469,942,253,718,642,724,643,576,939,936,644,961,819,813,172,199,132,733,646,184,648,524,915,361,134,362,652,364,174,732,328,366,258,734,656,144,654,146,336,463,263,528,268,923,532,738,944,578,176,537,534,742,536,866,429,369,433,744,178,186,436,925,136,869,343,746,158,926,439,466,916,112,664,111,826,298,542,927,967,846,443,299,917,582,544,474,941,754,446,698,666&s=NGDPDPC&grp=0&a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/gdp_vs_satisfaction.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Make a quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.hvplot.scatter(\n",
    "    x=\"gdp_per_capita\",\n",
    "    y=\"life_satisfaction\",\n",
    "    hover_cols=[\"Country\"],\n",
    "    ylim=(4, 9),\n",
    "    xlim=(1e3, 1.1e5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### There's a roughly linear trend here...let's start there\n",
    "\n",
    "<img src=\"imgs/life-happy.png\" width=600></img>\n",
    "\n",
    "A simple model with only two parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model object from scikit-learn.\n",
    "\n",
    "This is not *really* machine learning — it simply finds the [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Our input features (in this case we only have 1)\n",
    "X = data['gdp_per_capita'].values\n",
    "X = X[:, np.newaxis]\n",
    "\n",
    "# The labels (values we are trying to predict)\n",
    "y = data['life_satisfaction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::: {.callout-caution}\n",
    "\n",
    "Watch out! Scikit-learn expects the features to be a 2D array with shape: `(number of observations, number of features)`. \n",
    "\n",
    "In this case, we are explicitly adding a second axis with the `np.newaxis` variable.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, fit the model using the `model.fit(X, y)` syntax. \n",
    "\n",
    "This will \"train\" our model, using an optimization algorithm to identify the bestfit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "intercept = model.intercept_\n",
    "slope = model.coef_[0]\n",
    "\n",
    "print(f\"bestfit intercept = {intercept:.2f}\")\n",
    "print(f\"bestfit slope = {slope:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's with the \"_\" at the end of variable names?\n",
    "\n",
    "These represent \"estimated\" properties of the model — this is how scikit learn signals to the user that these attributes depend on the `fit()` function being called beforehand.\n",
    "\n",
    "More info [here](https://scikit-learn.org/dev/developers/develop.html#estimated-attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note:** In this case, our model is the same as ordinary least squares, and no actually optimization is performed since an exact solution exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How good is the fit?\n",
    "\n",
    "- Each scikit learn model has a built-in `score()` function that provides a score to evaluate the fit by.\n",
    "- In the case of the linear model, the score is the R-squared coefficient of the fit\n",
    "\n",
    "**Note:** you must call the `fit()` function before calling the `score()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Rsq = model.score(X, y)\n",
    "Rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's plot the data and the predicted values\n",
    "\n",
    "Use the `predict()` function to predict new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The values we want to predict (ranging from our min to max GDP per capita)\n",
    "gdp_pred = np.linspace(1e3, 1.1e5, 100)\n",
    "\n",
    "# Sklearn needs the second axis!\n",
    "X_pred = gdp_pred[:, np.newaxis]\n",
    "\n",
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with plt.style.context(\"fivethirtyeight\"):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the predicted values\n",
    "    ax.plot(X_pred / 1e5, y_pred, label=\"Predicted values\", color=\"#666666\")\n",
    "\n",
    "    # Training data\n",
    "    ax.scatter(\n",
    "        data[\"gdp_per_capita\"] / 1e5,\n",
    "        data[\"life_satisfaction\"],\n",
    "        label=\"Training data\",\n",
    "        s=100,\n",
    "        zorder=10,\n",
    "        color=\"#f40000\",\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"GDP Per Capita ($\\\\times$ $10^5$)\")\n",
    "    ax.set_ylabel(\"Life Satisfaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Not bad....but what did we do wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=https://media.giphy.com/media/1Bh2zyW2yuBEalV0rO/giphy.gif width=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Problems!\n",
    "\n",
    "#### 1. We also fit and evaluated our model on the same training set!\n",
    "\n",
    "#### 2. We didn't scale our input data features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit learn provides a utility function to split our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# I'll use a 70/30% split\n",
    "train_set, test_set = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are new DataFrame objects, with lengths determined by the split percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"size of full dataset = \", len(data))\n",
    "print(\"size of training dataset = \", len(train_set))\n",
    "print(\"size of test dataset = \", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, make our feature and label arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = train_set['gdp_per_capita'].values\n",
    "X_train = X_train[:, np.newaxis]\n",
    "\n",
    "X_test = test_set['gdp_per_capita'].values\n",
    "X_test = X_test[:, np.newaxis]\n",
    "\n",
    "# Labels\n",
    "y_train = train_set['life_satisfaction'].values\n",
    "y_test = test_set['life_satisfaction'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale the GDP per capita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the training features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's fit on the *training set* and evaluate on the *test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Unsurprisingly, our fit gets worst when we test on unseen data**\n",
    "\n",
    "Our accuracy was artifically inflated the first time, since we trained and tested on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we do better? Let's do some feature engineering...\n",
    "\n",
    "We'll use scikit learn's [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to add new polynomial features from the GDP per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's try up to degree 3 polynomials ($x^3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we have two transformations to make:\n",
    "\n",
    "1. Scale our features\n",
    "1. Create the polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "X_train_scaled_poly = poly.fit_transform(scaler.fit_transform(X_train))\n",
    "\n",
    "# Test\n",
    "X_test_scaled_poly = poly.fit_transform(scaler.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.score(X_test_scaled_poly, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The accuracy improved!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pipelines: making multiple transformations *much* easier\n",
    "\n",
    "We can turn our preprocessing steps into a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) object using the [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), PolynomialFeatures(degree=3))\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Individual steps can be accessed via their names in a dict-like fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "pipe['standardscaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "pipe['polynomialfeatures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's apply this pipeline to our predicted GDP values for our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(pipe.fit_transform(X_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with plt.style.context(\"fivethirtyeight\"):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the predicted values\n",
    "    y_pred = model.predict(pipe.fit_transform(X_pred))\n",
    "    ax.plot(X_pred / 1e5 , y_pred, label=\"Predicted values\", color=\"#666666\")\n",
    "\n",
    "    # Training data\n",
    "    ax.scatter(\n",
    "        data[\"gdp_per_capita\"] / 1e5,\n",
    "        data[\"life_satisfaction\"],\n",
    "        label=\"Training data\",\n",
    "        s=100,\n",
    "        zorder=10,\n",
    "        color=\"#f40000\",\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"GDP Per Capita ($\\\\times$ $10^5$)\")\n",
    "    ax.set_ylabel(\"Life Satisfaction\");\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The additional polynomial features introduced some curvature and improved the fit!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How about large polynomial degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with plt.style.context(\"fivethirtyeight\"):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Original data set\n",
    "    ax.scatter(\n",
    "        data[\"gdp_per_capita\"] / 1e5,\n",
    "        data[\"life_satisfaction\"],\n",
    "        label=\"Training data\",\n",
    "        s=100,\n",
    "        zorder=10,\n",
    "        color=\"#666666\",\n",
    "    )\n",
    "    \n",
    "    # Plot the predicted values\n",
    "    for degree in [3, 5, 10]:\n",
    "        print(f\"degree = {degree}\")\n",
    "        \n",
    "        # Create out pipeline\n",
    "        p = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree))\n",
    "        \n",
    "        # Fit the model on the training set\n",
    "        model.fit(p.fit_transform(X_train), y_train)\n",
    "        \n",
    "        # Evaluate on the training set\n",
    "        training_score = model.score(p.fit_transform(X_train), y_train)\n",
    "        print(f\"Training Score = {training_score}\")\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_score = model.score(p.fit_transform(X_test), y_test)\n",
    "        print(f\"Test Score = {test_score}\")\n",
    "        \n",
    "        # Plot\n",
    "        y_pred = model.predict(p.fit_transform(X_pred))\n",
    "        ax.plot(X_pred / 1e5, y_pred, label=f\"Degree = {degree}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "    ax.legend(ncol=2, loc=0)\n",
    "    ax.set_ylim(4, 9)\n",
    "    ax.set_xlabel(\"GDP Per Capita ($\\\\times$ $10^5$)\")\n",
    "    ax.set_ylabel(\"Life Satisfaction\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting alert!\n",
    "\n",
    "As we increase the polynomial degree, two things happen:\n",
    "\n",
    "1. Training accuracy goes way up \n",
    "1. Test accuracy goes way down\n",
    "    \n",
    "This is the classic case of overfitting — our model does not generalize well at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization to the rescue?\n",
    "\n",
    "- The [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) adds regularization to the linear regression least squares model\n",
    "- Parameter *alpha* determines the level of regularization\n",
    "- Larger values of *alpha* mean stronger regularization — results in a \"simpler\" bestfit\n",
    "\n",
    "**Remember, regularization penalizes large parameter values and complex fits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "To be continued next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
