[
  {
    "objectID": "resource/file-paths.html",
    "href": "resource/file-paths.html",
    "title": "File paths and working directories",
    "section": "",
    "text": "Note\n\n\n\nBelow is a guide from one of our course‚Äôs previous TAs, Eugene Chong. It does a great job laying out the common issues associated with file paths and Python.\nA common challenge in collaborative data science work is dealing with file paths. A csv on your computer has a different file path than a csv on your colleague‚Äôs computer, and if you don‚Äôt plan ahead for that, your colleague won‚Äôt be able run your code on their computer without making (possibly many) changes.\nThis post contains a suggestion for how to organize your files and a glossary of terms. I‚Äôll update it as things come up. Please let us know any questions!"
  },
  {
    "objectID": "resource/file-paths.html#absolute-v.-relative-file-paths",
    "href": "resource/file-paths.html#absolute-v.-relative-file-paths",
    "title": "File paths and working directories",
    "section": "Absolute v. relative file paths",
    "text": "Absolute v. relative file paths\nProblem: I write pd.read_csv(\"C:/Users/eugene/data.csv\") in my code and send it to a project partner, Gritty. Gritty tries to run it, but it fails, and they have to change the code to pd.read_csv(\"C:/Users/gritty/data.csv\") to make it work. These are known as absolute file paths, which point to the exact location of a file on a computer, and generally they make it difficult to share code.\nOne way to deal with this is to organize your projects (e.g., a homework assignment) into self-contained folders. Something like the below:\nC:\n|__ /Users\n    |__ /eugene\n        |__ /MUSA550\n            |__ /Homework1\n                 |__ homework1_notebook.ipynb\n                 |__ /Data\n                      |__ data.csv\nWith this, I could instead write pd.read_csv(\"Data/data.csv\"). Then, I could send Gritty my entire Homework1 folder or upload that folder to GitHub as a repository, and Gritty can run the notebook without making any changes. These are relative file paths, which point to the location of a file on a computer relative to the working directory (i.e., the folder where homework1_notebook.ipynb is saved).\nFor your homework assignments, the most straightforward way to structure your files is to download the entire GitHub repository for that assignment (see below). Then, you can create a Jupyter notebook in that folder and, when you‚Äôre finished, upload it to GitHub. This way, when we download the repository for grading, or you open the notebook on a different computer, everything will run without changes (assuming you used relative file paths)."
  },
  {
    "objectID": "resource/file-paths.html#glossary",
    "href": "resource/file-paths.html#glossary",
    "title": "File paths and working directories",
    "section": "Glossary",
    "text": "Glossary\nThese terms/commands work in Jupyter Notebooks, and they also apply to any command line tools you might encounter, like the Terminal, git bash, etc. except the Windows Command Prompt, Miniforge Prompt (which is actually just a wrapper around the Command Prompt), and Windows PowerShell.\nhome directory: Also referred to as ~. This is the directory for your particular user on your computer. In Windows, it‚Äôs usually something like C:/Users/eugene. If you open Terminal/Miniforge Prompt, it will be at this location by default.\nroot directory: Also referred to as /. This is the very highest level directory in your computer, where your operating system folders and such are located. We won‚Äôt be doing anything here, since deleting files can mess things up (I don‚Äôt think you can even open the root directory in the Windows File Explorer).\nworking directory: Also referred to as .. Relative file paths will be relative to this location on your computer. You can run the command pwd (‚Äúprint working directory‚Äù) in either your Jupyter Notebook or the Terminal to see your current working directory. Note that ./subfolder/data.csv and subfolder/data.csv are the same; the first explicitly references the working directory, whereas it‚Äôs only implied in the second."
  },
  {
    "objectID": "resource/common-issues.html",
    "href": "resource/common-issues.html",
    "title": "Troubleshooting common installation issues",
    "section": "",
    "text": "Having trouble with mamba/conda or your Python installation? You‚Äôve come to the right place. Below, we outline some of the most common issues encountered during local installation of Python packages, as well as the troubleshooting steps to try to fix the issues."
  },
  {
    "objectID": "resource/common-issues.html#common-problems",
    "href": "resource/common-issues.html#common-problems",
    "title": "Troubleshooting common installation issues",
    "section": "Common Problems",
    "text": "Common Problems\nBelow we list some of the most common issues encountered when installing packages with mamba.\n\nMissing package errors\nIf you have successfully followed the steps outlined in the installation guide to create your environment, but receive an ImportError when importing packages, you might have launched the notebook from the 'base' environment instead of the ‚Äòmusa-550-fall-2023‚Äô environment. Be sure to activate the ‚Äòmusa-550-fall-2023‚Äô environment before launching the notebook.s\n\n\nThe file extension of the environment file on Windows\nBe sure that Windows does not automatically add an .txt extension to your environment.yml file. This will sometimes cause mamba to fail, with a cryptic error:\nSpecNotFound: environment with requirements.txt needs a name\nThe environment file needs to end in .yml. You can change the extension for a file on Windows following these instructions.\n\n\nMixing pip and mamba\nThe command pip can also be used to install Python packages. However, using pip to install packages into a mamba environment can lead to issues. It‚Äôs best to stick to using the mamba env update command to update your environment or mamba install package_name to install specific packages.\n\n\nImport errors for geopandas\nWhen importing geopandas, you can sometimes receive errors about missing libraries. This is usually because package versions got mixed up during installation. This can sometimes happen, and geopandas is particularly sensitive to the versions of its dependencies.\nThe best and easiest thing to do to try to solve it is use the steps above to create a fresh environment.\n\n\nNumpy errors\nIf you receive the following error:\nImportError: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/path/to/old/version/of/numpy/'']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.\nFrom the Miniforge Prompt (Windows) or Terminal (Mac), run:\nmamba install --force-reinstall --clobber numpy"
  },
  {
    "objectID": "resource/common-issues.html#most-common-fix-install-a-fresh-environment",
    "href": "resource/common-issues.html#most-common-fix-install-a-fresh-environment",
    "title": "Troubleshooting common installation issues",
    "section": "Most common fix: install a fresh environment",
    "text": "Most common fix: install a fresh environment\nUnfortunately, mamba/conda environments can sometimes become corrupted, preventing new packages from being installed or imported into Python properly. Most issues like this can be solved by simply deleting your current environment and starting fresh with a new version.\nThe following steps can be used to try to solve common issues:\n\n\n\n\n\n\nNote\n\n\n\nThe commands here should be executed via the command line, either using the Miniforge Prompt on Windows or the Terminal app on MacOS.\n\n\n\nStep 1: Delete any existing environment\nWe want to create a fresh environment, so you can delete any environment that was giving you issues. If that environment was called ‚Äòmusa-550-fall-2023‚Äô, you can run the following commands to delete it:\nmamba deactivate\nmamba env remove --name musa-550-fall-2023\nThe first command will make sure the environment we are deleting isn‚Äôt active, and then the second command will perform the deletion.\n\n\nStep 2: Create a fresh environment\nFollow the instructions outlined here to create a fresh version of the course website."
  },
  {
    "objectID": "resource/python.html",
    "href": "resource/python.html",
    "title": "Python resources",
    "section": "",
    "text": "MUSA 550 assumes some general familiarity with programming concepts, but there aren‚Äôt any formal Python prerequisites. However, we recommend all students that they use some of the free, online resources for learning Python‚Äôs core concepts. Below, we include a number of online resources that are freely available for students in the course."
  },
  {
    "objectID": "resource/python.html#datacamp-courses",
    "href": "resource/python.html#datacamp-courses",
    "title": "Python resources",
    "section": "DataCamp courses",
    "text": "DataCamp courses\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great ‚Äî the more the merrier!\n\n\n\n\n\n\nImportant\n\n\n\nTo gain access, use this unique invite link. You will need to create or sign in to a DataCamp account with an ‚Äúupenn.edu‚Äù email address.\n\n\nIntroductory DataCamp courses include:\n\nIntroduction to Python for Data Science\nPython Data Science Toolbox, Part 1\nPython DataScience Toolbox, Part 2\nIntroduction to NumPy\n\nAnd there are also shorter, free tutorials available on some core Python concepts:\n\nIf/else statements\nFor loops\nWhile loops\n\nA few courses covering more advanced topics include:\n\nIntermediate Python\nWriting functions in Python\n\nThere are also courses available to help reinforce topics we will cover in detail during the semester, including:\n\nData manipulation with pandas\nJoining data with pandas\nIntroduction to Data Visualization with seaborn\nIntroduction to Data Visualization with matplotlib\nIntermediate Data Visualization with seaborn\nSupervised Learning with scikit-learn\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the full list of available Python courses on DataCamp‚Äôs website."
  },
  {
    "objectID": "resource/python.html#introductory-python-tutorials",
    "href": "resource/python.html#introductory-python-tutorials",
    "title": "Python resources",
    "section": "Introductory Python tutorials",
    "text": "Introductory Python tutorials\nThe following tutorials assume no background in Python and provide a fairly comprehensive introduction to Python and its core concepts.\n\nPractical Python Programming by David Beazley\nPython for Social Science (in particular, the first four chapters)\nScientific Python Basics from the Berkeley Institute for Data Science (notebook version)"
  },
  {
    "objectID": "resource/python.html#more-in-depth-resources",
    "href": "resource/python.html#more-in-depth-resources",
    "title": "Python resources",
    "section": "More in-depth resources",
    "text": "More in-depth resources\nThere are two books available online for free that can serve as good resources for introductory Python basics as well as more advanced data science concepts.\n\nPython for Data Analysis\nThe Python for Data Analysis book by Wes McKinney (the creator of Pandas) is an excellent resource that covers the pandas library in great detail. The first few chapters are very good at covering the foundations of Python that we will use in this course:\n\nChatper 2: Python Basics\nChapter 3: Built-in Python Features\nChapter 4: NumPy Basics\nChapter 5: Intro to pandas\nChapter 9: Plotting & visualization\n\n\n\nThe Python Data Science Handbook\nThe The Python Data Science Handbook by Jake VanderPlas is a free, online textbook covering the Python basics needed for this course. It is a bit more advanced than the resources in the previous section and assumes some familiarity with Python.\nIn particular, the first four chapters are excellent:\n\nChapter 1: IPython/Jupyter\nChapter 2: Numpy\nChapter 3: Pandas\nChapter 4: matplotlib\n\nThe data analysis library pandas and the visualization library matplotlib will be covered extensively in this course, but the above chapters provide additional background material on this foundational Python tools.\nNote: You can click on the ‚ÄúOpen in Colab‚Äù button for each chapter and run the examples interactively in a cloud computing environment directly in the browser (using Google Colab)."
  },
  {
    "objectID": "resource/python.html#additional-resources",
    "href": "resource/python.html#additional-resources",
    "title": "Python resources",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Berkeley Institute for Data Science has compiled a number of Python resources\nThe subreddit r/learnpython is a good place for Python resources¬†‚Äî it maintains a comprehensive wiki of resources and tutorials."
  },
  {
    "objectID": "resource/jupyter.html",
    "href": "resource/jupyter.html",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "",
    "text": "In this course, we will perform most of our Python data analysis in files known as Jupyter notebooks. These files, which have an extension of ‚Äú.ipynb‚Äù, combine live runnable code with narrative text (via the Markdown language), images, interactive visualizations and other rich output.\nTo work with notebook files, we will use an application called JupyterLab. JupyterLab is a browser-based interface that allows users to edit and execute notebook files. It can also handle all sorts of additional file formats and even has a built-in command-line feature.\nOn this page, I‚Äôll discuss two common issues when starting out with Jupyter notebooks and JupyterLab: launching JupyterLab and ensuring the right files are available."
  },
  {
    "objectID": "resource/jupyter.html#launching-jupyterlab",
    "href": "resource/jupyter.html#launching-jupyterlab",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "Launching JupyterLab",
    "text": "Launching JupyterLab\nThe recommended approach for starting JupyterLab is to use the Miniforge Prompt on Windows or the Terminal app on MacOS. To do so, we simply need to activate our ‚Äòmusa-550-fall-2023‚Äô environment and then launch the notebook.\nFrom the command line, run:\nmamba activate musa-550-fall-2023\njupyter lab\nThis will create the local Jupyter server and should launch the JupyterLab dashboard in a browser. If it does not open in a browser, copy the link that is output by the command into your favorite browser. Typically, the server will be running at http://localhost:8888. The dashboard should like look something like this:\n\nOn the left, you‚Äôll see a file browser for the files in the folder where you ran the jupyter lab command. On the right you will see the ‚ÄúLauncher‚Äù, which allows you to easily create various types of new files. Click on the ‚ÄúPython 3‚Äù button under the ‚ÄúNotebook‚Äù section and you‚Äôll create your first notebook. Alternatively, you can use the File -&gt; New -&gt; Notebook option from the menu bar. The new notebook, entitled ‚ÄúUntitled.ipynb‚Äù, is created within the same directory.\nThe ‚ÄúLauncher‚Äù also shows you what other actions you can take with JupyterLab, including creating text files, Python files (‚Äú.py‚Äù files), Markdown files, new terminals or Python consoles. One of the most powerful features of JupyterLab is its ability to handle multiple file formats at once. You can have multiple file types open in the main work area and drag and resize these files to view them all at once, as described here.\n\n\n\n\n\n\nTip\n\n\n\nMore info on the various components of the JupyterLab interface, with several useful videos, is available here."
  },
  {
    "objectID": "resource/jupyter.html#changing-the-jupyterlab-start-up-folder",
    "href": "resource/jupyter.html#changing-the-jupyterlab-start-up-folder",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "Changing the JupyterLab start-up folder",
    "text": "Changing the JupyterLab start-up folder\nBy default, JupyterLab launches from the home directory. When you see the file browser on the left of the dashboard, you should see all of the files in this folder.\nWhen working with weekly lectures or assignments, it is easiest to launch JupyterLab from the specific assignment or week folder that you are working on.\nThere are two options to do this:\n\nChange to the desired folder before launching JupyterLab\nUse the ‚Äúnotebook-dir‚Äù option to specify the desired folder when launching JupyterLab\n\n\nOption 1\n\nStep 1: Change to the desired directory\nLet‚Äôs imagine we want to change to a folder named:\n/Users/YourUserName/MUSA_550 (on a Mac),\nor\nC:\\Users\\YourUserName\\MUSA_550 (on Windows)\nIf you need help finding the folder name‚Äôs path, this guide for Windows. (I usually use Method #2). On MacOS, you can use this guide to copy a folder‚Äôs path name.\nNext, use the following steps:\nStep 1. On Windows, open the Miniforge Prompt, or on Mac, open the Terminal.\nStep 2. Navigate to the folder where the environment file is located. From the Prompt or Terminal run:\n\nWindows\ncd C:\\Users\\YourUserName\\MUSA_550\nMac\ncd /Users/YourUserName/MUSA_550/\n\n\n\nStep 2: Launch JupyterLab\nNow, type the following command, either in Anaconda Prompt or the Terminal:\njupyter lab\nAnd you should now see the desired files in the file browser on the left sidebar of the JupyterLab interface!\n\n\n\nOption 2\nFrom the command line (Miniforge Prompt or Terminal), we can use the ‚Äúnotebook-dir‚Äù option to specify what working directory we want JupyterLab to use. For example, if we want to start from ‚Äú/Users/YourUserName/MUSA_550/‚Äù, we could do:\n\nWindows\njupyter lab --notebook-dir=C:\\Users\\YourUserName\\MUSA_550\nMac\njupyter lab --notebook-dir=/Users/YourUserName/MUSA_550/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Geospatial Data Science\n    \n      Use Python to gather, visualize, and analyze geospatial data with an urban planning and public policy focus\n    \n    \n      MUSA 550 ‚Ä¢ Fall 2023Masters of Urban Spatial AnalyticsWeitzman School of Design, University of Pennsylvania\n    \n  \n  \n    \n      \n        \n      \n    \n  \n\n\n\n\n\nInstructors\n\nSection 401\n\n ¬† Dr.¬†Nick Hand\n ¬† nhand@design.upenn.edu\n ¬† nicholashand\n\n\n\nSection 402\n\n ¬† Dr.¬†Eric Delmelle\n ¬† ericdel@design.upenn.edu\n ¬† ericdelmelle\n\n\n\n\nCourse details\n\nSection 401\n\n ¬† Mondays and Wednesdays\n ¬† 8:30AM-10:00AM\n ¬† Williams Hall 202\n\n\n\nSection 402\n\n ¬† Thursdays\n ¬† 1:45PM-4:45PM\n ¬† Meyerson Hall B2\n\n\n\n\nContacting us\nE-mail is the best way to get in contact with us. We will try to respond to all course-related e-mails within 24 hours, although we do have a few tiny humans üë∂ at home, so it may be a bit longer on occassion. Please be patient!"
  },
  {
    "objectID": "assignment/402/assignment-1.html",
    "href": "assignment/402/assignment-1.html",
    "title": "HW #1",
    "section": "",
    "text": "Section 402: Thursday, September 14 at 11:59 PM\n\nThis week‚Äôs assignment will be broken into two parts:\n\n\n\nFollow the initial installation guide on the course website for instructions on how to set up Python locally and launch JupyterLab\nPlease see the list of recommended readings for tutorials and background reading to get familiar with Python, mamba/conda, and the Jupyter notebook.\nProblems? Post your question to Ed Discussion!\n\n\nIf you‚Äôve successfully followed the installation guide, you should be able to launch JupyterLab by running jupyter lab (after activating the course environment) from the command line. This should launch the JupyterLab dashboard. Now you can launch the assignment notebook ‚Äúassignment-1.ipynb‚Äù and get started on the homework!\nThe notebook will execute code from the current working directory (the folder that the notebook was launched from). This folder is usually your home folder on your laptop. If you are using relative file paths to load the data, the path should be relative to this working directory. From within the Jupyter notebook, you can find out the current working directory by running the following command in a cell:\npwd\nIf you‚Äôve downloaded the assignment-1 repository to your computer, it usually makes sense to launch the Jupyter notebook from this folder instead of the default folder. You can change the start-up folder by first navigating to your assignment folder in the command line: see instructions here.\n\n\n\n\nIn part #2, you will explore the Donut Effect for home values in different parts of Philadelphia. This part will be submitted as a Jupyter notebook (a .ipynb file). There is already a starter notebook in this repository to help guide you through this part.\n\n\nNote: Be sure to read the documentation for assignment guidelines and submission.\nYou will submit your assignment through GitHub. For each assignment, I will provide a GitHub link that can be used to create a new repository. Each student will have their own private repository on GitHub where the assignment can be submitted. Only the student and instructors will have access to the private repository.\nThe invitation link depends on your section. For this assignments, the links are:\n\nSection 402: https://classroom.github.com/a/CGz9BQNG\n\nIf you do not have a GitHub account yet, you should be prompted to make an account. After clicking on this link, GitHub will create a new private repo with permissions such that only you and the instructors can view the commits.\nYour assignment should be added to this newly crearted GitHub repository before the deadline. You can add files to the repository through the web (github.com) interface or using the command line on your laptop.\nImportant: Files should be committed to the newly created private repository (after following the above link) and not to your forked version of the assignment-1 repository.\nYour Jupyter notebook should be submitted to your private repository by the deadline."
  },
  {
    "objectID": "assignment/402/assignment-1.html#due-date",
    "href": "assignment/402/assignment-1.html#due-date",
    "title": "HW #1",
    "section": "",
    "text": "Section 402: Thursday, September 14 at 11:59 PM\n\nThis week‚Äôs assignment will be broken into two parts:"
  },
  {
    "objectID": "assignment/402/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "href": "assignment/402/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "title": "HW #1",
    "section": "",
    "text": "Follow the initial installation guide on the course website for instructions on how to set up Python locally and launch JupyterLab\nPlease see the list of recommended readings for tutorials and background reading to get familiar with Python, mamba/conda, and the Jupyter notebook.\nProblems? Post your question to Ed Discussion!\n\n\nIf you‚Äôve successfully followed the installation guide, you should be able to launch JupyterLab by running jupyter lab (after activating the course environment) from the command line. This should launch the JupyterLab dashboard. Now you can launch the assignment notebook ‚Äúassignment-1.ipynb‚Äù and get started on the homework!\nThe notebook will execute code from the current working directory (the folder that the notebook was launched from). This folder is usually your home folder on your laptop. If you are using relative file paths to load the data, the path should be relative to this working directory. From within the Jupyter notebook, you can find out the current working directory by running the following command in a cell:\npwd\nIf you‚Äôve downloaded the assignment-1 repository to your computer, it usually makes sense to launch the Jupyter notebook from this folder instead of the default folder. You can change the start-up folder by first navigating to your assignment folder in the command line: see instructions here."
  },
  {
    "objectID": "assignment/402/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "href": "assignment/402/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "title": "HW #1",
    "section": "",
    "text": "In part #2, you will explore the Donut Effect for home values in different parts of Philadelphia. This part will be submitted as a Jupyter notebook (a .ipynb file). There is already a starter notebook in this repository to help guide you through this part.\n\n\nNote: Be sure to read the documentation for assignment guidelines and submission.\nYou will submit your assignment through GitHub. For each assignment, I will provide a GitHub link that can be used to create a new repository. Each student will have their own private repository on GitHub where the assignment can be submitted. Only the student and instructors will have access to the private repository.\nThe invitation link depends on your section. For this assignments, the links are:\n\nSection 402: https://classroom.github.com/a/CGz9BQNG\n\nIf you do not have a GitHub account yet, you should be prompted to make an account. After clicking on this link, GitHub will create a new private repo with permissions such that only you and the instructors can view the commits.\nYour assignment should be added to this newly crearted GitHub repository before the deadline. You can add files to the repository through the web (github.com) interface or using the command line on your laptop.\nImportant: Files should be committed to the newly created private repository (after following the above link) and not to your forked version of the assignment-1 repository.\nYour Jupyter notebook should be submitted to your private repository by the deadline."
  },
  {
    "objectID": "assignment/402/assignment-2.html",
    "href": "assignment/402/assignment-2.html",
    "title": "HW #2",
    "section": "",
    "text": "Assigned on Thursday, September 14\n\n\nDue on Thursday, September 28 at 11:59 PM"
  },
  {
    "objectID": "assignment/402/assignment-2.html#due-dates",
    "href": "assignment/402/assignment-2.html#due-dates",
    "title": "HW #2",
    "section": "Due Dates",
    "text": "Due Dates\n\nSection 402: Thursday, September 28 by end of day (11:59 PM)"
  },
  {
    "objectID": "assignment/402/assignment-2.html#recommended-readings",
    "href": "assignment/402/assignment-2.html#recommended-readings",
    "title": "HW #2",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nMatplotlib:\n\nGuide to getting started with matplotlib\nPlotting & visualization chapter of Python for Data Analysis\nMatplotlib chapter of the Data Science Handbook\n\nAltair:\n\nIntroduction to Statistical Visualization\nExploring Seattle Weather: A Case Study\n\nData viz design: Introductory slides of London‚Äôs design guidelines"
  },
  {
    "objectID": "assignment/402/assignment-2.html#reference-materials",
    "href": "assignment/402/assignment-2.html#reference-materials",
    "title": "HW #2",
    "section": "Reference materials",
    "text": "Reference materials\n\nmatplotlib\nseaborn\naltair"
  },
  {
    "objectID": "assignment/402/assignment-2.html#exploratory-data-visualization",
    "href": "assignment/402/assignment-2.html#exploratory-data-visualization",
    "title": "HW #2",
    "section": "Exploratory Data Visualization",
    "text": "Exploratory Data Visualization\nIn this part, you‚Äôll use matplotlib, seaborn, and altair to explore a dataset of your choosing and generate some charts in a Jupyter notebook.\n\nPart 1: Select a dataset\nFor this assignment, you can choose your own dataset to explore. I recommend selecting a dataset from OpenDataPhilly. You are welcome to to use a dataset from elsewhere, but please email your instructor and let them know what you want to analyze.\nDatasets with timestamped entries will be particularly good for analysis, but there are many interesting datasets to consider. They include:\n\n311 Requests\nVoter turnout\nComplaints against police\nParking violations\nShooting victims\nL+I Violations\n\nand many more‚Ä¶\nFor OpenDataPhilly datasets, data files can be downloaded in the form of CSV files.\n\n\nPart 2: Explore and visualize the data\nFrom within a Jupyter notebook, you should explore the datasets and generate charts visualizing different aspects of the data.\nRequirements:\n\n1 Matplotlib chart (of any type)\n\nYou should consider what aspect of the dataset might be best plotted using matplotlib.\nInclude your reasoning for using matplotlib to visualize this specific aspect of the data.\nYou will be graded on the aesthetics of the plot, namely, color choices and clarity.\n\n1 seaborn chart (of any type)\n\nPlease include the motivation behind your choice for the type of seaborn plot used and why.\n\n3 altair plots\n\nBoth of the following techniques should be used:\n\nA transformation (mean, count, binning, etc)\nBrush selection\n\n\nInclude a short discussion (a few sentences) of the main conclusion of each chart (in a markdown cell below each chart). It does not need to be interesting or insightful, but it is good practice to always note the main conclusions so the notebook make sense after time passes.\nExtra credit:: 2-chart altair dashboard,\n\nThis should be in addition to the 3 charts above\nFiltering on one chart should cross-filter the other chart (via transform_filter())\n\n\nNote: You may include geospatial charts to satisfy the above requirements, but you are not required to."
  },
  {
    "objectID": "assignment/402/assignment-2.html#grading-guidelines",
    "href": "assignment/402/assignment-2.html#grading-guidelines",
    "title": "HW #2",
    "section": "Grading Guidelines",
    "text": "Grading Guidelines\nSee the grading rubric for more details.\nImportant: Your notebooks should be a polished finished product. For example:\n\nPlease remove any extra or unneccesary code.\nPlease try to use markdown cells with section headers to mark different sections of the analysis."
  },
  {
    "objectID": "assignment/402/assignment-2.html#submission",
    "href": "assignment/402/assignment-2.html#submission",
    "title": "HW #2",
    "section": "Submission",
    "text": "Submission\nNote: Be sure to read the documentation for assignment guidelines and submission.\nWe‚Äôll be using GitHub for assignment submission again. The invitation link depends on your section. For this assignments, the links are:\n\nSection 402: https://classroom.github.com/a/coiOv0PU\n\nYour assignment notebook should be added to this GitHub repository before the deadline.\nNote: File sizes are limited to 25 MB when adding files to a repository via GitHub in a browser. If your dataset is larger than this, you can try compressing it into a ‚Äú.zip‚Äù to make the file smaller, and then add it to the repository on GitHub. If you run into problems, please email your instructor for help."
  },
  {
    "objectID": "assignment/402/assignment-5.html",
    "href": "assignment/402/assignment-5.html",
    "title": "HW #5",
    "section": "",
    "text": "Assigned on Thursday, November 2\n\n\nDue on Tuesday, November 21 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-5\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/402/final-project.html",
    "href": "assignment/402/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "Due on Wednesday, December 20 at 11:59 PM\nThe final project is to replicate the pipeline approach on a dataset (or datasets) of your choosing.\nThe final deliverable will be a web-based data visualization and accompanying description including a summary of the results and the methods used in each step of the process (collection, analysis and visualization)."
  },
  {
    "objectID": "assignment/402/final-project.html#due-dates",
    "href": "assignment/402/final-project.html#due-dates",
    "title": "Final Project",
    "section": "Due dates",
    "text": "Due dates\nNote: Due dates are tentative and subject to change\nWritten proposal due date: Monday, December 4th (or earlier)\nProject due date: Wednesday, December 20th at 11:59 PM\nFinal projects must receive prior approval in the form of a written proposal."
  },
  {
    "objectID": "assignment/401/schedule.html",
    "href": "assignment/401/schedule.html",
    "title": "Assignments: Section 401",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n\n\n\n\n\n   Assigned on\n\n\n   Due on\n\n\n\n\n\n\nHW #1\n\n\nWednesday, September 6\n\n\nMonday, September 25\n\n\n\n\nHW #2\n\n\nMonday, September 25\n\n\nMonday, October 9\n\n\n\n\nHW #3\n\n\nMonday, October 9\n\n\nMonday, October 23\n\n\n\n\nHW #4\n\n\nMonday, October 23\n\n\nMonday, November 6\n\n\n\n\nHW #5\n\n\nMonday, November 6\n\n\nMonday, November 20\n\n\n\n\nHW #6\n\n\nMonday, November 20\n\n\nMonday, December 4\n\n\n\n\nFinal project proposal\n\n\n\n\nMonday, December 4\n\n\n\n\nFinal project\n\n\n\n\nWednesday, December 20"
  },
  {
    "objectID": "assignment/401/assignment-3.html",
    "href": "assignment/401/assignment-3.html",
    "title": "HW #3",
    "section": "",
    "text": "Assigned on Monday, October 9\n\n\nDue on Monday, October 23 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-3\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/401/assignment-6.html",
    "href": "assignment/401/assignment-6.html",
    "title": "HW #6",
    "section": "",
    "text": "Assigned on Monday, November 20\n\n\nDue on Monday, December 4 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-6\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/401/assignment-4.html",
    "href": "assignment/401/assignment-4.html",
    "title": "HW #4",
    "section": "",
    "text": "Assigned on Monday, October 23\n\n\nDue on Monday, November 6 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-4\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/401/final-project-proposal.html",
    "href": "assignment/401/final-project-proposal.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Due on Monday, December 4 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/final-project-proposal\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "content/week-1/lecture-1A.html",
    "href": "content/week-1/lecture-1A.html",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "",
    "text": "Section 401\nAug.¬†30, 2023"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#today",
    "href": "content/week-1/lecture-1A.html#today",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Today",
    "text": "Today\n\nCourse logistics\nUsing Jupyter Notebooks and Jupyter Lab\nIntroduction to Python & Pandas"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#who-am-i",
    "href": "content/week-1/lecture-1A.html#who-am-i",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Who am I?",
    "text": "Who am I?\n\nMy day job\n\nMy name is Nick Hand\nFor the past 5+ years, I have led a small data science team in the City Controller‚Äôs Office\nWhat did we do?:\n\nObjective, data-driven analysis of financial policies impacting Philadelphia\nIncreasing transparency through data releases and interactive reports\n\nIn two weeks, I start as a data scientist working at the Consumer Finance Protection Bureau in the federal government\n\nIn my time at the Controller‚Äôs Office, we covered a range of policy issues in the city:\n\nAnalysis of the fairness and accuracy of property assessments\nInteractive reports for the City‚Äôs cash levels\nAnalysis of the 10-Year Tax Abatement program\nInteractive dashboard of Soda Tax spending\nVisualization of paving & potholes\nSeries on the impact of COVID-19 on Philadelphia‚Äôs small businesses and neighborhoods\nVisualization of the City‚Äôs most recent budget\nInteractive report on redlining in Philadelphia\nAnalysis of the impact of gun violence on housing prices\nInteractive dashboard of shooting victims in Philadelphia\nInteractive dashboard of neighborhood well-being\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo see more of our work, check out: https://controller.phila.gov/policy-analysis\n\n\n\n\nPreviously:Astrophysics Ph.D.¬†at Berkeley\n\n\n\n\n\n\n\nHow did I get here?\n\nAstrophysics/physics to data science is becoming increasingly common\nLanded a job through Twitter: https://www.parkingjawn.com\n\nDashboard visualization of monthly parking tickets in Philadelphia\nData from OpenDataPhilly\n\n\n\nA nice example of how exploratory analysis + a well-designed dashbord can lead to insights: - The power of cross-filtering: different views of the same data across multiple dimensions - See drop in parking tickets over Jan 24-26, 2016 due to snowstorm\nParking Jawn is not Python based, but dovetails nicely with one of the main goals of the course: &gt; How can we effectively explore and extract insight from complex datasets?"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#course-logistics",
    "href": "content/week-1/lecture-1A.html#course-logistics",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Course logistics",
    "text": "Course logistics\n\nGeneral Info\n\nTwo 90-minute lectures per week ‚Äî mix of lecturing, interactive demos, and in-class lab time\nMy email: nhand@design.upenn.edu\n\nOffice Hours:\n\n2-hours during the week\nOffice hours will be by appointment and remote via Zoom. You will be able to sign up for 1 (or more) 15-minute time slot via the Canvas calendar.\nTime is to be determined\n\n\nTeaching Assistant: Teresa Chang\n\nEmail: thchang@design.upenn.edu\nOffice hours: TBD\n\n\n\n\nCourse Websites\nCourse has four websites (sorry!). They are:\n\nMain Course: https://musa-550-fall-2023.github.io\nGithub: https://github.com/MUSA-550-Fall-2023\nCanvas: https://canvas.upenn.edu/courses/1740535\nEd Discussion: https://edstem.org/us/courses/42616/discussion/\n\nEach will have its own purpose:\n\nMain course website\n\nCourse schedule with links to weekly slides\nResources for learning Python, setting up software, and dealing with common issues\nGeneral course info and policies\nQuick links to the other websites for the course\n\n\n\nGithub\n\nGithub organization set up for the course\nEach week and assignment will have its own Github repository\nAssignments will also be submitted through Github\n\n\n\nCanvas\n\nWill be used sign up for remote office hours and provide Zoom links for office hours\nGrading will also be tracked here\n\n\n\nEd Discussion\n\nWill be used for question & answer forum for course materials and assignments\nAnnouncements will also be made here so make sure you check frequently or turn on your notifications!\nMain method of communication will be through announcements on this site\nParticipation grade (5% of total grade) will also be determined by user activity on the forum"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#main-course-website-1",
    "href": "content/week-1/lecture-1A.html#main-course-website-1",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Main course website",
    "text": "Main course website\n\n  \n\n\nHighlights\n\nSyllabus\nSchedule\nResources & Guides:\n\nPython resources\nInitial installation guide\n\nWeekly content\nAssignments\nQuick links to Canvas, Ed Discussion, GitHub homepage"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#course-github",
    "href": "content/week-1/lecture-1A.html#course-github",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Course Github",
    "text": "Course Github"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-goals-of-this-course",
    "href": "content/week-1/lecture-1A.html#the-goals-of-this-course",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The goals of this course",
    "text": "The goals of this course\n\nProvide students with the knowledge and tools to turn data into meaningful insights and stories\nFocus on the modern data science tools within the Python ecosystem\nThe pipeline approach to data science:\n\ngathering, storing, analyzing, and visualizing data to tell stories\n\nReal-world applications of analysis techniques in the urban planning and public policy realm"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#what-well-cover",
    "href": "content/week-1/lecture-1A.html#what-well-cover",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "What we‚Äôll cover",
    "text": "What we‚Äôll cover\n\nModule 1\nExploratory Data Science: Students will be introduced to the main tools needed to get started analyzing and visualizing data using Python\n\n\nModule 2\nIntroduction to Geospatial Data Science: Building on the previous set of tools, this module will teach students how to work with geospatial datasets using a range of modern Python toolkits.\n\n\nModule 3\nData Ingestion & Big Data: Students will learn how to collect new data through web scraping and APIs, as well as how to work effectively with the large datasets often encountered in real-world applications.\n\n\nModule 4\nFrom Exploration to Storytelling: With a solid foundation, students will learn the latest tools to present their analysis results using web-based formats to transform their insights into interactive stories.\n\n\nModule 5\nGeospatial Data Science in the Wild: Armed with the necessary data science tools, the final module introduces a range of advanced analytic and machine learning techniques using a number of innovative examples from modern researchers."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#assignments-and-grading",
    "href": "content/week-1/lecture-1A.html#assignments-and-grading",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Assignments and grading",
    "text": "Assignments and grading\n\nGrading:\n\n50% homework\n45% final project\n5% participation (based on class and Ed Discussion participation)\n\nWhile you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nThere‚Äôs no penalty for late assignments. I would highly recommend staying caught up on lectures and assignments as much as possible, but if you need to turn something in a few days late, there won‚Äôt be a penalty.\n\nNote: Homeworks will be assigned (roughly) every two and a half weeks."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-course-schedule",
    "href": "content/week-1/lecture-1A.html#the-course-schedule",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The course schedule",
    "text": "The course schedule\nCheck out the schedule page for the most up-to-date details on lectures, assignment due dates, etc."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#final-project",
    "href": "content/week-1/lecture-1A.html#final-project",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Final project",
    "text": "Final project\nThe final project is to replicate the pipeline approach on a dataset (or datasets) of your choosing.\nStudents will be required to use several of the analysis techniques taught in the class and produce a web-based data visualization that effectively communicates the empirical results to a non-technical audience.\nMore info will be posted here: https://github.com/MUSA-550-Fall-2023/final-project"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#any-questions-so-far",
    "href": "content/week-1/lecture-1A.html#any-questions-so-far",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Any questions so far?",
    "text": "Any questions so far?"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#initial-surveys",
    "href": "content/week-1/lecture-1A.html#initial-surveys",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Initial surveys",
    "text": "Initial surveys\nRoll call: https://bit.ly/musa550-roll-call\nSome initial feedback: https://bit.ly/musa550-initial-feedback"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-incredible-growth-of-python",
    "href": "content/week-1/lecture-1A.html#the-incredible-growth-of-python",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The Incredible Growth of Python",
    "text": "The Incredible Growth of Python\nSource: A 2017 analysis of StackOverflow posts"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-rise-of-the-jupyter-notebook",
    "href": "content/week-1/lecture-1A.html#the-rise-of-the-jupyter-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The rise of the Jupyter notebook",
    "text": "The rise of the Jupyter notebook\n\nThe engine of collaborative data science\n\nFirst started by a physics grad student around 2001\nKnown as the IPython notebook originally\nStarting getting popular in ~2011\nFirst funding received in 2015 ‚Äî the Jupyter notebook was born\n\n\n\nGoogle searches for Jupyter notebook\n\n\n\nKey features\n\nAimed at ‚Äúcomputational narratives‚Äù ‚Äî telling stories with data\nInteractive, reproducible, shareable, user-friendly, visualization-focused\nFully open-source and managed by the community\n\nVery versatile: good for both exploratory data analysis and polished finished products\n\n\n\n\n\n\nImportant\n\n\n\nThe lecture slides in the course will all be Jupyter notebooks. The preferred interface for editing and executing them will be JupyterLab. That‚Äôs what I‚Äôm using now!\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more info on Jupyter notebooks and JupyterLab, check out the guide on the course website.\nIn particulary, I strongly encourage you to go through the official documentation for JupyterLab and Jupyter notebooks:\n\nStarting JupyterLab\nThe JupyterLab interface\nThe structure of a notebook document\nThe notebook workflow\nWorking with notebooks in JupyterLab\nWorking with files"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#beyond-the-jupyter-notebook",
    "href": "content/week-1/lecture-1A.html#beyond-the-jupyter-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Beyond the Jupyter notebook",
    "text": "Beyond the Jupyter notebook\nGoogle Colab is the most popular alternative to Jupyter notebooks.\n\n\nA fancier notebook experience built on top of Jupyter notebook\nRunning in the cloud on Google‚Äôs servers\nAn internal Google product that was released publicly\nVery popular for Python-based machine learning, since it provides low-barrier access to GPU resources which can be very helpful for training machine learning models\nWe‚Äôll focus on the open-source Jupyter notebook as the foundation for this course\n\nSee, for example: https://colab.research.google.com/notebooks/welcome.ipynb"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-binder-service",
    "href": "content/week-1/lecture-1A.html#the-binder-service",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The Binder service",
    "text": "The Binder service\nhttps://mybinder.org\n\n\nA free, open-source service, supported by donors\nAllows you to launch a repository of Jupyter notebooks on GitHub in an executable environment in the cloud\nAmazing if you want to make your code immediately reproducible by anyone, anywhere.\nNote: as a free service, it can be a bit slow sometimes"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#class-lectures",
    "href": "content/week-1/lecture-1A.html#class-lectures",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Class lectures",
    "text": "Class lectures\nWeekly lectures are available on Binder! In the README for each week‚Äôs repository on GitHub, you will see badges to launch the lecture slides on Binder.\n\n\n\n\nYou can also access these links from the content section of the course website. For example, here is:\nhttps://musa-550-fall-2023.github.io/content/week-1/\n\n\n\n\n\n\n\n\n\nSuggested weekly workflow\n\n\n\n\nSet up local Python environment as part of first homework assignment\nEach week, you will have two options to follow along with lectures:\n\nUsing Binder in the cloud, launching via the button on the week‚Äôs repository\nDownload the week‚Äôs repository to your laptop and launch the notebook locally\n\nWork on homeworks locally on your laptop ‚Äî Binder is only a temporary environment (no save features)\n\n\n\nCheck out the content overview page on the main course website for more info!"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#now-to-the-fun-stuff",
    "href": "content/week-1/lecture-1A.html#now-to-the-fun-stuff",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Now to the fun stuff‚Ä¶",
    "text": "Now to the fun stuff‚Ä¶\nJupyter notebooks are a mix of code cells and text cells in Markdown. You can change the type of cell in the top menu bar.\nThis cell is a Markdown cell.\n\n# Comments begin with a \"#\" character in Python\n# A simple code cell\n# SHIFT-ENTER to execute\n\n\nx = 10\nprint(x)\n\n10\n\n\n\nPython data types\n\n# integer\na = 10\n\n# float\nb = 10.5\n\n# string\nc = \"this is a test string\"\n\n# lists\nd = list(range(0, 10))\n\n# booleans\ne = False\n\n# dictionaries\nf = {\"key1\": 1, \"key2\": 2}\n\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)\nprint(f)\n\n10\n10.5\nthis is a test string\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFalse\n{'key1': 1, 'key2': 2}\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike R, you‚Äôll need to use quotes more often in Python, particularly around strings and keys of dictionaries.\n\n\n\n\nAlternative method for creating a dictionary\nWe can use the dict() function, which is built in to the Python language. More on functions in a bit‚Ä¶\n\nf = dict(key1=1, key2=2, key3=3)\n\nf\n\n{'key1': 1, 'key2': 2, 'key3': 3}\n\n\n\n\nAccessing dictionary values\n\n# Access the value with key 'key1'\nf['key1']\n\n1\n\n\n\n\nAccessing list values\n\nd\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n# Access the second list entry (0 is the first index)\nd[0]  \n\n0\n\n\n\n\nAccessing characters of a string\n\nc\n\n'this is a test string'\n\n\n\n# the first character\nc[0]\n\n't'\n\n\n\n\nIterators and for loops\n\n\n\n\n\n\nImportant\n\n\n\nBe sure to use the right indentation in for loops!\n\n\n\n# Variable that will track the sum\nresult_sum = 0\n\n# Variable i takes on values [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor i in range(0, 10):\n        \n    # Indented, so it runs for each iteration of the loop\n    print(i)\n\n    result_sum = result_sum + i\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nprint(result_sum)\n\n45\n\n\n\n\nPython‚Äôs inline syntax\n\na = range(0, 10) # this is an iterator\n\n\nprint(a)\n\nrange(0, 10)\n\n\nUse the list() function to iterate over it and make it into a list:\n\n# convert it to a list explicitly\na = list(range(10))\n\n# Output it from the cell\na\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n# or use the INLINE syntax; this is the SAME\na = [i for i in range(10)]\n\na\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\nPython functions\ndef function_name(arg1, arg2, arg3):\n    \n    .\n    .\n    .\n    code lines (indented)\n    .\n    .\n    .\n    \n    return result\n\ndef compute_square(x):\n    \n    sq = x * x\n    return sq\n\n\nsq = compute_square(10)\nprint(sq)\n\n100\n\n\n\n\nKeywords: arguments with a default!\n\ndef compute_product(x, y=5):\n    return x * y\n\n\n# use the default value for y\nprint(compute_product(3))\n\n15\n\n\n\n# specify a y value other than the default\nprint(compute_product(5, 10))\n\n50\n\n\n\n# can also explicitly tell Python which arguments are which\nprint(compute_product(5, y=2))\nprint(compute_product(y=2, x=5))\n\n10\n10\n\n\n\nprint(compute_product(x=5, y=4))\n\n20\n\n\n\n# argument names must match the function signature though!\nprint(compute_product(x=5, z=5))\n\nTypeError: compute_product() got an unexpected keyword argument 'z'\n\n\n\n\nGetting help in the notebook\nUse tab auto-completion and the ? and ?? operators\n\nthis_variable_has_a_long_name = 5\n\n\n# try hitting tab after typing this_ \nthis_variable_has_a_long_name\n\n\n# Forget how to create a range? --&gt; use the help message\nrange?\n\n\n\nPeeking at the source code for a function\nUse the ?? operator\n\n# Lets re-define compute_product() and add a docstring between \"\"\" \"\"\"\ndef compute_product(x, y=5):\n    \"\"\"\n    This computes the product of x and y\n    \n    \n    This is all part of the comment.\n    \"\"\"\n    return x * y\n\n\ncompute_product??\n\n\n\n\n\n\n\nNote\n\n\n\nThe question mark operator gives you access to the help message for any variable or function.\nI use this frequently and it is a great way to understand what a function actually does.\n\n\n\n\nThe JupyterLab Debugger\nYou can enable Debugging mode in JupyterLab by clicking on the ‚Äúbug‚Äù icon in the top right:\n\nThis should open the Debugger panel on the right side of JupyterLab. One of the most useful parts of this panel is the ‚ÄúVariables‚Äù section, which gives you the current values of all defined variables in the notebook.\n\n\n\n\n\n\n\nTip\n\n\n\nFor more information on the debugger, see the JupyterLab docs."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#getting-more-python-help",
    "href": "content/week-1/lecture-1A.html#getting-more-python-help",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Getting more Python help",
    "text": "Getting more Python help\nThis was a very brief introduction to Python and Python syntax. We‚Äôll continue practicing and reinforcing the proper syntax throughout the next few weeks, but it can definitely be frustrating. Hang in there!\n\n\n\n\n\n\nImportant: DataCamp Tutorials\n\n\n\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great ‚Äî the more the merrier!\nFor more info, including how to sign up, check out the resources section of the website.\n\n\nAdditional Python resources are listed on our course website under ‚ÄúResources‚Äù\nhttps://musa-550-fall-2023.github.io/resource/python.html\nIn addition to the DataCamp videos, there are links to lots of online tutorials:\n\nIntroductory level tutorials\nMore in depth tutorials\nThe r/learnpython subreddit has a great wiki of resources\nThe Berkeley Institute for Data Science has also compiled a number of Python resources"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#one-more-thing-working-outside-the-notebook",
    "href": "content/week-1/lecture-1A.html#one-more-thing-working-outside-the-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "One more thing: working outside the notebook",
    "text": "One more thing: working outside the notebook\nIn this class, we will almost exclusively work inside Jupyter notebooks ‚Äî you‚Äôll be writing Python code and doing data analysis directly in the notebook.\nThe more traditional method of using Python is to put your code into a .py file and execute it via the command line (known as the Miniforge/Anaconda Prompt on Windows or Terminal app on MacOS).\nSee this section of the Practical Python Programming tutorial for more info.\n\nThe JupyterLab text editor\nThere is a file called hello_world.py in the repository for week 1. If we execute it, it should print out ‚ÄúHello, World‚Äù to the command line.\nFirst, let‚Äôs open up the .py file in the JupyterLab text editor. Double click on the ‚Äúhello_world.py‚Äù item in the file browser on the left:\n\nThis will open the file and allow you to make edits. You should see the following:\n# Our first Python program\nprint(\"Hello World!\")\n\n\n\n\n\n\nTip\n\n\n\nSee the JupyterLab docs for more info on the text editor.\n\n\n\n\nUsing the JupyterLab Terminal\nTo execute the file, we can use the built-in Terminal feature in JupyterLab using the following steps:\n\nBring up the ‚ÄúLauncher‚Äù tab by clicking on the blue button with a plus sign in the upper left.\nClick on the ‚ÄúTerminal‚Äù button‚Äù\nWhen the terminal opens, type the following:\n\npython hello_world.py\nAnd you should see the following output:\nHello World!\nIt should look something like this:"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#code-editors",
    "href": "content/week-1/lecture-1A.html#code-editors",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Code editors",
    "text": "Code editors\nThe JupyterLab text editor will work in a pinch, but it‚Äôs not usually the best option when writing software outside the notebook. Other code editors will provide a nice interface for writing Python code and some even have fancy features, like real-time syntax checking and syntax highlighting.\nMy recommended option is Visual Studio Code."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#see-you-next-week",
    "href": "content/week-1/lecture-1A.html#see-you-next-week",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "See you next week!",
    "text": "See you next week!\n\nNo lecture on Monday next week due to Labor Day\nNext class a week from today on Wednesday 9/6\nIn the meantime:\n\nFollow [the guide] for setting up your local Python environment\nCheck out Python resources + DataCamp courses in the meantime\nHomework #1 will be posted a week from today (Wednesday 9/6, due on 9/20)"
  },
  {
    "objectID": "content/week-1/index.html",
    "href": "content/week-1/index.html",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "",
    "text": "Content for lectures 1A and 1B\n\n\n        \n            \n            ¬†\n            View materials:\n            ¬†\n            MUSA-550-Fall-2023/week-1\n        \n\n        \n            \n            ¬†\n            HTML slides:\n            ¬†\n            \n                Lecture 1A\n            \n            ¬†\n            \n                Lecture 1B\n            \n        \n        \n\n        \n            \n            ¬†\n            Executable slides:\n            ¬†\n            \n                Lecture 1A\n            \n            ¬†\n            \n                Lecture 1B"
  },
  {
    "objectID": "content/week-1/index.html#topics",
    "href": "content/week-1/index.html#topics",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Topics",
    "text": "Topics\n\nCourse Logistics\nUsing Jupyter Notebooks\nIntroduction to Pandas"
  },
  {
    "objectID": "content/week-1/index.html#initial-survey",
    "href": "content/week-1/index.html#initial-survey",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Initial Survey",
    "text": "Initial Survey\nPlease fill out this anonymous survey for some initial feedback:\nhttps://bit.ly/musa550-initial-feedback"
  },
  {
    "objectID": "content/week-1/index.html#recommended-readings",
    "href": "content/week-1/index.html#recommended-readings",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nCourse syllabus\nGuide for installing Python and initial set up\nGuide for mamba/conda\n\nCheck out this 20-minute guide on the conda docs\n\nGuide for Jupyter notebooks and JupyterLab\n\nGood readings on the Jupyter docs include:\n\nStarting JupyterLab\nThe JupyterLab interface\nThe structure of a notebook document\nThe notebook workflow\nWorking with notebooks in JupyterLab\nWorking with files\n\n\nRecommended tutorial for students with little Python background: Practical Python Programming\n\n\nPython tutorials & DataCamp access\nA number of Python resources are listed on the course website. In particular, DataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great ‚Äî the more the merrier!\nCheck out the resources page for more information, including how to sign up."
  },
  {
    "objectID": "content/week-1/index.html#additional-materials",
    "href": "content/week-1/index.html#additional-materials",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Additional materials",
    "text": "Additional materials\n\nDocumentation\n\nGit and GitHub:\n\nSetting up git\nManaging files on GitHub\n\nConda user guide\nJupyter notebook\nGuide to Markdown in notebook cells\nBinder Documentation\npandas documentation\nmatplotlib documentation\n\n\n\nReadings/reference\n\nStackOverflow: The Incredible Growth of Python\n2015 Funding Proposal for the Jupyter Notebook\nA history of Jupyter Notebooks by its founder\nGoogle‚Äôs Colab\nTidy Data\nJoining Datasets\nZIllow Data\nIntroduction to writing your first Python program\nVisual Studio Code"
  },
  {
    "objectID": "content/week-2/index.html",
    "href": "content/week-2/index.html",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "",
    "text": "Content for lectures 2A and 2B\n\n\n        \n            \n            ¬†\n            View materials:\n            ¬†\n            MUSA-550-Fall-2023/week-2\n        \n\n        \n            \n            ¬†\n            HTML slides:\n            ¬†\n            \n                Lecture 2A\n            \n            ¬†\n            \n                Lecture 2B\n            \n        \n        \n\n        \n            \n            ¬†\n            Executable slides:\n            ¬†\n            \n                Lecture 2A\n            \n            ¬†\n            \n                Lecture 2B"
  },
  {
    "objectID": "content/week-2/index.html#topics",
    "href": "content/week-2/index.html#topics",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Topics",
    "text": "Topics\n\nColor fundamentals and available tools\nIntroduction to matplotlib, seaborn, and altair"
  },
  {
    "objectID": "content/week-2/index.html#recommended-readings",
    "href": "content/week-2/index.html#recommended-readings",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nGuide to getting started with matplotlib\nPlotting & visualization chapter of Python for Data Analysis\n\nA good introduction to plotting with matplotlib, pandas, and seaborn\n\nAltair:\n\nIntroduction to Statistical Visualization\nInteractive charts with altair\nExploring Seattle Weather: A Case Study\n\nData viz design: Introductory slides of London‚Äôs design guidelines"
  },
  {
    "objectID": "content/week-2/index.html#documentation",
    "href": "content/week-2/index.html#documentation",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Documentation",
    "text": "Documentation\n\nmatplotlib\n\nExample plots\nPlot types\n\nseaborn docs homepage\nSeaborn tutorial\n\nVisualizing statistical relationships\nCategorical data\nVisualizing the distribution of a data set\nVisualizing linear relationships\n\naltair\n\nThe user guide\nExample gallery"
  },
  {
    "objectID": "content/week-2/index.html#other-readings",
    "href": "content/week-2/index.html#other-readings",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Other Readings",
    "text": "Other Readings\n\nRemaking Historical Data Viz\nModern Statistical Atlas\nA 6-part blog series on W. E. B. Du Bois‚Äô Data Visualization\nData Sketches\nThe 7 Kinds of Data Visualization People\nFast vs Slow Data Visualization\nWhat Charts Say\nColor Guide and Overview of Color Tools\nThe Importance of Grey\n2017 PyCon presentation by Jake VanderPlas\nInterview with John Burn-Murdoch about his COVID Data Viz"
  },
  {
    "objectID": "content/week-2/index.html#color-resources",
    "href": "content/week-2/index.html#color-resources",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Color Resources",
    "text": "Color Resources\n\nColorBrewer 2.0\nViz Palette\nAdobe Color CC\nColorpicker for data\nChroma.js Color Scale Helper\nChoosing Color Maps in Matplotlib\nChoosing Color Palettes in seaborn\nNames of available color maps in Altair"
  },
  {
    "objectID": "content/week-2/lecture-2B.html",
    "href": "content/week-2/lecture-2B.html",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "",
    "text": "Section 401\nSep 13, 2023"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#housekeeping",
    "href": "content/week-2/lecture-2B.html#housekeeping",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nHW #1 due on Wednesday 9/20\nHW #2 posted on same day (9/20)\nLots of good questions on Ed Discussion so far!\n\nEmail me if you need access: https://edstem.org/us/courses/42616/discussion/\n\n\nReminder: Quick links to course materials and main sites (Ed Discussion, Canvas, Github) can be found in the upper right corner of the top navbar:\nhttps://musa-550-fall-2023.github.io/"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#reminder-office-hours",
    "href": "content/week-2/lecture-2B.html#reminder-office-hours",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Reminder: Office Hours",
    "text": "Reminder: Office Hours\n\nNick:\nTeresa: Fridays 10:30AM-12:00PM\nRemote: sign-up for time slots on Canvas calendar"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#week-2-recap",
    "href": "content/week-2/lecture-2B.html#week-2-recap",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Week #2 Recap",
    "text": "Week #2 Recap\n\nWeek #2 repository: https://github.com/MUSA-550-Fall-2023/week-2\nRecommended readings for the week listed here\n\n\nLast time\n\nA brief overview of data visualization\nPractical tips on color in data vizualization\n\n\n\nToday\n\nThe Python landscape:\n\nmatplotlib\npandas\n\nOne more static plotting function: seaborn\nAdding interaction to our plots!\nIntro to the Altair package\nLab: Reproducing a famous Wall Street Journal data visualization with Altair"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#reminder-following-along-with-lectures",
    "href": "content/week-2/lecture-2B.html#reminder-following-along-with-lectures",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Reminder: following along with lectures",
    "text": "Reminder: following along with lectures\n\nEasiest option: Binder\n\n\n\nHarder option: downloading Github repository contents"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#recommended-readings",
    "href": "content/week-2/lecture-2B.html#recommended-readings",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Recommended readings",
    "text": "Recommended readings\nBe sure to check out the recommended readings for the week:\n\nGuide to getting started with matplotlib\nPlotting & visualization chapter of Python for Data Analysis\n\nA good introduction to plotting with matplotlib, pandas, and seaborn\n\nAltair:\n\nIntroduction to Statistical Visualization\nInteractive charts with altair\nExploring Seattle Weather: A Case Study\n\nData viz design: Introductory slides of London‚Äôs design guidelines\n\n\n# The imports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Make sure plots show up in JupyterLab!\n%matplotlib inline"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#the-python-data-viz-landscape",
    "href": "content/week-2/lecture-2B.html#the-python-data-viz-landscape",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "The Python data viz landscape",
    "text": "The Python data viz landscape\nSo many tools‚Ä¶so little time"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#which-one-is-the-best",
    "href": "content/week-2/lecture-2B.html#which-one-is-the-best",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Which one is the best?",
    "text": "Which one is the best?"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#there-isnt-one",
    "href": "content/week-2/lecture-2B.html#there-isnt-one",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "There isn‚Äôt one‚Ä¶",
    "text": "There isn‚Äôt one‚Ä¶\nYou‚Äôll use different packages to achieve different goals, and they each have different things they are good at.\nToday, we‚Äôll focus on: - matplotlib: the classic - pandas: built on matplotlib, quick plotting built in to DataFrames - seaborn: built on matplotlib, adds functionality for fancy statistical plots - altair: interactive, relying on javascript plotting library Vega\nAnd next week for geospatial data: - holoviews/geoviews - matplotlib/cartopy - geopandas/geopy\nGoal: introduce you to the most common tools and enable you to know the best package for the job in the future"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#the-classic-matplotlib",
    "href": "content/week-2/lecture-2B.html#the-classic-matplotlib",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "The classic: matplotlib",
    "text": "The classic: matplotlib\n\nVery well tested, robust plotting library\nCan reproduce just about any plot (sometimes with a lot of effort)\n\n\n\nWith some downsides‚Ä¶\n\nImperative, overly verbose syntax\nLittle support for interactive/web graphics\n\n\n\nAvailable functionality\n\nDon‚Äôt need to memorize syntax for all of the plotting functions\nExample gallery: https://matplotlib.org/stable/gallery/index.html\nSee the cheat sheet available in this repository\n\n\n\nMost commonly used:\n\nSimple line plots: plot()\nMultiple axes per figure: subplot()\n2D image (RGB) data : imshow()\n2D arrays: pcolormesh()\nHistograms: hist()\nBar charts: bar()\nPie charts: pie()\nScatter plots: scatter()\n\n\n\nWorking with matplotlib\nWe‚Äôll use the object-oriented interface to matplotlib - Create Figure and Axes objects - Add plots to the Axes object - Customize any and all aspects of the Figure or Axes objects\n\nPro: Matplotlib is extraordinarily general ‚Äî you can do pretty much anything with it\nCon: There‚Äôs a steep learning curve, with a lot of matplotlib-specific terms to learn\n\n\n\nLearning the matplotlib language\n\nSource\n\n\nRecommended reading\n\nIntroduction to the object-oriented interface\nA good walk through on using matplotlib to customize plots\nListed in the README for this week‚Äôs repository too"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#lets-load-some-data-to-plot",
    "href": "content/week-2/lecture-2B.html#lets-load-some-data-to-plot",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Let‚Äôs load some data to plot‚Ä¶",
    "text": "Let‚Äôs load some data to plot‚Ä¶\nWe‚Äôll use the Palmer penguins data set, data collected for three species of penguins at Palmer station in Antartica\n\n\nArtwork by @allison_horst\n\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"./data/penguins.csv\")\n\n\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007\n\n\n\n\n\n\n\nData is already in tidy format"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#a-simple-visualization-3-different-ways",
    "href": "content/week-2/lecture-2B.html#a-simple-visualization-3-different-ways",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs.¬†bill length, colored by the penguin species"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#using-matplotlib",
    "href": "content/week-2/lecture-2B.html#using-matplotlib",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "1. Using matplotlib",
    "text": "1. Using matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group in penguins.groupby(\"species\"):\n    print(f\"Plotting {species}...\")\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group[\"flipper_length_mm\"],\n        group[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True)\n\nPlotting Adelie...\nPlotting Chinstrap...\nPlotting Gentoo..."
  },
  {
    "objectID": "content/week-2/lecture-2B.html#how-about-in-pandas",
    "href": "content/week-2/lecture-2B.html#how-about-in-pandas",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "2. How about in pandas?",
    "text": "2. How about in pandas?\nDataFrames have a built-in ‚Äúplot‚Äù function that can make all of the basic type of matplotlib plots!\n\n# Tab complete on the plot attribute of a dataframe to see the available functions\n#penguins.plot.\n\nFirst, we need to add a new ‚Äúcolor‚Äù column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=ax, # IMPORTANT: Make sure to plot on the axes object we created already!\n)\n\n# Format the axes finally\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\n\n\n\n\nNote: no easy way to get legend added to the plot in this case‚Ä¶\n\nDisclaimer\n\nIn my experience, I have found the pandas plotting capabilities are good for quick and unpolished plots during the data exploration phase\nMost of the pandas plotting functions serve as shorcuts, removing some biolerplate matplotlib code\nIf I‚Äôm trying to make polished, clean data visualization, I‚Äôll usually opt to use matplotlib from the beginning"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#seaborn-statistical-data-visualization",
    "href": "content/week-2/lecture-2B.html#seaborn-statistical-data-visualization",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "3. Seaborn: statistical data visualization",
    "text": "3. Seaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column‚Ä¶\n\n  \n\n\nimport seaborn as sns\n\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\", # the x column\n    y=\"bill_length_mm\", # the y column\n    hue=\"species\", # the third dimension (color)\n    data=penguins, # pass in the data\n    ax=ax, # plot on the axes object we made\n    **style # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x1572e6170&gt;\n\n\n\n\n\n\n\n\n\n\n\nSide note: the **kwargs syntax\n\n\n\nThe ** syntax is the unpacking operator. It will unpack the dictionary and pass each keyword to the function.\nSo the previous code is the same as:\nsns.scatterplot(\n    x=\"flipper_length_mm\", \n    y=\"bill_length_mm\", \n    hue=\"species\",\n    data=penguins, \n    ax=ax, \n    palette=color_map, # defined in the style dict \n    edgecolor=\"none\", # defined in the style dict\n    alpha=0.5 # defined in the style dict\n)\nBut we can use **style as a shortcut!\n\n\n\nAn aside: the seaborn objects interface\nSeaborn recently introduced an ‚Äúobjects‚Äù interface, a completely new syntax that aims to be more declarative. It hides the interaction with matplotlib for the user and provides an more intuitive way to customize charts.\nYou‚Äôll see a lot of similarities between the ‚Äúobjects‚Äù interface and the next library we will talk about: altair.\n\n\n\n\n\n\nNote\n\n\n\nSince it‚Äôs so new and not yet finalized, we won‚Äôt recommend using it during this course. However, we wanted to make sure you‚Äôre aware of it as it could be a good option in the future. More info can be found on seaborn‚Äôs documentation.\n\n\nAs a reference, our scatterplot example would look like this in the ‚Äúobjects‚Äù interface:\n\nimport seaborn.objects as so\n\n\n(\n    so.Plot(x=\"flipper_length_mm\", y=\"bill_length_mm\", color=\"species\", data=penguins)\n    .add(so.Dot())\n    .scale(color=color_map)\n    .layout(size=(10, 6))\n    .label(x=\"Flipper Length (mm)\", y=\"Bill Length (mm)\")\n    # Warning: this theme syntax is not yet finalized!\n    .theme({\"axes.facecolor\": \"w\", \"axes.edgecolor\": \"k\"})\n)\n\n\n\n\n\n\nMany more functions available\nIn general, seaborn is fantastic for visualizing relationships between variables in a more quantitative way\nDon‚Äôt memorize every function‚Ä¶\nI always look at the beautiful Example Gallery for ideas.\n\nHow about adding linear regression lines?\nUse lmplot()\n\nsns.lmplot(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    hue=\"species\",\n    data=penguins,\n    height=6,\n    aspect=1.5,\n    palette=color_map,\n    scatter_kws=dict(edgecolor=\"none\", alpha=0.5),\n);\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nHow about the smoothed 2D distribution?\nUse jointplot()\n\nsns.jointplot(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    data=penguins,\n    height=8,\n    kind=\"kde\",\n    cmap=\"viridis\",\n);\n\n\n\n\n\n\nHow about comparing more than two variables at once?\nUse pairplot()\n\n# The variables to plot\nvariables = [\n    \"species\",\n    \"bill_length_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"bill_depth_mm\",\n]\n\n# Set the seaborn style\nsns.set_context(\"notebook\", font_scale=1.5)\n\n# make the pair plot\nsns.pairplot(\n    penguins[variables].dropna(),\n    palette=color_map,\n    hue=\"species\",\n    plot_kws=dict(alpha=0.5, edgecolor=\"none\"),\n)\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nLet‚Äôs explore the bill length differences across species and gender\nWe can use seaborn‚Äôs functionality for exploring categorical data sets: catplot()\n\nsns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=penguins);\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\nSeaborn tutorials broken down by data type\n\nTutorial landing page\n\nVisualizing statistical relationships\nCategorical data\nVisualizing the distribution of a data set\nVisualizing linear relationships\n\n\n\n\nColor palettes in seaborn\nGreat tutorial available in the seaborn documentation\n\n\n\n\n\n\nTip\n\n\n\nThe color_palette() function in seaborn is very useful. For me, it is the easiest way to get a list of hex strings for a specific color map.\n\n\n\n# This is a list of hex strings values for the colors\nviridis = sns.color_palette(\"viridis\", n_colors=7).as_hex()\n\n# Print it out to see the list\nprint(viridis)\n\n['#472d7b', '#3b528b', '#2c728e', '#21918c', '#28ae80', '#5ec962', '#addc30']\n\n\nCan we preview the colors in JupyterLab?\n\n# Option 1: Use the sns.palplot() function to make a matplotlib figure\nsns.palplot(viridis)\n\n\n\n\n\n# Option 2: If you output it from a cell, JupyterLab automatically renders it\nviridis\n\n\n\n\nYou can also create custom light, dark, or diverging color maps, based on the desired hues at either end of the color map.\n\nsns.diverging_palette(10, 220, sep=50, n=7)"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#altair-declarative-data-viz-in-python",
    "href": "content/week-2/lecture-2B.html#altair-declarative-data-viz-in-python",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "4. Altair: Declarative Data Viz in Python",
    "text": "4. Altair: Declarative Data Viz in Python\n\n\n\nDocumentation available at: https://altair-viz.github.io/\n\nThe altair import statement\n\nimport altair as alt  \n\n\n\nA visualization grammar\n\nSpecify what should be done\nDetails determined automatically\nCharts are really just visualization specifications and the data to make the plot\nRelies on vega and vega-lite\n\nImportant: focuses on tidy data ‚Äî you‚Äôll often find yourself running pd.melt() to get to tidy format\nLet‚Äôs try out our flipper length vs bill length example:\n\n# Step 1: Initialize the chart with the data\nchart = alt.Chart(penguins)\n\n# Step 2: Define what kind of marks to use\nchart = chart.mark_circle(size=60)\n\n# Step 3: Encode the visual channels\nchart = chart.encode(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    color=\"species\", \n    tooltip=[\"species\", \"flipper_length_mm\", \"bill_length_mm\", \"island\", \"sex\"],\n)\n\n# Optional: Make the chart interactive\nchart.interactive()\n\n\n\n\n\n\n\n\n\nAltair shorcuts\n\nThere are built-in objects to represent ‚Äúx‚Äù, ‚Äúy‚Äù, ‚Äúcolor‚Äù, ‚Äútooltip‚Äù, etc..\nUsing the object syntax allows your to customize how different elements behave\n\nExample: previous code is the same as\nchart = chart.encode(\n    x=alt.X(\"flipper_length_mm\"),\n    y=alt.Y(\"bill_length_mm\"),\n    color=alt.Color(\"species\"),\n    tooltip=alt.Tooltip([\"species\", \"flipper_length_mm\", \"bill_length_mm\", \"island\", \"sex\"]),\n)\n\n\nChanging Altair chart axis limits\n\nBy default, Altair assumes the axis will start at 0\nTo center on the data automatically, we need to use a alt.Scale() object to specify the scale\n\n\n# initialize the chart with the data\nchart = alt.Chart(penguins)\n\n# define what kind of marks to use\nchart = chart.mark_circle(size=60)\n\n# encode the visual channels\nchart = chart.encode(\n    x=alt.X(\"flipper_length_mm\", scale=alt.Scale(zero=False)), # This part is new!\n    y=alt.Y(\"bill_length_mm\", scale=alt.Scale(zero=False)), # This part is new!\n    color=\"species\",\n    tooltip=[\"species\", \"flipper_length_mm\", \"bill_length_mm\", \"island\", \"sex\"],\n)\n\n# make the chart interactive\nchart = chart.interactive()\n\nchart\n\n\n\n\n\n\n\n\n\nEncodings\n\nX: x-axis value\nY: y-axis value\nColor: color of the mark\nOpacity: transparency/opacity of the mark\nShape: shape of the mark\nSize: size of the mark\nRow: row within a grid of facet plots\nColumn: column within a grid of facet plots\n\nFor a complete list of these encodings, see the Encodings section of the documentation.\nAltair charts can be fully specified as JSON \\(\\rightarrow\\) easy to embed in HTML on websites!\n\n# Save the chart as a JSON string!\njson = chart.to_json()\n\n\n# Print out the first 1,000 characters\nprint(json[:1000])\n\n{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.14.1.json\",\n  \"config\": {\n    \"view\": {\n      \"continuousHeight\": 300,\n      \"continuousWidth\": 300\n    }\n  },\n  \"data\": {\n    \"name\": \"data-6e6be28484bfcb7bdf9764c3163fc5aa\"\n  },\n  \"datasets\": {\n    \"data-6e6be28484bfcb7bdf9764c3163fc5aa\": [\n      {\n        \"bill_depth_mm\": 18.7,\n        \"bill_length_mm\": 39.1,\n        \"body_mass_g\": 3750.0,\n        \"color\": \"#1f77b4\",\n        \"flipper_length_mm\": 181.0,\n        \"island\": \"Torgersen\",\n        \"sex\": \"male\",\n        \"species\": \"Adelie\",\n        \"year\": 2007\n      },\n      {\n        \"bill_depth_mm\": 17.4,\n        \"bill_length_mm\": 39.5,\n        \"body_mass_g\": 3800.0,\n        \"color\": \"#1f77b4\",\n        \"flipper_length_mm\": 186.0,\n        \"island\": \"Torgersen\",\n        \"sex\": \"female\",\n        \"species\": \"Adelie\",\n        \"year\": 2007\n      },\n      {\n        \"bill_depth_mm\": 18.0,\n        \"bill_length_mm\": 40.3,\n        \"body_mass_g\": 3250.0,\n        \"color\": \"#1f77b4\",\n        \n\n\n\n\nPublishing the visualization online\n\nchart.save(\"chart.html\")\n\n\n# Display IFrame in IPython\nfrom IPython.display import IFrame\nIFrame('chart.html', width=600, height=375)\n\n\n        \n        \n\n\n\n\nWatch out for large datasets!\nNote that the data is embedded inside the JSON representation of the chart. That means that if you pass a DataFrame to your chart with a lot of data, your browser might be overwhelmed and everything might freeze. To avoid this, altair will throw an error if your DataFrame has more than 5,000 rows.\nThere are a number of strategies outlined on the docs for dealing with larger datasets. One is to simply disable the max rows check ‚Äî this could be a good idea if your dataset is just a bit larger than the limit.\nalt.data_transformers.disable_max_rows()\nAnother strategy is to use the more flexible ‚Äúvegafusion‚Äù library, which has improved implementations of data transformations and allows charts with data up to 100,000 rows. You can enable this transformer with:\nalt.data_transformers.enable(\"vegafusion\")\n\n\n\n\n\n\nNote\n\n\n\nIf you get an error about missing packages, make sure you update your course environment to the latest version. See the instructions here.\n\n\n\n\nUsually, the function calls are chained together\nSurround your code with parentheses, and put each line of code on a new line\n\nchart = (\n    alt.Chart(penguins)\n    .mark_circle(size=60)\n    .encode(\n        x=alt.X(\"flipper_length_mm\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"bill_length_mm\", scale=alt.Scale(zero=False)),\n        color=\"species:N\",\n    )\n    .interactive()\n)\n\nchart\n\n\n\n\n\n\n\nNote that the interactive() call allows users to pan and zoom.\nAltair is able to automatically determine the type of the variable using built-in heuristics. Altair and Vega-Lite support four primitive data types:\n\n\n\n\nData Type\n\n\nCode\n\n\nDescription\n\n\n\n\nquantitative\n\n\nQ\n\n\nNumerical quantity (real-valued)\n\n\n\n\nnominal\n\n\nN\n\n\nName / Unordered categorical\n\n\n\n\nordinal\n\n\nO\n\n\nOrdered categorial\n\n\n\n\ntemporal\n\n\nT\n\n\nDate/time\n\n\n\n\nYou can set the data type of a column explicitly using a one letter code attached to the column name with a colon:\n\n\nFaceting\nEasily create multiple views of a dataset with faceting\n\n(\n    alt.Chart(penguins)\n    .mark_point()\n    .encode(\n        x=alt.X(\"flipper_length_mm:Q\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"bill_length_mm:Q\", scale=alt.Scale(zero=False)),\n        color=\"species:N\",\n    )\n    .properties(width=200, height=200)\n    .facet(column=\"species\")\n    .interactive()\n)\n\n\n\n\n\n\n\nNote: I‚Äôve added the variable type identifiers (Q, N) to the previous example\nLots of features to create compound charts: repeated charts, faceted charts, vertical and horizontal stacking of subplots.\nSee the documentation for examples\n\n\nA grammar of interaction\nA relatively new addition to altair, vega, and vega-lite. This allows you to define what happens when users interact with your visualization.\n\n\n\n\n\n\nTip\n\n\n\nI highly recommend reading through the documentation section on interactive charts. Altair‚Äôs interaction language is very complex and you can do a lot, including adding widgets (e.g., sliders) and multiple kinds of selection windows.\n\n\n\n\nA faceted plot, now with interaction!\n\n# Create the selection box\nbrush = alt.selection_interval()\n\n\n(\n    alt.Chart(penguins)  # Create the chart\n    .mark_point()  # Use point markers\n    .encode(  # Encode\n        x=alt.X(\"flipper_length_mm\", scale=alt.Scale(zero=False)),  # X\n        y=alt.Y(\"bill_length_mm\", scale=alt.Scale(zero=False)),  # Y\n        # Use a conditional color based on brush\n        color=alt.condition(brush, \"species\", alt.value(\"lightgray\")),  # Color\n        tooltip=[\"species\", \"flipper_length_mm\", \"bill_length_mm\"],  # Tooltip\n    )\n    .add_params(brush)  # Add brush parameter\n    .properties(width=200, height=200)  # Set width/height\n    .facet(column=\"species\") # Facet\n)\n\n\n\n\n\n\n\n\n\nMore on conditions\nWe used the alt.condition() function to specify a conditional color for the markers. It takes three arguments:\n\nThe brush object determines if a data point is currently selected\nIf inside the brush, color the marker according to the ‚Äúspecies‚Äù column\nIf outside the brush, use the literal hex color ‚Äúlightgray‚Äù\n\n\n\nSelecting across multiple variables\nLet‚Äôs examine the relationship between flipper_length_mm, bill_length_mm, and body_mass_g\nWe‚Äôll use a repeated chart that repeats variables across rows and columns.\nUse a conditional color again, based on a brush selection.\n\n# Setup the selection brush\nbrush = alt.selection(type=\"interval\", resolve=\"global\")\n\n# Setup the chart\n(\n    alt.Chart(penguins)\n    .mark_circle()\n    .encode(\n        x=alt.X(alt.repeat(\"column\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        y=alt.Y(alt.repeat(\"row\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        color=alt.condition(\n            brush, \"species:N\", alt.value(\"lightgray\")\n        ),  # conditional color\n    )\n    .properties(\n        width=200,\n        height=200,\n    )\n    .add_params(brush)\n    .repeat(  # repeat variables across rows and columns\n        row=[\"flipper_length_mm\", \"bill_length_mm\", \"body_mass_g\"],\n        column=[\"body_mass_g\", \"bill_length_mm\", \"flipper_length_mm\"],\n    )\n)\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'selection' is deprecated.\n   Use 'selection_point()' or 'selection_interval()' instead; these functions also include more helpful docstrings.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n\n\n\n\n\n\n\n\n\n\nMore exploratory visualization\nLet‚Äôs try out some more features of Altair‚Ä¶these examples are meant as reference for you to showcase some common features.\n\n\n\n\n\n\nReminder\n\n\n\nThe Example Gallery is a great place to learn the full functionality of Altair and includes a lot of great examples!\n\n\n\nExample 1: Color schemes\n\nScatter flipper length vs body mass for each species, colored by sex\n\n\n(\n    alt.Chart(penguins)\n    .mark_point()\n    .encode(\n        x=alt.X(\"flipper_length_mm\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"body_mass_g\", scale=alt.Scale(zero=False)),\n        color=alt.Color(\"sex:N\", scale=alt.Scale(scheme=\"set2\")),\n    )\n    .properties(width=400, height=150)\n    .facet(row=\"species\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôve specified the scale keyword to the alt.Color() object and passed a scheme value:\nscale=alt.Scale(scheme=\"set2\")\n\nThe scheme ‚Äúset2‚Äù is a Color Brewer color. The available color schemes are very similar to those matplotlib. A list is available on the Vega documentation: https://vega.github.io/vega/docs/schemes/.\n\n\n\n\nExample 2: Histogram aggregations with count\n\nNext, plot the total number of penguins per species by the island they are found on.\n\n\n(\n    alt.Chart(penguins)\n    .mark_bar()\n    .encode(\n        # X should show the (normalized) count of each group\n        x=alt.X(\"*:Q\", aggregate=\"count\", stack=\"normalize\"), # The * is a placeholder here\n        y=\"island:N\",\n        color=\"species:N\",\n        tooltip=[\"island\", \"species\", \"count(*):Q\"],\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI like to think of altair aggregations in terms of the pandas groupby syntax. Under the hood, altair is going to group our data by the other encodings we specified, ‚Äúisland‚Äù and ‚Äúspecies‚Äù. The dimension (‚ÄúX‚Äù) that gets specified as the count aggregation is then the size of each of those groups.\n\n\n\n\nExample 3: The count() shorthand\n\nPlot a histogram of number of penguins by flipper length, grouped by species.\n\n\n(\n    alt.Chart(penguins)\n    .mark_bar()\n    .encode(\n        x=alt.X(\"flipper_length_mm\", bin=alt.Bin(maxbins=20)),\n        y=\"count():Q\",  # Shorthand\n        color=\"species\",\n        tooltip=[\"species\", alt.Tooltip(\"count()\", title=\"Number of Penguins\")],\n    )\n    .properties(height=250)\n)\n\n\n\n\n\n\n\n\n\nExample 4: Binning data and using the mean aggregation\n\nFinally, let‚Äôs bin the data by body mass and plot the average flipper length per bin, colored by the species.\n\nIn this example, we use a ‚Äúbinning‚Äù transformation to bin the data along a certain encoding (‚ÄúX‚Äù in this case), and then we will take the mean along the ‚ÄúY‚Äù encoding.\n\n(\n    alt.Chart(penguins.dropna())\n    .mark_line()\n    .encode(\n        x=alt.X(\"body_mass_g:Q\", bin=alt.Bin(maxbins=10)), # Bin the data!\n        y=alt.Y(\n            \"mean(flipper_length_mm):Q\", scale=alt.Scale(zero=False) # Mean of flipper length\n        ),  \n        color=\"species:N\",\n        tooltip=[\"mean(flipper_length_mm):Q\", \"count():Q\"],\n    )\n    .properties(height=300, width=500)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn addition to mean() and count(), you can apply a number of different transformations to the data before plotting, including binning, arbitrary functions, and filters.\nSee the Data Transformations section of the user guide for more details.\n\n\n\n\n\nDashboards become easy to make‚Ä¶\n\n# Setup a brush selection\nbrush = alt.selection(type=\"interval\")\n\n# The top scatterplot: flipper length vs bill length\npoints = (\n    alt.Chart()\n    .mark_point()\n    .encode(\n        x=alt.X(\"flipper_length_mm:Q\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"bill_length_mm:Q\", scale=alt.Scale(zero=False)),\n        color=alt.condition(brush, \"species:N\", alt.value(\"lightgray\")),\n    )\n    .properties(width=800)\n    .add_params(brush)\n)\n\n# The bottom bar plot\nbars = (\n    alt.Chart()\n    .mark_bar()\n    .encode(\n        x=\"count(species):Q\",\n        y=\"species:N\",\n        color=\"species:N\",\n    )\n    .transform_filter(\n        brush  # the filter transform uses the selection to filter the input data to this chart\n    )\n    .properties(width=800)\n)\n\n# Final chart is a vertical stack\nchart = alt.vconcat(points, bars, data=penguins)\n\n# Output the chart\nchart"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#exercise-visualizing-the-impact-of-the-measles-vaccination",
    "href": "content/week-2/lecture-2B.html#exercise-visualizing-the-impact-of-the-measles-vaccination",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "Exercise: Visualizing the impact of the measles vaccination",
    "text": "Exercise: Visualizing the impact of the measles vaccination\nLet‚Äôs reproduce this famous Wall Street Journal visualization showing measles incidence over time using altair.\nhttp://graphics.wsj.com/infectious-diseases-and-vaccines/\n\nStep 1: Load the data\n\n# Note we are using a relative path\npath = \"./data/measles_incidence.csv\"\n\n# Skip first two rows and convert \"-\" to NaN automatically\ndata = pd.read_csv(path, skiprows=2, na_values=\"-\")\n\ndata.head()\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows √ó 53 columns\n\n\n\nNote: the data is weekly and in wide format\n\n\nStep 2: Calculate the total incidents in a given year per state\n\n\n\n\n\n\nHints\n\n\n\n\nYou‚Äôll want to take the sum over weeks to get the annual total ‚Äî¬†you can take advantage of the groupby() then sum() work flow.\nIt will be helpful if you drop the WEEK column ‚Äî¬†you don‚Äôt need that in the grouping operation. Take a look at the df.drop(columns=[]) function (docs).\n\n\n\n\n\nStep 3: Transform to tidy format\n\n\n\n\n\n\nHints\n\n\n\nYou can use melt() to get tidy data. You should have 3 columns: year, state, and total incidence.\n\n\n\n\nStep 4: Make the plot\n\n\n\n\n\n\nHints\n\n\n\n\nTake a look at this heatmap example for an example of the syntax of Altair‚Äôs heatmap functionality.\nYou can use the mark_rect() function to encode the values as rectangles and then color them according to the average annual measles incidence per state.\nYou‚Äôll want to take advantage of the custom color map defined below to best match the WSJ‚Äôs graphic.\n\n\n\n\n# Define a custom colormape using Hex codes & HTML color names\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n\n\nChallenges\n\nDo you agree with the visualization choices made by the WSJ?\n\nTry experimenting with different color scales to see if you can improve the heatmap\nSee the names of available color maps in Altair\n\nTry adding a second chart above the heatmap that shows a line chart of the annual average across all 50 states.\n\n\nChallenge #1: Exploring other color maps\nThe categorical color scale choice is properly not the best. It‚Äôs best to use a perceptually uniform color scale like viridis. See below:\n\n\nChallenge #2: Add the annual average chart on top"
  },
  {
    "objectID": "content/week-2/lecture-2B.html#thats-it",
    "href": "content/week-2/lecture-2B.html#thats-it",
    "title": "Week 2B: Data Visualization Fundamentals",
    "section": "That‚Äôs it!",
    "text": "That‚Äôs it!\n\nHW #1 due on Wednesday Sept 20 before the end of the day\nWrapping up altair and geospatial analysis and visualization next week!"
  },
  {
    "objectID": "content/week-3/lecture-3B.html",
    "href": "content/week-3/lecture-3B.html",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "",
    "text": "Section 401\nSep 20, 2023"
  },
  {
    "objectID": "content/week-3/lecture-3B.html#housekeeping",
    "href": "content/week-3/lecture-3B.html#housekeeping",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nHomework #1 due on Monday (9/25)\nHomework #2 will be assigned same day\nChoose a dataset to visualize and explore\n\nOpenDataPhilly or one your choosing\nEmail me if you want to analyze one that‚Äôs not on OpenDataPhilly"
  },
  {
    "objectID": "content/week-3/lecture-3B.html#agenda-for-today",
    "href": "content/week-3/lecture-3B.html#agenda-for-today",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "Agenda for today",
    "text": "Agenda for today\n\nIntroduction to GeoPandas and vector data\nSpatial relationships and joins\n\n\n\n\n\n\n\nNote\n\n\n\nWe‚Äôll continue geospatial analysis next week, with: - Visualization for geospatial data - Demo: 311 requests by neighborhood in Philadelphia - Exercise: Property assessments by neighborhood\n\n\n\n# Let's setup the imports we'll need first\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline"
  },
  {
    "objectID": "content/week-3/lecture-3B.html#part-1-vector-geospatial-data-in-python",
    "href": "content/week-3/lecture-3B.html#part-1-vector-geospatial-data-in-python",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "Part 1: Vector geospatial data in Python",
    "text": "Part 1: Vector geospatial data in Python\n\nVector refers to discrete geometric entities\nThe Open Geospatial Consortium has standardized a set of simple features\nIncludes points, lines, and polygons\n\n\n\n\nSource\n\nA couple of terminology notes\n\nA feature refers to both the geometry and attributes of specific piece of vector data\nA feature collection is a list, or collection, of features\n\nBoth terms are very common in Python geospatial software.\n\n\nCommon formats for vector datasets\n\n1. A shapefile\nActually several files with the same common prefix.\nMandatory files:\n\n.shp: the file containing the geometries\n.shx: the file that indexes the geometry\n.dbf: tabular data format storing the attributes for each geometry\n\nAnd many optional files for documentation, projection information, etc.\nLet‚Äôs take a look at an example shapefile:\nWe‚Äôll use the %ls command to list out all of the files in an example shapefile in the data/ folder\n\n%ls \"data/ne_110m_admin_0_countries/\"\n\nne_110m_admin_0_countries.cpg  ne_110m_admin_0_countries.shp\nne_110m_admin_0_countries.dbf  ne_110m_admin_0_countries.shx\nne_110m_admin_0_countries.prj\n\n\n\n\n2. The GeoJSON file\n\nStores simple features in a JSON format\nArose due to the prevalence of the JSON format, especially on the web\n\n\n\n\n\n\n\nAdditional GeoJSON resources\n\n\n\n\nGitHub lets you view GeoJSON files natively. See Philadelphia ZIP Codes in the data/ folder of this week‚Äôs repo.\nhttp://geojson.io provides interactive creation and viewing of small GeoJSON files\n\n\n\n\n\n\nAnalyzing vector data with GeoPandas\ngeopandas provides a simple, intuitive for the main types of geospatial vector file formats\n\n\n# The import statement\nimport geopandas as gpd\n\nExample: Let‚Äôs load a shape file of countries in the world‚Ä¶\nSource: Natural Earth Data\nWe can use the read_file() function to read shapefiles and GeoJSON files.\n\n# Read the shape file, giving the name of the directory\ncountries = gpd.read_file(\"./data/ne_110m_admin_0_countries\")\n\n\ncountries.head()\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n0\nAFG\nAfghanistan\nAsia\n34124811.0\n64080.0\nPOLYGON ((61.21082 35.65007, 62.23065 35.27066...\n\n\n1\nAGO\nAngola\nAfrica\n29310273.0\n189000.0\nMULTIPOLYGON (((23.90415 -11.72228, 24.07991 -...\n\n\n2\nALB\nAlbania\nEurope\n3047987.0\n33900.0\nPOLYGON ((21.02004 40.84273, 20.99999 40.58000...\n\n\n3\nARE\nUnited Arab Emirates\nAsia\n6072475.0\n667200.0\nPOLYGON ((51.57952 24.24550, 51.75744 24.29407...\n\n\n4\nARG\nArgentina\nSouth America\n44293293.0\n879400.0\nMULTIPOLYGON (((-66.95992 -54.89681, -67.56244...\n\n\n\n\n\n\n\n\ntype(countries)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\nWhat‚Äôs a GeoDataFrame?\nJust like a DataFrame but with a new, special geometry column:\n\nPrint out the first 10 entires of the ‚Äúgeometry‚Äù column:\n\ncountries[\"geometry\"].head(n=10)\n\n0    POLYGON ((61.21082 35.65007, 62.23065 35.27066...\n1    MULTIPOLYGON (((23.90415 -11.72228, 24.07991 -...\n2    POLYGON ((21.02004 40.84273, 20.99999 40.58000...\n3    POLYGON ((51.57952 24.24550, 51.75744 24.29407...\n4    MULTIPOLYGON (((-66.95992 -54.89681, -67.56244...\n5    POLYGON ((43.58275 41.09214, 44.97248 41.24813...\n6    MULTIPOLYGON (((-59.57209 -80.04018, -59.86585...\n7    POLYGON ((68.93500 -48.62500, 69.58000 -48.940...\n8    MULTIPOLYGON (((145.39798 -40.79255, 146.36412...\n9    POLYGON ((16.97967 48.12350, 16.90375 47.71487...\nName: geometry, dtype: geometry\n\n\nTake a look at the first geometry polygon by using the .iloc[] selector:\n\ncountries.geometry.iloc[0]\n\n\n\n\n\n\nWe can still leverage the power of pandas‚Ä¶\nCalculate the total world population:\n\ncountries[\"pop_est\"].sum() / 1e9  # In billions\n\n7.383089462\n\n\nCalculate the total population on each continent:\n\ngrouped = countries.groupby(\"continent\")\n\ngrouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x13fdcd120&gt;\n\n\n\n\n\n\n\n\nRemember\n\n\n\nThe groupby() function does not return a DataFrame ‚Äî you need to call sum(), mean() etc, or apply() a function to get your desired result!\n\n\nAccess the ‚Äúpop_est‚Äù column from the groupby variable (‚Äúgrouped‚Äù) and then call the .sum() function to calculate our desired statistic:\n\npop_by_continent = grouped[\"pop_est\"].sum()\n\npop_by_continent\n\ncontinent\nAfrica                     1.219176e+09\nAntarctica                 4.050000e+03\nAsia                       4.389145e+09\nEurope                     7.463985e+08\nNorth America              5.730421e+08\nOceania                    3.678284e+07\nSeven seas (open ocean)    1.400000e+02\nSouth America              4.185407e+08\nName: pop_est, dtype: float64\n\n\n\n# Sort values\npop_by_continent.sort_values(ascending=False, inplace=True)\n\n# Output sorted values from cell\npop_by_continent / 1e9\n\ncontinent\nAsia                       4.389145e+00\nAfrica                     1.219176e+00\nEurope                     7.463985e-01\nNorth America              5.730421e-01\nSouth America              4.185407e-01\nOceania                    3.678284e-02\nAntarctica                 4.050000e-06\nSeven seas (open ocean)    1.400000e-07\nName: pop_est, dtype: float64\n\n\nFilter the data frame based on a boolean selection:\n\n# Is the country name USA?\nis_USA = countries[\"name\"] == \"United States of America\"\n\n# Get the row with USA\nUSA = countries.loc[is_USA]\n\nUSA\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n168\nUSA\nUnited States of America\nNorth America\n326625791.0\n18560000.0\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n\n\n\n\nUSA.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nReminder: the squeeze() function\n\n\n\nThe squeeze() function does just one it sounds like: if you have a DataFrame with only one row, it will ‚Äúsqueeze‚Äù the row dimension by removing it, returning just a Series object:\n\n\n\n# Squeeze\nUSA = USA.squeeze()\n\n# Print out the types\nprint(\"The type of USA is: \", type(USA))\n\n# Output\nUSA\n\nThe type of USA is:  &lt;class 'pandas.core.series.Series'&gt;\n\n\niso_a3                                                      USA\nname                                   United States of America\ncontinent                                         North America\npop_est                                             326625791.0\ngdp_md_est                                           18560000.0\ngeometry      MULTIPOLYGON (((-122.84000000000005 49.0000000...\nName: 168, dtype: object\n\n\nThe simple features (Lines, Points, Polygons) are implemented by the shapely library\n\ntype(USA.geometry)\n\nshapely.geometry.multipolygon.MultiPolygon\n\n\nJupyterLab renders shapely geometries automatically:\n\n# a mini USA\nUSA.geometry\n\n\n\n\n\n\nHow does geopandas handle coordinate systems and map projections?\nA coordinate reference system (CRS) relates the position of a geometry object on the spherical earth to its two-dimensional coordinates.\nA GeoDataFrame or GeoSeries has a .crs attribute which specifies the coordinate reference system.\n\ncountries.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nThe EPSG=4326 CRS\n\nEPSG 4326 is known as WGS 84 where x and y are longitude and latitude.\nIt is is the default coordinate system for GPS systems.\nIt‚Äôs also known as Plate Carr√©e or equirectangular\n\n\n\nHow to plot all of the geometries at once?\nUse the plot() function to get a quick and dirty plot of all of the geometry features.\nNote: the plot() returns the current maplotlib axes, allowing you to format the chart after plotting.\n\n# Create a figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the countries on our axes\nax = countries.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\n\n# Add a title\nax.set_title(\"Equirectangular Projection\");\n\n\n\n\n\n\n\n\n\nCan we convert to other coordinate systems?\nUse the df.to_crs() function! The most well-known projections can be specified by their EPSG code.\nGeopandas documentation on re-projecting: Managing Projections\n\nLet‚Äôs convert to the Mercator projection\nEPSG code: 3395\n\n# Remove Antartica, as the Mercator projection\n# cannot deal with the poles\nno_antartica = countries.loc[(countries[\"name\"] != \"Antarctica\")]\n\n\n# Two ways to specify the EPSG code\ncountries_mercator = no_antartica.to_crs(epsg=3395)\n\n# Alternatively:\n# countries_mercator = no_antartica.to_crs(\"EPSG:3395\")\n\n\ncountries_mercator.head()\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n0\nAFG\nAfghanistan\nAsia\n34124811.0\n64080.0\nPOLYGON ((6813956.990 4227673.562, 6927484.435...\n\n\n1\nAGO\nAngola\nAfrica\n29310273.0\n189000.0\nMULTIPOLYGON (((2660998.216 -1305442.810, 2680...\n\n\n2\nALB\nAlbania\nEurope\n3047987.0\n33900.0\nPOLYGON ((2339940.185 4961221.199, 2337708.178...\n\n\n3\nARE\nUnited Arab Emirates\nAsia\n6072475.0\n667200.0\nPOLYGON ((5741805.754 2765811.385, 5761611.935...\n\n\n4\nARG\nArgentina\nSouth America\n44293293.0\n879400.0\nMULTIPOLYGON (((-7453944.198 -7306880.704, -75...\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTake a look at the values in the ‚Äúgeometry‚Äù column above. The magnitude of the coordinates changed! This is a quick and easy way to tell if the re-projection worked properly!\n\n\n\n\nNow let‚Äôs plot it!\nThe easy way‚Ä¶with geopandas built-in plot() function\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use built-in plot() of the GeoDataFrame\nax = countries_mercator.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\n\n# Add a title\nax.set_title(\"Mercator Projection\");\n\n\n\n\n\n\n\n\n\nWhich one is your favorite?\n\nSource: xkcd\n\n\n\n\n\n\nTip: Choosing the right CRS\n\n\n\n\nFor city-based data, the Web Mercator CRS (EPSG=3857) is best. This CRS became popular after Google Maps adopted it.\nYou can also use a CRS specific to individual states, e.g., the PA State Plane EPSG=2272 is a good option for Philadelphia\n\n\n\n\n\n\nLet‚Äôs load the city limits for Philadelphia\nWe‚Äôll use the provided City_Limits shape file in the data/ folder\n\ncity_limits = gpd.read_file(\"./data/City_Limits\")\n\ncity_limits\n\n\n\n\n\n\n\n\nOBJECTID\nShape__Are\nShape__Len\ngeometry\n\n\n\n\n0\n1\n0.038911\n1.259687\nPOLYGON ((-75.01497 40.13793, -75.01456 40.137...\n\n\n\n\n\n\n\n\nWhat‚Äôs the CRS?\nUse the .crs attribute to find out!\n\ncity_limits.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nPlot it with the EPSG=4326 CRS:\n\n# Create our figure and axes\nfig, ax = plt.subplots(figsize=(5, 5))\n\n# Plot\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\n\n# Format\nax.set_title(\"Equirectangular\")\nax.set_axis_off()  # This will remove the axes completely\nax.set_aspect(\"equal\")  # This forces an equal aspect ratio\n\n\n\n\nThis is not what Philadelphia looks like..\nLet‚Äôs try EPSG=3857 instead:\n\n# Create the figure\nfig, ax = plt.subplots(figsize=(5, 5))\n\n# Convert to EPSG:3857\ncity_limits_3857 = city_limits.to_crs(epsg=3857)\n\n# Plot and format\ncity_limits_3857.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\n\nax.set_title(\"Web Mercator\")\nax.set_axis_off()\nax.set_aspect(\"equal\");\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe equirectangular CRS (EPSG=4326) is often used by default and will make cities appear wider and flatter than they really are.\n\n\n\n\nSaving GeoDataFrames\nUse the to_file() function and specify the driver.\n\n# ESRI shape file\ncity_limits_3857.to_file(\"./data/city_limits_3857\", driver=\"ESRI Shapefile\")\n\n\n# GeoJSON is also an option\ncity_limits_3857.to_file(\"./data/city_limits_3857.geojson\", driver=\"GeoJSON\")\n\n\n\nHow about as a CSV file?\nYes, but reading requires more work‚Ä¶\n\n# save a csv file\ncity_limits_3857.to_csv(\"./data/city_limits_3857.csv\", index=False)\n\n\ndf = pd.read_csv(\"./data/city_limits_3857.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nOBJECTID\nShape__Are\nShape__Len\ngeometry\n\n\n\n\n0\n1\n0.038911\n1.259687\nPOLYGON ((-8350627.97509646 4886006.88680784, ...\n\n\n\n\n\n\n\n\n\nLooks similar‚Ä¶\nBut, the ‚Äúgeometry‚Äù column is just stored as a string‚Ä¶it‚Äôs not a shapely Polygon\n\ntype(df.geometry)\n\npandas.core.series.Series\n\n\n\n\nUse shapely to parse the string version of the polygons\n\nfrom shapely import wkt\n\n# wkt.loads will convert from string to Polygon object\ndf[\"geometry\"] = df[\"geometry\"].apply(wkt.loads)\n\n\ndf.geometry.iloc[0]\n\n\n\n\nSuccess!\n\n\nConverting from a DataFrame to a GeoDataFrame\nWe can initialize the GeoDataFrame directly from a DataFrame but we need to specify two things:\n\nThe name of the ‚Äúgeometry‚Äù column\nThe CRS of the ‚Äúgeometry‚Äù column\n\n\n\n\n\n\n\nWatch out!\n\n\n\nThe CRS that you specify when you create the GeoDataFrame must match the CRS of the data. It is not the CRS that you wish the data was in. If you would like to convert to a different CRS, you‚Äôll need to call the .to_crs() after creating your GeoDataFrame.\n\n\nIn this case, the geometry column was saved in Web Mercator EPSG=3857\n\n# Make specifying the name of the geometry column and CRS\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:3857\")\n\n# Now plot\nfig, ax = plt.subplots(figsize=(5, 5))\nax = gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\nLet‚Äôs convert back to 4326 and plot\nThe tilt should be a bit more obvious now‚Ä¶\n\nax = gdf.to_crs(epsg=4326).plot(facecolor=\"none\", edgecolor=\"black\")\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nI didn‚Äôt use plt.subplots() here to create a figure/axes ‚Äì I let geopandas automatically make one\nI‚Äôve chained together the to_crs() and .plot() functions in one line\nThe .plot() function returns the axes object that geopandas used to plot ‚Äî¬†this lets you customizes the axes after plotting\n\n\n\n\n\n\nSo, when should you use GeoPandas?\n\nFor exploratory data analysis and visualization, including in Jupyter notebooks\nPre-processing data to be fed into a desktop GIS program\nFor compact, readable, and reproducible code\nIf you‚Äôre comfortable with Pandas and/or R data frames.\n\n\n\nWhen it may not be the best tool:\n\nFor polished multilayer map creation ‚Äî one option is to use a desktop GIS like QGIS.\nIf you need very high performance ‚Äî¬†geopandas can be slow compared to other GIS software."
  },
  {
    "objectID": "content/week-3/lecture-3B.html#part-2-spatial-relationships-and-joins",
    "href": "content/week-3/lecture-3B.html#part-2-spatial-relationships-and-joins",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "Part 2: Spatial Relationships and Joins",
    "text": "Part 2: Spatial Relationships and Joins\nLet‚Äôs explore joins and merges between GeoDataFrames based on geospatial relationships‚Ä¶\n\n# Load some cities data from the data/ folder\ncities = gpd.read_file(\"./data/ne_110m_populated_places\")\n\n\ncities\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nVatican City\nPOINT (12.45339 41.90328)\n\n\n1\nSan Marino\nPOINT (12.44177 43.93610)\n\n\n2\nVaduz\nPOINT (9.51667 47.13372)\n\n\n3\nLobamba\nPOINT (31.20000 -26.46667)\n\n\n4\nLuxembourg\nPOINT (6.13000 49.61166)\n\n\n...\n...\n...\n\n\n238\nRio de Janeiro\nPOINT (-43.22697 -22.92308)\n\n\n239\nS√£o Paulo\nPOINT (-46.62697 -23.55673)\n\n\n240\nSydney\nPOINT (151.18323 -33.91807)\n\n\n241\nSingapore\nPOINT (103.85387 1.29498)\n\n\n242\nHong Kong\nPOINT (114.18306 22.30693)\n\n\n\n\n243 rows √ó 2 columns\n\n\n\n\n\n\n(Image by Krauss, CC BY-SA 3.0)\nAll of these operations are available as functions of a GeoDataFrame.\n\nA quick example\nWhat country is New York in?\nSpoiler: the USA\n\n# Select the Point representing New York City\nnew_york = cities.loc[cities[\"name\"] == \"New York\"].geometry.squeeze()\nnew_york\n\n\n\n\n\ntype(new_york)\n\nshapely.geometry.point.Point\n\n\n\ncountries.contains(new_york)\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n172    False\n173    False\n174    False\n175    False\n176    False\nLength: 177, dtype: bool\n\n\n\n# Find the country that contains New York\ncountries.loc[countries.contains(new_york)]\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n168\nUSA\nUnited States of America\nNorth America\n326625791.0\n18560000.0\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n\n\n\n\n# Get the geometry column of the country containing NYC\nUSA = countries.loc[countries.contains(new_york)].squeeze().geometry\nUSA\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe .loc[] function can take the index selector as the first argument, and the name of a column as a second argument (separated by a comma)\n\n\n\ntype(USA)\n\nshapely.geometry.multipolygon.MultiPolygon\n\n\n\n# Is New York within the USA?\nnew_york.within(USA)\n\nTrue\n\n\n\nUSA\n\n\n\n\n\n\nReferences\nThe different functions for checking spatial relationships:\n\nequals\ncontains\ncrosses\ndisjoint\nintersects\noverlaps\ntouches\nwithin\ncovers\n\nSee the shapely documentation for an overview of these methods.\n\n\n1. The spatial join: sjoin()\nSPATIAL JOIN = merging attributes from two geometry layers based on their spatial relationship\nDifferent parts of this operations:\n\nThe GeoDataFrame to which we want add information\nThe GeoDataFrame that contains the information we want to add\nThe spatial relationship we want to use to match both datasets (intersects, contains, within)\nThe type of join: left or inner join\n\nIn this case, we want to join the cities dataframe, containing Point geometries, with the information of the countries dataframe, containing Polygon geometries.\nTo match cities with countries, we‚Äôll use the within spatial relationship.\nThe geopandas.sjoin() function performs this operation:\n\njoined = gpd.sjoin(cities, countries, predicate=\"within\", how=\"left\")\n\n\njoined.head()\n\n\n\n\n\n\n\n\nname_left\ngeometry\nindex_right\niso_a3\nname_right\ncontinent\npop_est\ngdp_md_est\n\n\n\n\n0\nVatican City\nPOINT (12.45339 41.90328)\n79.0\nITA\nItaly\nEurope\n62137802.0\n2221000.0\n\n\n1\nSan Marino\nPOINT (12.44177 43.93610)\n79.0\nITA\nItaly\nEurope\n62137802.0\n2221000.0\n\n\n2\nVaduz\nPOINT (9.51667 47.13372)\n9.0\nAUT\nAustria\nEurope\n8754413.0\n416600.0\n\n\n3\nLobamba\nPOINT (31.20000 -26.46667)\n152.0\nSWZ\nSwaziland\nAfrica\n1467152.0\n11060.0\n\n\n4\nLuxembourg\nPOINT (6.13000 49.61166)\n97.0\nLUX\nLuxembourg\nEurope\n594130.0\n58740.0\n\n\n\n\n\n\n\nAs we can see above, the attributes of the cities (left) and countries (right) have been merged based on whether the city is inside the country.\nLet‚Äôs select cities in Italy only:\n\ncities_in_italy = joined.loc[joined[\"name_right\"] == \"Italy\"]\ncities_in_italy\n\n\n\n\n\n\n\n\nname_left\ngeometry\nindex_right\niso_a3\nname_right\ncontinent\npop_est\ngdp_md_est\n\n\n\n\n0\nVatican City\nPOINT (12.45339 41.90328)\n79.0\nITA\nItaly\nEurope\n62137802.0\n2221000.0\n\n\n1\nSan Marino\nPOINT (12.44177 43.93610)\n79.0\nITA\nItaly\nEurope\n62137802.0\n2221000.0\n\n\n226\nRome\nPOINT (12.48131 41.89790)\n79.0\nITA\nItaly\nEurope\n62137802.0\n2221000.0\n\n\n\n\n\n\n\n\n# Extract Italy\nitaly = countries.loc[countries[\"name\"] == \"Italy\"]\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 8))\nitaly.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n# Plot the first city in the joined data frame (Vatican City)\n# Use the same axes by passing in the ax=ax keyword\nax = cities_in_italy.plot(ax=ax, color=\"red\")\n\n\n\n\n\n\n2. Spatial overlay operation: overlay()\nWe can also perform the ‚Äújoin‚Äù operation on the geometries rather than just combining attributes.\nThe overlay() function combines geometries, e.g.¬†by taking the intersection of the geometries.\nSelect all countries in Africa:\n\nafrica = countries.loc[countries[\"continent\"] == \"Africa\"]\n\n\nafrica.head()\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n1\nAGO\nAngola\nAfrica\n29310273.0\n189000.0\nMULTIPOLYGON (((23.90415 -11.72228, 24.07991 -...\n\n\n11\nBDI\nBurundi\nAfrica\n11466756.0\n7892.0\nPOLYGON ((29.34000 -4.49998, 29.27638 -3.29391...\n\n\n13\nBEN\nBenin\nAfrica\n11038805.0\n24310.0\nPOLYGON ((2.69170 6.25882, 1.86524 6.14216, 1....\n\n\n14\nBFA\nBurkina Faso\nAfrica\n20107509.0\n32990.0\nPOLYGON ((2.15447 11.94015, 1.93599 11.64115, ...\n\n\n25\nBWA\nBotswana\nAfrica\n2214858.0\n35900.0\nPOLYGON ((29.43219 -22.09131, 28.01724 -22.827...\n\n\n\n\n\n\n\nWhat is the CRS?\n\nafrica.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Let's transform to a CRS that uses meters\n# instead of degrees (EPSG=3857)\nafrica = africa.to_crs(epsg=3857)\n\nafrica.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06¬∞S and 85.06¬∞N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nPlot it:\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nafrica.plot(ax=ax, facecolor=\"#b9f2b1\")\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n# Important CRS needs to match!\ncities_3857 = cities.to_crs(epsg=3857)\n\n\n# Create a copy of the GeoDataFrame\nbuffered_cities = cities_3857.copy()\n\n# Add a buffer region of 250 km around all cities\nbuffered_cities[\"geometry\"] = buffered_cities.buffer(250e3)\n\n\nPlot the difference of the two geometries\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Calculate the difference of the geometry sets\ndiff = gpd.overlay(\n    africa,\n    buffered_cities,\n    how=\"difference\",\n)\n\n# Plot\ndiff.plot(facecolor=\"#b9f2b1\", ax=ax)\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:77: RuntimeWarning: invalid value encountered in difference\n  return lib.difference(a, b, **kwargs)\n\n\n\n\n\n\n# Data attributes are the same as the first data frame (africa)\n# with an updated geometry column\ndiff.head()\n\n\n\n\n\n\n\n\niso_a3\nname\ncontinent\npop_est\ngdp_md_est\ngeometry\n\n\n\n\n0\nAGO\nAngola\nAfrica\n29310273.0\n189000.0\nMULTIPOLYGON (((2673464.087 -1449571.330, 2441...\n\n\n1\nBEN\nBenin\nAfrica\n11038805.0\n24310.0\nPOLYGON ((100138.898 1231805.081, 138422.412 1...\n\n\n2\nBFA\nBurkina Faso\nAfrica\n20107509.0\n32990.0\nMULTIPOLYGON (((100138.898 1231805.081, 26368....\n\n\n3\nBWA\nBotswana\nAfrica\n2214858.0\n35900.0\nPOLYGON ((3065120.801 -2659823.621, 3061281.52...\n\n\n4\nCAF\nCentral African Rep.\nAfrica\n5625118.0\n3206.0\nPOLYGON ((1792937.514 836963.765, 1813457.017 ...\n\n\n\n\n\n\n\n\n\nPlot the intersection of the two geometries\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# The intersection of the geometry sets\nintersection = gpd.overlay(africa, buffered_cities, how=\"intersection\")\n\n# Plot\nintersection.plot(ax=ax, facecolor=\"#b9f2b1\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/nhand/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\n\n\n\n\n\n\nRecap: spatial operations\n\nSpatial join: merge attributes from one data frame to another based on the spatial relationship\nSpatial overlay: creating new geometries based on spatial operation between both data frames (and not combining attributes of both data frames)"
  },
  {
    "objectID": "content/week-3/lecture-3B.html#example-exploring-trash-related-311-tickets-by-neighborhood-in-2020",
    "href": "content/week-3/lecture-3B.html#example-exploring-trash-related-311-tickets-by-neighborhood-in-2020",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "Example: Exploring trash-related 311 tickets by neighborhood in 2020",
    "text": "Example: Exploring trash-related 311 tickets by neighborhood in 2020\nLet‚Äôs test out our geospatial skills by exploring data for 311 requests in Philadelphia during the pandemic in 2020. We‚Äôll get started by summarizing the data by month and then dive into examining trends by neighborhood.\n\n\n\n\n\n\nFind out more‚Ä¶\n\n\n\nTrash collection was a big concern in Philadelphia when the pandemic began. Check out this article in the Philadelphia Inquirer for more details!\n\n\nFirst, let‚Äôs load our data from the data/ folder. I downloaded the data from OpenDataPhilly for tickets from 2020 and trimmed to those requests that have been categorized as ‚ÄúRubbish/Recyclable Material Collection‚Äù. These are tickets related to missed trash pickup by the City‚Äôs Streets Department.\n\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"data/trash_311_requests_2020.csv\",  # Use the file path relative to the current folder\n)\n\n\nprint(\"number of requests = \", len(trash_requests_df))\n\nnumber of requests =  47730\n\n\n\ntrash_requests_df.head()\n\n\n\n\n\n\n\n\nobjectid\nservice_request_id\nstatus\nstatus_notes\nservice_name\nservice_code\nagency_responsible\nservice_notice\nrequested_datetime\nupdated_datetime\nexpected_datetime\naddress\nzipcode\nmedia_url\nlat\nlon\n\n\n\n\n0\n8180042\n13269656\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-04-02 19:22:24\n2020-04-06 07:02:57\n2020-04-06 20:00:00\n624 FOULKROD ST\nNaN\nNaN\n40.034389\n-75.106518\n\n\n1\n8180043\n13266979\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-04-02 08:40:53\n2020-04-06 07:02:58\n2020-04-05 20:00:00\n1203 ELLSWORTH ST\nNaN\nNaN\n39.936164\n-75.163497\n\n\n2\n7744426\n13066443\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-02 19:17:55\n2020-01-04 05:46:06\n2020-01-06 19:00:00\n9054 WESLEYAN RD\nNaN\nNaN\n40.058737\n-75.018345\n\n\n3\n7744427\n13066540\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-03 07:01:46\n2020-01-04 05:46:07\n2020-01-06 19:00:00\n2784 WILLITS RD\nNaN\nNaN\n40.063658\n-75.022347\n\n\n4\n7801094\n13089345\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-15 13:22:14\n2020-01-16 14:03:29\n2020-01-16 19:00:00\n6137 LOCUST ST\nNaN\nNaN\n39.958186\n-75.244732\n\n\n\n\n\n\n\n\nFirst, convert to a GeoDataFrame\nRemove the requests missing lat/lon coordinates\n\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\nCreate Point objects for each lat and lon combination.\nWe can use the helper utility function: geopandas.points_from_xy()\n\ntrash_requests_df[\"geometry\"] = gpd.points_from_xy(\n    trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]\n)\n\n\ntrash_requests_df[\"geometry\"].head()\n\n0    POINT (-75.10652 40.03439)\n1    POINT (-75.16350 39.93616)\n2    POINT (-75.01835 40.05874)\n3    POINT (-75.02235 40.06366)\n4    POINT (-75.24473 39.95819)\nName: geometry, dtype: geometry\n\n\nNow, let‚Äôs create our GeoDataFrame object.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon‚Äôt forget to set the CRS manually!\nThe CRS you specify when creating a GeoDataFrame should tell geopandas what the coordinate system the input data is in.\nUsually you will be reading lat/lng coordinates, and will need to specify the crs as EPSG code 4326\nYou should specify the crs as a string using the syntax: ESPG:4326\n\n\n\nSince we‚Äôre only using a few EPSG codes in this course, you can usually tell what the CRS is by looking at the values in the Point() objects.\nPhiladelphia has a latitude of about 40 deg and longitude of about -75 deg.\nOur data must be in the usual lat/lng EPSG=4326!\n\nFinally, let‚Äôs create our GeoDataFrame:\n\ntrash_requests_gdf = gpd.GeoDataFrame(\n    trash_requests_df, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\n\n\nLet‚Äôs plot the monthly trash requests\nLet‚Äôs add some new columns based on the ‚Äúrequested_datetime‚Äù. Right now, this column is a string, denoted by the dtype ‚Äúobject‚Äù in pandas language:\n\ntrash_requests_gdf[\"requested_datetime\"].head()\n\n0    2020-04-02 19:22:24\n1    2020-04-02 08:40:53\n2    2020-01-02 19:17:55\n3    2020-01-03 07:01:46\n4    2020-01-15 13:22:14\nName: requested_datetime, dtype: object\n\n\nWe can now use the pd.to_datetime() function to convert to a column of special Datetime objects, which have all sorts of useful functionality.\n\n# Convert the requested datetime to a column of Datetime objects\ntrash_requests_gdf[\"requested_datetime_dt\"] = pd.to_datetime(\n    trash_requests_gdf[\"requested_datetime\"]\n)\n\n\ntrash_requests_gdf[\"requested_datetime_dt\"].head()\n\n0   2020-04-02 19:22:24\n1   2020-04-02 08:40:53\n2   2020-01-02 19:17:55\n3   2020-01-03 07:01:46\n4   2020-01-15 13:22:14\nName: requested_datetime_dt, dtype: datetime64[ns]\n\n\n\nIt looks like it worked! We have a ‚Äúdatetime64[ns]‚Äù dtype for our new column now.\nNow that the column is a Datetime Series, it has a special ‚Äú.dt‚Äù attribute. This attribute allows you to quickly extract out parts of the date, e.g., month, day, year, etc., as new columns.\nFor all of the possible options, see the API docs for pd.Series ‚Äî take a look at all of the entries that start with pd.Series.dt.\n\n\n\n\n\n\nTip: The strftime() function\n\n\n\nFor converting datetime objects to strings in a certain format, we can use the ‚Äústrftime‚Äù function (docs). This uses a special syntax to convert the date object to a string with a specific format.\nHelpful reference: Use this strftime guide to look up the syntax!\n\n\n\n# Use the .dt attribute to extract out the month integer (starting at 0)\ntrash_requests_gdf[\"month_int\"] = trash_requests_gdf[\"requested_datetime_dt\"].dt.month\n\n# Get the month name too ‚Äî this is what %B means - (use strftime.org to see the codes!)\ntrash_requests_gdf[\"month\"] = trash_requests_gdf[\"requested_datetime_dt\"].dt.strftime(\n    \"%B\"\n)\n\n\ntrash_requests_gdf.head()\n\n\n\n\n\n\n\n\nobjectid\nservice_request_id\nstatus\nstatus_notes\nservice_name\nservice_code\nagency_responsible\nservice_notice\nrequested_datetime\nupdated_datetime\nexpected_datetime\naddress\nzipcode\nmedia_url\nlat\nlon\ngeometry\nrequested_datetime_dt\nmonth_int\nmonth\n\n\n\n\n0\n8180042\n13269656\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-04-02 19:22:24\n2020-04-06 07:02:57\n2020-04-06 20:00:00\n624 FOULKROD ST\nNaN\nNaN\n40.034389\n-75.106518\nPOINT (-75.10652 40.03439)\n2020-04-02 19:22:24\n4\nApril\n\n\n1\n8180043\n13266979\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-04-02 08:40:53\n2020-04-06 07:02:58\n2020-04-05 20:00:00\n1203 ELLSWORTH ST\nNaN\nNaN\n39.936164\n-75.163497\nPOINT (-75.16350 39.93616)\n2020-04-02 08:40:53\n4\nApril\n\n\n2\n7744426\n13066443\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-02 19:17:55\n2020-01-04 05:46:06\n2020-01-06 19:00:00\n9054 WESLEYAN RD\nNaN\nNaN\n40.058737\n-75.018345\nPOINT (-75.01835 40.05874)\n2020-01-02 19:17:55\n1\nJanuary\n\n\n3\n7744427\n13066540\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-03 07:01:46\n2020-01-04 05:46:07\n2020-01-06 19:00:00\n2784 WILLITS RD\nNaN\nNaN\n40.063658\n-75.022347\nPOINT (-75.02235 40.06366)\n2020-01-03 07:01:46\n1\nJanuary\n\n\n4\n7801094\n13089345\nClosed\nNaN\nRubbish/Recyclable Material Collection\nSR-ST03\nStreets Department\n2 Business Days\n2020-01-15 13:22:14\n2020-01-16 14:03:29\n2020-01-16 19:00:00\n6137 LOCUST ST\nNaN\nNaN\n39.958186\n-75.244732\nPOINT (-75.24473 39.95819)\n2020-01-15 13:22:14\n1\nJanuary\n\n\n\n\n\n\n\nNow, let‚Äôs group by the month integer and month name, and get the size of the groups using the built-in .size() function of the groupby object:\n\ntotals_by_month = trash_requests_gdf.groupby(\n    by=[\"month_int\", \"month\"], as_index=False\n).size()\n\ntotals_by_month\n\n\n\n\n\n\n\n\nmonth_int\nmonth\nsize\n\n\n\n\n0\n1\nJanuary\n2710\n\n\n1\n2\nFebruary\n2067\n\n\n2\n3\nMarch\n2460\n\n\n3\n4\nApril\n5778\n\n\n4\n5\nMay\n6572\n\n\n5\n6\nJune\n6485\n\n\n6\n7\nJuly\n9627\n\n\n7\n8\nAugust\n4466\n\n\n8\n9\nSeptember\n1704\n\n\n9\n10\nOctober\n1335\n\n\n10\n11\nNovember\n1729\n\n\n11\n12\nDecember\n2757\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôve used the as_index=False keyword to the groupby() function. This will force the size() function to return a DataFrame instead of having the month column as the index of the resulted groupby operation.\nRemember, the columns you group by become the index of the aggregated DataFrame. I often like to have them as columns in the DataFrame instead ‚Äî the as_index=False flag exists for exactly this problem.\n\n\n\n\nPlot a bar chart with seaborn\nFor making static bar charts with Python, seaborn‚Äôs sns.barplot() is usually the best option!\n\n# Initialize figure/axes\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot!\nsns.barplot(\n    x=\"month_int\",\n    y=\"size\",\n    data=totals_by_month,\n    color=\"#2176d2\",\n    ax=ax,\n)\n\nax.set_xticklabels(totals_by_month[\"month\"]);\n\n\n\n\n\n\nReference: Let‚Äôs improve the aesthetics via matplotlib\nThe trend is clear in the previous chart, but can we do a better job with the aesthetics? Yes!\nFor reference, here is a common way to clean up charts in matplotlib:\n\n# Initialize figure/axes\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot!\nsns.barplot(\n    x=\"month\",\n    y=\"size\",\n    data=totals_by_month,\n    color=\"#2176d2\",\n    ax=ax,\n    order=[\n        \"January\",\n        \"February\",\n        \"March\",\n        \"April\",\n        \"May\",\n        \"June\",\n        \"July\",\n        \"August\",\n        \"September\",\n        \"October\",\n        \"November\",\n        \"December\",\n    ],\n    zorder=999,  # Make sure the bar charts are on top of the grid\n)\n\n# Remove x/y axis labels\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\n\n# Set the yticks to go from 0 to 10,000 with 2,000 step\nax.set_yticks(np.arange(0, 1e4 + 1, 2e3))\n\n# Format the ytick labels to use a comma and no decimal places\nax.set_yticklabels([f\"{yval:,.0f}\" for yval in ax.get_yticks()])\n\n# Rotate the xticks\nplt.setp(ax.get_xticklabels(), rotation=60)\n\n# Add a grid backgrou d\nax.grid(True, axis=\"y\")\n\n# Remove the top and right axes lines\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# Add a title\nax.set_title(\n    \"Philadelphia's Trash-Related 311 Requests in 2020\", weight=\"bold\", fontsize=16\n);\n\n\n\n\nTakeaway: A huge spike in trash related tickets over the summer in 2020!\n\n\n\n\n\n\nTip\n\n\n\nThis example could come in handy for reference as you work on assignment 2, where you will need to create polished and clear visualizations using both matplotlib and seaborn."
  },
  {
    "objectID": "content/week-3/lecture-3B.html#thats-it",
    "href": "content/week-3/lecture-3B.html#thats-it",
    "title": "Week 3B: Intro to Geospatial Data Analysis with GeoPandas",
    "section": "That‚Äôs it!",
    "text": "That‚Äôs it!\n\nWe‚Äôll continue our 311 example next week\nMore geopandas and geospatial data analysis coming up\nSee you on Monday next week!"
  },
  {
    "objectID": "schedule/401.html",
    "href": "schedule/401.html",
    "title": "Schedule: Section 401",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Repo\n\n\n\nHTML Slides\n\n\n\nExecutable Slides\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\nExploratory Data Science in Python\n\n\n\n\n\n\n\n\n\n\nWednesday, August 30\n\n\n\nLecture 1A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 6\n\n\n\nLecture 1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\nData Visualization Fundamentals\n\n\n\n\n\n\n\n\n\n\nMonday, September 11\n\n\n\nLecture 2A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 13\n\n\n\nLecture 2B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\nMore Interactive Data Viz, Intro to Vector Data & GeoPandas\n\n\n\n\n\n\n\n\n\n\nMonday, September 18\n\n\n\nLecture 3A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 20\n\n\n\nLecture 3B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\nGeospatial Analysis & Mapping\n\n\n\n\n\n\n\n\n\n\nMonday, September 25\n\n\n\nLecture 4A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 27\n\n\n\nLecture 4B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\nMore Geospatial Analysis: Urban Networks and Raster Data\n\n\n\n\n\n\n\n\n\n\nMonday, October 2\n\n\n\nLecture 5A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 4\n\n\n\nLecture 5B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\nWeb Scraping\n\n\n\n\n\n\n\n\n\n\nMonday, October 9\n\n\n\nLecture 6A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 11\n\n\n\nLecture 6B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\nWorking with APIs\n\n\n\n\n\n\n\n\n\n\nMonday, October 16\n\n\n\nLecture 7A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 18\n\n\n\nLecture 7B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\nAnalyzing and Visualizing Large Datasets\n\n\n\n\n\n\n\n\n\n\nMonday, October 23\n\n\n\nLecture 8A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 25\n\n\n\nLecture 8B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\nFrom Notebooks to the Web: Part 1\n\n\n\n\n\n\n\n\n\n\nMonday, October 30\n\n\n\nLecture 9A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 1\n\n\n\nLecture 9B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\nFrom Notebooks to the Web: Part 2\n\n\n\n\n\n\n\n\n\n\nMonday, November 6\n\n\n\nLecture 10A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 8\n\n\n\nLecture 10B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\nCase Study: Predicting Home Prices in Philadelphia\n\n\n\n\n\n\n\n\n\n\nMonday, November 13\n\n\n\nLecture 11A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 15\n\n\n\nLecture 11B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\nCase Study: Predicting Rideshare Demand\n\n\n\n\n\n\n\n\n\n\nMonday, November 20\n\n\n\nLecture 12A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, November 27\n\n\n\nLecture 12B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\nCase Study: Clustering Analysis in Python\n\n\n\n\n\n\n\n\n\n\nWednesday, November 29\n\n\n\nLecture 13A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 4\n\n\n\nLecture 13B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal project proposal due\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\nCase Study: Advanced Raster Analysis\n\n\n\n\n\n\n\n\n\n\nWednesday, December 6\n\n\n\nLecture 14A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 11\n\n\n\nLecture 14B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, December 20\n\n\n\nFinal project due"
  },
  {
    "objectID": "schedule/402.html",
    "href": "schedule/402.html",
    "title": "Schedule: Section 402",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Repo\n\n\n\nHTML Slides\n\n\n\nExecutable Slides\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\nExploratory Data Science in Python\n\n\n\n\n\n\n\n\n\n\nThursday, August 31\n\n\n\nLecture 1A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\nData Visualization Fundamentals\n\n\n\n\n\n\n\n\n\n\nThursday, September 7\n\n\n\nLecture 2A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 2B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\nMore Interactive Data Viz, Intro to Vector Data & GeoPandas\n\n\n\n\n\n\n\n\n\n\nThursday, September 14\n\n\n\nLecture 3A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 3B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\nGeospatial Analysis & Mapping\n\n\n\n\n\n\n\n\n\n\nThursday, September 21\n\n\n\nLecture 4A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 4B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\nMore Geospatial Analysis: Urban Networks and Raster Data\n\n\n\n\n\n\n\n\n\n\nThursday, September 28\n\n\n\nLecture 5A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 5B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\nWeb Scraping\n\n\n\n\n\n\n\n\n\n\nThursday, October 5\n\n\n\nLecture 6A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 6B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\nWorking with APIs\n\n\n\n\n\n\n\n\n\n\nThursday, October 19\n\n\n\nLecture 7A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 7B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\nAnalyzing and Visualizing Large Datasets\n\n\n\n\n\n\n\n\n\n\nThursday, October 26\n\n\n\nLecture 8A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 8B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\nFrom Notebooks to the Web: Part 1\n\n\n\n\n\n\n\n\n\n\nThursday, November 2\n\n\n\nLecture 9A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 9B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\nFrom Notebooks to the Web: Part 2\n\n\n\n\n\n\n\n\n\n\nThursday, November 9\n\n\n\nLecture 10A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 10B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\nCase Study: Predicting Home Prices in Philadelphia\n\n\n\n\n\n\n\n\n\n\nThursday, November 16\n\n\n\nLecture 11A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 11B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\nCase Study: Predicting Rideshare Demand\n\n\n\n\n\n\n\n\n\n\nTuesday, November 21\n\n\n\nLecture 12A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 12B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\nCase Study: Clustering Analysis in Python\n\n\n\n\n\n\n\n\n\n\nThursday, November 30\n\n\n\nLecture 13A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 13B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 4\n\n\n\nHW #6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal project proposal due\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\nCase Study: Advanced Raster Analysis\n\n\n\n\n\n\n\n\n\n\nThursday, December 7\n\n\n\nLecture 14A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 14B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, December 20\n\n\n\nFinal project due"
  },
  {
    "objectID": "content/week-3/index.html",
    "href": "content/week-3/index.html",
    "title": "Week 3: More Interactive Data Viz, Intro to Vector Data & GeoPandas",
    "section": "",
    "text": "Content for lectures 3A and 3B\n\n\n        \n            \n            ¬†\n            View materials:\n            ¬†\n            MUSA-550-Fall-2023/week-3\n        \n\n        \n            \n            ¬†\n            HTML slides:\n            ¬†\n            \n                Lecture 3A\n            \n            ¬†\n            \n                Lecture 3B\n            \n        \n        \n\n        \n            \n            ¬†\n            Executable slides:\n            ¬†\n            \n                Lecture 3A\n            \n            ¬†\n            \n                Lecture 3B"
  },
  {
    "objectID": "content/week-3/index.html#topics",
    "href": "content/week-3/index.html#topics",
    "title": "Week 3: More Interactive Data Viz, Intro to Vector Data & GeoPandas",
    "section": "Topics",
    "text": "Topics\n\nInteractive data viz with the holoviz ecosystem\nVector data and introduction to GeoPandas\nSpatial relationships and joins\nDemo: 311 requests by neighborhood in Philadelphia"
  },
  {
    "objectID": "content/week-3/index.html#general-background-reading",
    "href": "content/week-3/index.html#general-background-reading",
    "title": "Week 3: More Interactive Data Viz, Intro to Vector Data & GeoPandas",
    "section": "General Background Reading",
    "text": "General Background Reading\nSome general background readings that will help establish the foundation for this week‚Äôs work. From Python for Data Analysis by Wes McKinney:\n\nChatper 2: Python Basics\nChapter 3: Built-in Python Features\nChapter 4: NumPy Basics\nChapter 5: Intro to pandas\nChapter 9: Plotting & visualization"
  },
  {
    "objectID": "content/week-3/index.html#documentation",
    "href": "content/week-3/index.html#documentation",
    "title": "Week 3: More Interactive Data Viz, Intro to Vector Data & GeoPandas",
    "section": "Documentation",
    "text": "Documentation\n\nGeoPandas\n\nChoropleth Maps\nConverting DataFrames to GeoDataFrames\nManaging Projections\nMerging Data\n\nClassification Schemes"
  },
  {
    "objectID": "content/week-3/index.html#further-reading-references",
    "href": "content/week-3/index.html#further-reading-references",
    "title": "Week 3: More Interactive Data Viz, Intro to Vector Data & GeoPandas",
    "section": "Further Reading / References",
    "text": "Further Reading / References\n\nWest Wing and the Gall-Peters Projections\nxkcd Comic on Projections\nThe True Size Of\nCoordinate Reference Systems\nEPSG Codes"
  },
  {
    "objectID": "content/week-3/lecture-3A.html",
    "href": "content/week-3/lecture-3A.html",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "",
    "text": "Section 401\nSep 18, 2023"
  },
  {
    "objectID": "content/week-3/lecture-3A.html#housekeeping",
    "href": "content/week-3/lecture-3A.html#housekeeping",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nHomework #1 due on Monday, 9/25\nHomework #2 will be assigned same day\nChoose a dataset to visualize and explore\n\nOpenDataPhilly or one your choosing\nEmail me if you want to analyze one that‚Äôs not on OpenDataPhilly"
  },
  {
    "objectID": "content/week-3/lecture-3A.html#agenda-for-week-3",
    "href": "content/week-3/lecture-3A.html#agenda-for-week-3",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "Agenda for Week #3",
    "text": "Agenda for Week #3\nTwo parts:\n\nPart 1: More interactive data visualization: the HoloViz ecosystem\nPart 2: Getting started with geospatial data analysis in Python"
  },
  {
    "objectID": "content/week-3/lecture-3A.html#part-1-more-interactive-data-viz",
    "href": "content/week-3/lecture-3A.html#part-1-more-interactive-data-viz",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "Part 1: More interactive data viz",
    "text": "Part 1: More interactive data viz\n\nRecap: The Python data viz landscape\n\n\n\n\n\nWhat have we learned so far\n\nMatplotlib\n\nThe classic, most flexible library\nCan handle geographic data well\nOverly verbose syntax, syntax is not declarative\n\n\n\nPandas\n\nQuick, built-in interface\nNot as many features as other libraries\n\n\n\nSeaborn\n\nBest for visualizing complex relationships between variables\nImproves matplotlib‚Äôs syntax: more declarative\n\n\n\nAltair\n\nEasy, declarative syntax\nLots of interactive features\nComplex visualizations with minimal amounts of code\n\n\n\n\nWe‚Äôll learn one more today‚Ä¶\n\n\n\n\n\nHoloViz: A set of coordinated visualization libraries in Python\n\n  \n\n\n\nThe motivation behind HoloViz mirrors the goals of this course\nProper data visualization is crucial throughout all of the steps of the data science pipeline: data wrangling, modeling, and storytelling\n\n\n\n\n\n\nToday: hvPlot, Holoviews, Geoviews\n\n\nLater in the course: Datashader, Param, Panel\n\n\nHoloViz: A quick overview\n\nBokeh: creating interactive visualizations using Javascript using Python\nHoloViews: a declarative, high-level library for creating bokeh visualizations\n\nNote: The relationship between Bokeh and Holoviews is similar to Altair and Vega\n\nA significant pro\nGeoViews builds on HoloViews to add support for geographic data\n\n\nThe major cons\n\nAll are relatively new\nBokeh is the most well-tested\nHoloViews, GeoViews, hvPlot are being actively developed but are very promising\n\n\n\n\nHow does hvPlot fit in?\nHigh-level visualization library designed to help you quickly create interactive charts during your data wrangling\nMain uses: - Quickly generate interactive plots from your data - Seamlessly handles pandas and geopandas data - Relies on Holoviews and Geoviews under the hood\nIt provides an interface just like the pandas plot() function, but much more useful.\n\n\nExample: let‚Äôs return to the measles dataset\n\n# Our usual imports\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n\n# Let's load the measles data from week 2\n\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2022/week-2/main/data/measles_incidence.csv\"\nmeasles_data_raw = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\nmeasles_data_raw.head()\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows √ó 53 columns\n\n\n\n\nConvert from wide to long formats‚Ä¶\n\nmeasles_data = measles_data_raw.melt(\n    id_vars=[\"YEAR\", \"WEEK\"], value_name=\"incidence\", var_name=\"state\"\n)\n\n\nmeasles_data.head()\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nstate\nincidence\n\n\n\n\n0\n1928\n1\nALABAMA\n3.67\n\n\n1\n1928\n2\nALABAMA\n6.25\n\n\n2\n1928\n3\nALABAMA\n7.95\n\n\n3\n1928\n4\nALABAMA\n12.58\n\n\n4\n1928\n5\nALABAMA\n8.03\n\n\n\n\n\n\n\n\n\nReminder: plotting with pandas\nThe default .plot() doesn‚Äôt know which variables to plot.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nmeasles_data.plot(ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\nBut we can group by the year, and plot the national average each year\n\nby_year = measles_data.groupby(\"YEAR\", as_index=False)[\"incidence\"].sum()\nby_year.head()\n\n\n\n\n\n\n\n\nYEAR\nincidence\n\n\n\n\n0\n1928\n16924.34\n\n\n1\n1929\n12060.96\n\n\n2\n1930\n14575.11\n\n\n3\n1931\n15427.67\n\n\n4\n1932\n14481.11\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the annual average by year\nby_year.plot(x='YEAR', y='incidence', ax=ax)\n\n# Add the vaccine year and label\nax.axvline(x=1963, c='k', linewidth=2)\nax.text(1963, 27000, \" Vaccine introduced\", ha='left', fontsize=18);\n\n\n\n\n\n\nAdding interactivity with hvplot\nUse the .hvplot() function to create interactive plots.\n\n# This will add the .hvplot() function to your DataFrame!\n# Import holoviews too\nimport holoviews as hv\nimport hvplot.pandas\n\n# Load bokeh\nhv.extension(\"bokeh\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimg = by_year.hvplot(x='YEAR', y='incidence', kind=\"line\")\n\nimg\n\n\n\n\n\n  \n\n\n\n\nIn this case, .hvplot() creates a Holoviews Curve object.\nNot unlike altair Chart objects, it‚Äôs an object that knows how to translate from your DataFrame data to a visualization.\n\nprint(img)\n\n:Curve   [YEAR]   (incidence)\n\n\n\n\nMany different chart types are available‚Ä¶\n\nby_year.hvplot(x=\"YEAR\", y=\"incidence\", kind=\"scatter\")\n\n\n\n\n\n  \n\n\n\n\n\nby_year.hvplot(x='YEAR', y='incidence', kind=\"bar\", rot=90, width=1000)\n\n\n\n\n\n  \n\n\n\n\n\n\nJust like in altair, we can also layer chart elements together\nUse the * operator to layer together chart elements.\nNote: the same thing can be accomplished in altair, but with the + operator.\n\n# The line chart of incidence vs year\nincidence = by_year.hvplot(x='YEAR', y='incidence', kind=\"line\")\n\n# Vertical line + label for vaccine year\nvline = hv.VLine(1963).opts(color=\"black\")\nlabel = hv.Text(1963, 27000, \" Vaccine introduced\", halign=\"left\")\n\nfinal_chart = incidence * vline * label\nfinal_chart\n\n\n\n\n\n  \n\n\n\n\n\n\nWe can group charts by a specific column, with automatic widget selectors\nThis is some powerful magic.\nLet‚Äôs calculate the annual measles incidence for each year and state:\n\nby_state = measles_data.groupby([\"YEAR\", \"state\"], as_index=False)[\"incidence\"].sum()\n\nby_state.head()\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1928\nALASKA\n0.00\n\n\n2\n1928\nARIZONA\n200.75\n\n\n3\n1928\nARKANSAS\n481.77\n\n\n4\n1928\nCALIFORNIA\n69.22\n\n\n\n\n\n\n\nNow, tell hvplot to plot produce a chart of incidence over time for each state:\n\nby_state_chart = by_state.hvplot(\n    x=\"YEAR\", y=\"incidence\", groupby=\"state\", width=400, kind=\"line\"\n)\n\nby_state_chart\n\n\n\n\n\n  \n\n\n\n\n\n\nWe can select out individual charts from the set of grouped objects\nUse the dict-like selection syntax: [key]\n\nPA = by_state_chart[\"PENNSYLVANIA\"].relabel(\"PA\") # .relabel() is optional ‚Äî it just changes the title\nNJ = by_state_chart[\"NEW JERSEY\"].relabel(\"NJ\")\n\n\n\nCombine charts as subplots with the + operator\n\ncombined = PA + NJ\n\ncombined\n\n\n\n\n\n  \n\n\n\n\n\nprint(combined)\n\n:Layout\n   .Curve.PA :Curve   [YEAR]   (incidence)\n   .Curve.NJ :Curve   [YEAR]   (incidence)\n\n\nThe charts are side-by-side by default. You can also specify the number of rows/columns explicitly.\n\n# one column\ncombined.cols(1)\n\n\n\n\n\n  \n\n\n\n\n\n\nWe can also show overlay lines on the same plot\nFirst, select a subset of states we want to highlight using the .isin() function:\n\nstates = [\"NEW YORK\", \"NEW JERSEY\", \"CALIFORNIA\", \"PENNSYLVANIA\"]\nsub_states = by_state.loc[ by_state['state'].isin(states) ]\n\n\nsub_states.head(n=10)\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n4\n1928\nCALIFORNIA\n69.22\n\n\n30\n1928\nNEW JERSEY\n797.14\n\n\n32\n1928\nNEW YORK\n649.97\n\n\n38\n1928\nPENNSYLVANIA\n583.95\n\n\n55\n1929\nCALIFORNIA\n72.80\n\n\n81\n1929\nNEW JERSEY\n181.86\n\n\n83\n1929\nNEW YORK\n249.09\n\n\n89\n1929\nPENNSYLVANIA\n489.56\n\n\n106\n1930\nCALIFORNIA\n760.24\n\n\n132\n1930\nNEW JERSEY\n602.87\n\n\n\n\n\n\n\nNow, use the by keyword to show multiple plots on the same axes:\n\nsub_state_chart = sub_states.hvplot(\n    x=\"YEAR\",  # year on x axis\n    y=\"incidence\",  # total incidence on y axis\n    by=\"state\",  # NEW: multiple states on same axes\n    kind=\"line\",\n)\n\nsub_state_chart * vline\n\n\n\n\n\n  \n\n\n\n\n\n\nWe can also show faceted plots\nWe can explicitly map variables to rows/columns of our visualization. This is similar to the functionality we saw in altair, when we used the alt.Chart().facet(column='state') syntax\nBelow, we specify the state column should be mapped to each column of the chart:\n\nimg = sub_states.reset_index().hvplot(\n    x=\"YEAR\",  # year on x axis\n    y=\"incidence\",  # total incidence on y axis\n    col=\"state\",  # NEW: map the \"state\" value to each column in the chart\n    kind=\"line\",\n    rot=90,\n    frame_width=200,\n)\n\nimg * vline\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFunctions for each kind of chart type are available too. Try tab complete on df.hvplot. to see the options. You can use these functions directly or use pass the kind='chart type' keyword to the .hvplot() function.\n\n\n\n# Tab complete\n# by_state.hvplot. \n\n\n\nFor example, we could plot a bar chart for these four states\nLet‚Äôs select a subset by both year and state:\n\n# Two selections\nsel_year = by_state[\"YEAR\"].isin(range(1960, 1970))\nsel_state = by_state[\"state\"].isin(states)\n\n# Use the boolean AND operator: &\nsel = sel_year & sel_state\n\n# Grouped bar chart for the desired states and years\nby_state.loc[sel].hvplot.bar(x=\"YEAR\", y=\"incidence\", by=\"state\", rot=90)\n\n\n\n\n\n  \n\n\n\n\nChange bar() to line() and we get the same thing as before.\n\nby_state.loc[sel].hvplot.line(x=\"YEAR\", y=\"incidence\", by=\"state\", rot=90)\n\n\n\n\n\n  \n\n\n\n\n\n\nCustomizing charts\nSee the help message for explicit hvplot functions:\n\nby_state.hvplot.line?\n\n\n\nSignature:\nby_state.hvplot.line(\n    x=None,\n    y=None,\n    *,\n    alpha,\n    color,\n    hover_alpha,\n    hover_color,\n    hover_line_alpha,\n    hover_line_cap,\n    hover_line_color,\n    hover_line_dash,\n    hover_line_join,\n    hover_line_width,\n    line_alpha,\n    line_cap,\n    line_color,\n    line_dash,\n    line_join,\n    line_width,\n    muted,\n    muted_alpha,\n    muted_color,\n    muted_line_alpha,\n    muted_line_cap,\n    muted_line_color,\n    muted_line_dash,\n    muted_line_join,\n    muted_line_width,\n    nonselection_alpha,\n    nonselection_color,\n    nonselection_line_alpha,\n    nonselection_line_cap,\n    nonselection_line_color,\n    nonselection_line_dash,\n    nonselection_line_join,\n    nonselection_line_width,\n    selection_alpha,\n    selection_color,\n    selection_line_alpha,\n    selection_line_cap,\n    selection_line_color,\n    selection_line_dash,\n    selection_line_join,\n    selection_line_width,\n    visible,\n    width,\n    height,\n    shared_axes,\n    grid,\n    legend,\n    rot,\n    xlim,\n    ylim,\n    xticks,\n    yticks,\n    colorbar,\n    invert,\n    title,\n    logx,\n    logy,\n    loglog,\n    xaxis,\n    yaxis,\n    xformatter,\n    yformatter,\n    xlabel,\n    ylabel,\n    clabel,\n    padding,\n    responsive,\n    max_height,\n    max_width,\n    min_height,\n    min_width,\n    frame_height,\n    frame_width,\n    aspect,\n    data_aspect,\n    fontscale,\n    datashade,\n    rasterize,\n    x_sampling,\n    y_sampling,\n    aggregator,\n    **kwargs,\n)\nDocstring:\nThe `line` plot connects the points with a continous curve.\nReference: https://hvplot.holoviz.org/reference/pandas/line.html\nParameters\n----------\nx : string, optional\n    Field name(s) to draw x-positions from. If not specified, the index is\n    used. Can refer to continous and categorical data.\ny : string or list, optional\n    Field name(s) to draw y-positions from. If not specified, all numerical\n    fields are used.\nby : string, optional\n    A single column or list of columns to group by. All the subgroups are visualized.\ngroupby: string, list, optional\n    A single field or list of fields to group and filter by. Adds one or more widgets to\n    select the subgroup(s) to visualize.\ncolor : str or array-like, optional.\n    The color for each of the series. Possible values are:\n    A single color string referred to by name, RGB or RGBA code, for instance 'red' or\n    '#a98d19.\n    A sequence of color strings referred to by name, RGB or RGBA code, which will be used\n    for each series recursively. For instance ['green','yellow'] each field‚Äôs line will be\n    filled in green or yellow, alternatively. If there is only a single series to be\n    plotted, then only the first color from the color list will be used.\n**kwds : optional\n    Additional keywords arguments are documented in `hvplot.help('line')`.\nReturns\n-------\nA Holoviews object. You can `print` the object to study its composition and run\n.. code-block::\n    import holoviews as hv\n    hv.help(the_holoviews_object)\nto learn more about its parameters and options.\nExamples\n--------\n.. code-block::\n    import hvplot.pandas\n    import pandas as pd\n    df = pd.DataFrame(\n        {\n            \"actual\": [100, 150, 125, 140, 145, 135, 123],\n            \"forecast\": [90, 160, 125, 150, 141, 141, 120],\n            \"numerical\": [1.1, 1.9, 3.2, 3.8, 4.3, 5.0, 5.5],\n            \"date\": pd.date_range(\"2022-01-03\", \"2022-01-09\"),\n            \"string\": [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n        },\n    )\n    line = df.hvplot.line(\n        x=\"numerical\",\n        y=[\"actual\", \"forecast\"],\n        ylabel=\"value\",\n        legend=\"bottom\",\n        height=500,\n        color=[\"steelblue\", \"teal\"],\n        alpha=0.7,\n        line_width=5,\n    )\n    line\nYou can can add *markers* to a `line` plot by overlaying with a `scatter` plot.\n.. code-block::\n    markers = df.hvplot.scatter(\n        x=\"numerical\", y=[\"actual\", \"forecast\"], color=[\"steelblue\", \"teal\"], size=50\n    )\n    line * markers\nPlease note that you can pass widgets or reactive functions as arguments instead of\nliteral values, c.f. https://hvplot.holoviz.org/user_guide/Widgets.html.\nReferences\n----------\n- Bokeh: https://docs.bokeh.org/en/latest/docs/reference/models/glyphs/line.html\n- HoloViews: https://holoviews.org/reference/elements/bokeh/Curve.html\n- Pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.line.html\n- Plotly: https://plotly.com/python/line-charts/\n- Matplotlib: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n- Seaborn: https://seaborn.pydata.org/generated/seaborn.lineplot.html\n- Wiki: https://en.wikipedia.org/wiki/Line_chart\nGeneric options\n---------------\nclim: tuple\n    Lower and upper bound of the color scale\ncnorm (default='linear'): str\n    Color scaling which must be one of 'linear', 'log' or 'eq_hist'\ncolorbar (default=False): boolean\n    Enables a colorbar\nfontscale: number\n    Scales the size of all fonts by the same amount, e.g. fontscale=1.5\n    enlarges all fonts (title, xticks, labels etc.) by 50%\nfontsize: number or dict\n    Set title, label and legend text to the same fontsize. Finer control\n    by using a dict: {'title': '15pt', 'ylabel': '5px', 'ticks': 20}\nflip_xaxis/flip_yaxis: boolean\n    Whether to flip the axis left to right or up and down respectively\ngrid (default=False): boolean\n    Whether to show a grid\nhover : boolean\n    Whether to show hover tooltips, default is True unless datashade is\n    True in which case hover is False by default\nhover_cols (default=[]): list or str\n    Additional columns to add to the hover tool or 'all' which will\n    includes all columns (including indexes if use_index is True).\ninvert (default=False): boolean\n    Swaps x- and y-axis\nframe_width/frame_height: int\n    The width and height of the data area of the plot\nlegend (default=True): boolean or str\n    Whether to show a legend, or a legend position\n    ('top', 'bottom', 'left', 'right')\nlogx/logy (default=False): boolean\n    Enables logarithmic x- and y-axis respectively\nlogz (default=False): boolean\n    Enables logarithmic colormapping\nloglog (default=False): boolean\n    Enables logarithmic x- and y-axis\nmax_width/max_height: int\n    The maximum width and height of the plot for responsive modes\nmin_width/min_height: int\n    The minimum width and height of the plot for responsive modes\npadding: number or tuple\n    Fraction by which to increase auto-ranged extents to make\n    datapoints more visible around borders. Supports tuples to\n    specify different amount of padding for x- and y-axis and\n    tuples of tuples to specify different amounts of padding for\n    upper and lower bounds.\nrescale_discrete_levels (default=True): boolean\n    If `cnorm='eq_hist'` and there are only a few discrete values,\n    then `rescale_discrete_levels=True` (the default) decreases\n    the lower limit of the autoranged span so that the values are\n    rendering towards the (more visible) top of the `cmap` range,\n    thus avoiding washout of the lower values.  Has no effect if\n    `cnorm!=`eq_hist`.\nresponsive: boolean\n    Whether the plot should responsively resize depending on the\n    size of the browser. Responsive mode will only work if at\n    least one dimension of the plot is left undefined, e.g. when\n    width and height or width and aspect are set the plot is set\n    to a fixed size, ignoring any responsive option.\nrot: number\n    Rotates the axis ticks along the x-axis by the specified\n    number of degrees.\nshared_axes (default=True): boolean\n    Whether to link axes between plots\ntransforms (default={}): dict\n    A dictionary of HoloViews dim transforms to apply before plotting\ntitle (default=''): str\n    Title for the plot\ntools (default=[]): list\n    List of tool instances or strings (e.g. ['tap', 'box_select'])\nxaxis/yaxis: str or None\n    Whether to show the x/y-axis and whether to place it at the\n    'top'/'bottom' and 'left'/'right' respectively.\nxformatter/yformatter (default=None): str or TickFormatter\n    Formatter for the x-axis and y-axis (accepts printf formatter,\n    e.g. '%.3f', and bokeh TickFormatter)\nxlabel/ylabel/clabel (default=None): str\n    Axis labels for the x-axis, y-axis, and colorbar\nxlim/ylim (default=None): tuple or list\n    Plot limits of the x- and y-axis\nxticks/yticks (default=None): int or list\n    Ticks along x- and y-axis specified as an integer, list of\n    ticks positions, or list of tuples of the tick positions and labels\nwidth (default=700)/height (default=300): int\n    The width and height of the plot in pixels\nattr_labels (default=None): bool\n    Whether to use an xarray object's attributes as labels, defaults to\n    None to allow best effort without throwing a warning. Set to True\n    to see warning if the attrs can't be found, set to False to disable\n    the behavior.\nsort_date (default=True): bool\n    Whether to sort the x-axis by date before plotting\nsymmetric (default=None): bool\n    Whether the data are symmetric around zero. If left unset, the data\n    will be checked for symmetry as long as the size is less than\n    ``check_symmetric_max``.\ncheck_symmetric_max (default=1000000):\n    Size above which to stop checking for symmetry by default on the data.\nDatashader options\n------------------\naggregator (default=None):\n    Aggregator to use when applying rasterize or datashade operation\n    (valid options include 'mean', 'count', 'min', 'max' and more, and\n    datashader reduction objects)\ndynamic (default=True):\n    Whether to return a dynamic plot which sends updates on widget and\n    zoom/pan events or whether all the data should be embedded\n    (warning: for large groupby operations embedded data can become\n    very large if dynamic=False)\ndatashade (default=False):\n    Whether to apply rasterization and shading (colormapping) using\n    the Datashader library, returning an RGB object instead of\n    individual points\ndynspread (default=False):\n    For plots generated with datashade=True or rasterize=True,\n    automatically increase the point size when the data is sparse\n    so that individual points become more visible\nrasterize (default=False):\n    Whether to apply rasterization using the Datashader library,\n    returning an aggregated Image (to be colormapped by the\n    plotting backend) instead of individual points\nx_sampling/y_sampling (default=None):\n    Specifies the smallest allowed sampling interval along the x/y axis.\nGeographic options\n------------------\ncoastline (default=False):\n    Whether to display a coastline on top of the plot, setting\n    coastline='10m'/'50m'/'110m' specifies a specific scale.\ncrs (default=None):\n    Coordinate reference system of the data specified as Cartopy\n    CRS object, proj.4 string or EPSG code.\nfeatures (default=None): dict or list\n    A list of features or a dictionary of features and the scale\n    at which to render it. Available features include 'borders',\n    'coastline', 'lakes', 'land', 'ocean', 'rivers' and 'states'.\n    Available scales include '10m'/'50m'/'110m'.\ngeo (default=False):\n    Whether the plot should be treated as geographic (and assume\n    PlateCarree, i.e. lat/lon coordinates).\nglobal_extent (default=False):\n    Whether to expand the plot extent to span the whole globe.\nproject (default=False):\n    Whether to project the data before plotting (adds initial\n    overhead but avoids projecting data when plot is dynamically\n    updated).\nprojection (default=None): str or Cartopy CRS\n    Coordinate reference system of the plot specified as Cartopy\n    CRS object or class name.\ntiles (default=False):\n    Whether to overlay the plot on a tile source. Tiles sources\n    can be selected by name or a tiles object or class can be passed,\n    the default is 'Wikipedia'.\nStyle options\n-------------\nalpha\ncolor\nhover_alpha\nhover_color\nhover_line_alpha\nhover_line_cap\nhover_line_color\nhover_line_dash\nhover_line_join\nhover_line_width\nline_alpha\nline_cap\nline_color\nline_dash\nline_join\nline_width\nmuted\nmuted_alpha\nmuted_color\nmuted_line_alpha\nmuted_line_cap\nmuted_line_color\nmuted_line_dash\nmuted_line_join\nmuted_line_width\nnonselection_alpha\nnonselection_color\nnonselection_line_alpha\nnonselection_line_cap\nnonselection_line_color\nnonselection_line_dash\nnonselection_line_join\nnonselection_line_width\nselection_alpha\nselection_color\nselection_line_alpha\nselection_line_cap\nselection_line_color\nselection_line_dash\nselection_line_join\nselection_line_width\nvisible\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/hvplot/plotting/core.py\nType:      method\n\n\n\n\n\n\nExercise: Can we reproduce the WSJ measles heatmap?\nLast week, we reproduced the WSJ measles heatmap using altair‚Ä¶.can we do it with hvplot too?\nTake a look at the help function for the .hvplot.heatmap() function:\n\nmeasles_data.hvplot.heatmap?\n\n\n\nSignature:\nmeasles_data.hvplot.heatmap(\n    x=None,\n    y=None,\n    C=None,\n    colorbar=True,\n    *,\n    alpha,\n    annular_alpha,\n    annular_color,\n    annular_fill_alpha,\n    annular_fill_color,\n    annular_hover_alpha,\n    annular_hover_color,\n    annular_hover_fill_alpha,\n    annular_hover_fill_color,\n    annular_hover_line_alpha,\n    annular_hover_line_cap,\n    annular_hover_line_color,\n    annular_hover_line_dash,\n    annular_hover_line_join,\n    annular_hover_line_width,\n    annular_line_alpha,\n    annular_line_cap,\n    annular_line_color,\n    annular_line_dash,\n    annular_line_join,\n    annular_line_width,\n    annular_muted,\n    annular_muted_alpha,\n    annular_muted_color,\n    annular_muted_fill_alpha,\n    annular_muted_fill_color,\n    annular_muted_line_alpha,\n    annular_muted_line_cap,\n    annular_muted_line_color,\n    annular_muted_line_dash,\n    annular_muted_line_join,\n    annular_muted_line_width,\n    annular_nonselection_alpha,\n    annular_nonselection_color,\n    annular_nonselection_fill_alpha,\n    annular_nonselection_fill_color,\n    annular_nonselection_line_alpha,\n    annular_nonselection_line_cap,\n    annular_nonselection_line_color,\n    annular_nonselection_line_dash,\n    annular_nonselection_line_join,\n    annular_nonselection_line_width,\n    annular_selection_alpha,\n    annular_selection_color,\n    annular_selection_fill_alpha,\n    annular_selection_fill_color,\n    annular_selection_line_alpha,\n    annular_selection_line_cap,\n    annular_selection_line_color,\n    annular_selection_line_dash,\n    annular_selection_line_join,\n    annular_selection_line_width,\n    annular_visible,\n    cmap,\n    color,\n    dilate,\n    fill_alpha,\n    fill_color,\n    hover_alpha,\n    hover_color,\n    hover_fill_alpha,\n    hover_fill_color,\n    hover_line_alpha,\n    hover_line_cap,\n    hover_line_color,\n    hover_line_dash,\n    hover_line_join,\n    hover_line_width,\n    line_alpha,\n    line_cap,\n    line_color,\n    line_dash,\n    line_join,\n    line_width,\n    muted,\n    muted_alpha,\n    muted_color,\n    muted_fill_alpha,\n    muted_fill_color,\n    muted_line_alpha,\n    muted_line_cap,\n    muted_line_color,\n    muted_line_dash,\n    muted_line_join,\n    muted_line_width,\n    nonselection_alpha,\n    nonselection_color,\n    nonselection_fill_alpha,\n    nonselection_fill_color,\n    nonselection_line_alpha,\n    nonselection_line_cap,\n    nonselection_line_color,\n    nonselection_line_dash,\n    nonselection_line_join,\n    nonselection_line_width,\n    selection_alpha,\n    selection_color,\n    selection_fill_alpha,\n    selection_fill_color,\n    selection_line_alpha,\n    selection_line_cap,\n    selection_line_color,\n    selection_line_dash,\n    selection_line_join,\n    selection_line_width,\n    ticks_text_align,\n    ticks_text_alpha,\n    ticks_text_baseline,\n    ticks_text_color,\n    ticks_text_font,\n    ticks_text_font_size,\n    ticks_text_font_style,\n    visible,\n    xmarks_alpha,\n    xmarks_color,\n    xmarks_hover_alpha,\n    xmarks_hover_color,\n    xmarks_hover_line_alpha,\n    xmarks_hover_line_cap,\n    xmarks_hover_line_color,\n    xmarks_hover_line_dash,\n    xmarks_hover_line_join,\n    xmarks_hover_line_width,\n    xmarks_line_alpha,\n    xmarks_line_cap,\n    xmarks_line_color,\n    xmarks_line_dash,\n    xmarks_line_join,\n    xmarks_line_width,\n    xmarks_muted,\n    xmarks_muted_alpha,\n    xmarks_muted_color,\n    xmarks_muted_line_alpha,\n    xmarks_muted_line_cap,\n    xmarks_muted_line_color,\n    xmarks_muted_line_dash,\n    xmarks_muted_line_join,\n    xmarks_muted_line_width,\n    xmarks_nonselection_alpha,\n    xmarks_nonselection_color,\n    xmarks_nonselection_line_alpha,\n    xmarks_nonselection_line_cap,\n    xmarks_nonselection_line_color,\n    xmarks_nonselection_line_dash,\n    xmarks_nonselection_line_join,\n    xmarks_nonselection_line_width,\n    xmarks_selection_alpha,\n    xmarks_selection_color,\n    xmarks_selection_line_alpha,\n    xmarks_selection_line_cap,\n    xmarks_selection_line_color,\n    xmarks_selection_line_dash,\n    xmarks_selection_line_join,\n    xmarks_selection_line_width,\n    xmarks_visible,\n    ymarks_alpha,\n    ymarks_color,\n    ymarks_hover_alpha,\n    ymarks_hover_color,\n    ymarks_hover_line_alpha,\n    ymarks_hover_line_cap,\n    ymarks_hover_line_color,\n    ymarks_hover_line_dash,\n    ymarks_hover_line_join,\n    ymarks_hover_line_width,\n    ymarks_line_alpha,\n    ymarks_line_cap,\n    ymarks_line_color,\n    ymarks_line_dash,\n    ymarks_line_join,\n    ymarks_line_width,\n    ymarks_muted,\n    ymarks_muted_alpha,\n    ymarks_muted_color,\n    ymarks_muted_line_alpha,\n    ymarks_muted_line_cap,\n    ymarks_muted_line_color,\n    ymarks_muted_line_dash,\n    ymarks_muted_line_join,\n    ymarks_muted_line_width,\n    ymarks_nonselection_alpha,\n    ymarks_nonselection_color,\n    ymarks_nonselection_line_alpha,\n    ymarks_nonselection_line_cap,\n    ymarks_nonselection_line_color,\n    ymarks_nonselection_line_dash,\n    ymarks_nonselection_line_join,\n    ymarks_nonselection_line_width,\n    ymarks_selection_alpha,\n    ymarks_selection_color,\n    ymarks_selection_line_alpha,\n    ymarks_selection_line_cap,\n    ymarks_selection_line_color,\n    ymarks_selection_line_dash,\n    ymarks_selection_line_join,\n    ymarks_selection_line_width,\n    ymarks_visible,\n    reduce_function,\n    logz,\n    width,\n    height,\n    shared_axes,\n    grid,\n    legend,\n    rot,\n    xlim,\n    ylim,\n    xticks,\n    yticks,\n    invert,\n    title,\n    logx,\n    logy,\n    loglog,\n    xaxis,\n    yaxis,\n    xformatter,\n    yformatter,\n    xlabel,\n    ylabel,\n    clabel,\n    padding,\n    responsive,\n    max_height,\n    max_width,\n    min_height,\n    min_width,\n    frame_height,\n    frame_width,\n    aspect,\n    data_aspect,\n    fontscale,\n    datashade,\n    rasterize,\n    x_sampling,\n    y_sampling,\n    aggregator,\n    **kwargs,\n)\nDocstring:\n`heatmap` visualises tabular data indexed by two key dimensions as a grid of colored values.\nThis allows spotting correlations in multivariate data and provides a high-level overview\nof how the two variables are plotted.\nReference: https://hvplot.holoviz.org/reference/pandas/heatmap.html\nParameters\n----------\nx : string, optional\n    Field name to draw x coordinates from. If not specified, the index is used. Can refer\n    to continous and categorical data.\ny : string\n    Field name to draw y-positions from. Can refer to continous and categorical data.\nC : string, optional\n    Field to draw heatmap color from. If not specified a simple count will be used.\ncolorbar: boolean, optional\n    Whether to display a colorbar. Default is True.\nlogz : bool\n    Whether to apply log scaling to the z-axis. Default is False.\nreduce_function : function, optional\n    Function to compute statistics for heatmap, for example `np.mean`.\n**kwds : optional\n    Additional keywords arguments are documented in `hvplot.help('heatmap')`.\nReturns\n-------\nA Holoviews object. You can `print` the object to study its composition and run\n.. code-block::\n    import holoviews as hv\n    hv.help(the_holoviews_object)\nto learn more about its parameters and options.\nExample\n-------\n.. code-block::\n    import hvplot.pandas\n    import numpy as np\n    from bokeh.sampledata import sea_surface_temperature as sst\n    df = sst.sea_surface_temperature\n    df.hvplot.heatmap(\n        x=\"time.month\", y=\"time.day\", C=\"temperature\", reduce_function=np.mean,\n        height=500, width=500, colorbar=False, cmap=\"blues\"\n    )\nReferences\n----------\n- Bokeh: https://docs.bokeh.org/en/latest/docs/gallery/categorical_heatmap.html\n- HoloViews: https://holoviews.org/reference/elements/bokeh/HeatMap.html\n- Matplotlib: https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n- Plotly: https://plotly.com/python/heatmaps/\n- Wiki: https://en.wikipedia.org/wiki/Heat_map\nGeneric options\n---------------\nclim: tuple\n    Lower and upper bound of the color scale\ncnorm (default='linear'): str\n    Color scaling which must be one of 'linear', 'log' or 'eq_hist'\ncolorbar (default=False): boolean\n    Enables a colorbar\nfontscale: number\n    Scales the size of all fonts by the same amount, e.g. fontscale=1.5\n    enlarges all fonts (title, xticks, labels etc.) by 50%\nfontsize: number or dict\n    Set title, label and legend text to the same fontsize. Finer control\n    by using a dict: {'title': '15pt', 'ylabel': '5px', 'ticks': 20}\nflip_xaxis/flip_yaxis: boolean\n    Whether to flip the axis left to right or up and down respectively\ngrid (default=False): boolean\n    Whether to show a grid\nhover : boolean\n    Whether to show hover tooltips, default is True unless datashade is\n    True in which case hover is False by default\nhover_cols (default=[]): list or str\n    Additional columns to add to the hover tool or 'all' which will\n    includes all columns (including indexes if use_index is True).\ninvert (default=False): boolean\n    Swaps x- and y-axis\nframe_width/frame_height: int\n    The width and height of the data area of the plot\nlegend (default=True): boolean or str\n    Whether to show a legend, or a legend position\n    ('top', 'bottom', 'left', 'right')\nlogx/logy (default=False): boolean\n    Enables logarithmic x- and y-axis respectively\nlogz (default=False): boolean\n    Enables logarithmic colormapping\nloglog (default=False): boolean\n    Enables logarithmic x- and y-axis\nmax_width/max_height: int\n    The maximum width and height of the plot for responsive modes\nmin_width/min_height: int\n    The minimum width and height of the plot for responsive modes\npadding: number or tuple\n    Fraction by which to increase auto-ranged extents to make\n    datapoints more visible around borders. Supports tuples to\n    specify different amount of padding for x- and y-axis and\n    tuples of tuples to specify different amounts of padding for\n    upper and lower bounds.\nrescale_discrete_levels (default=True): boolean\n    If `cnorm='eq_hist'` and there are only a few discrete values,\n    then `rescale_discrete_levels=True` (the default) decreases\n    the lower limit of the autoranged span so that the values are\n    rendering towards the (more visible) top of the `cmap` range,\n    thus avoiding washout of the lower values.  Has no effect if\n    `cnorm!=`eq_hist`.\nresponsive: boolean\n    Whether the plot should responsively resize depending on the\n    size of the browser. Responsive mode will only work if at\n    least one dimension of the plot is left undefined, e.g. when\n    width and height or width and aspect are set the plot is set\n    to a fixed size, ignoring any responsive option.\nrot: number\n    Rotates the axis ticks along the x-axis by the specified\n    number of degrees.\nshared_axes (default=True): boolean\n    Whether to link axes between plots\ntransforms (default={}): dict\n    A dictionary of HoloViews dim transforms to apply before plotting\ntitle (default=''): str\n    Title for the plot\ntools (default=[]): list\n    List of tool instances or strings (e.g. ['tap', 'box_select'])\nxaxis/yaxis: str or None\n    Whether to show the x/y-axis and whether to place it at the\n    'top'/'bottom' and 'left'/'right' respectively.\nxformatter/yformatter (default=None): str or TickFormatter\n    Formatter for the x-axis and y-axis (accepts printf formatter,\n    e.g. '%.3f', and bokeh TickFormatter)\nxlabel/ylabel/clabel (default=None): str\n    Axis labels for the x-axis, y-axis, and colorbar\nxlim/ylim (default=None): tuple or list\n    Plot limits of the x- and y-axis\nxticks/yticks (default=None): int or list\n    Ticks along x- and y-axis specified as an integer, list of\n    ticks positions, or list of tuples of the tick positions and labels\nwidth (default=700)/height (default=300): int\n    The width and height of the plot in pixels\nattr_labels (default=None): bool\n    Whether to use an xarray object's attributes as labels, defaults to\n    None to allow best effort without throwing a warning. Set to True\n    to see warning if the attrs can't be found, set to False to disable\n    the behavior.\nsort_date (default=True): bool\n    Whether to sort the x-axis by date before plotting\nsymmetric (default=None): bool\n    Whether the data are symmetric around zero. If left unset, the data\n    will be checked for symmetry as long as the size is less than\n    ``check_symmetric_max``.\ncheck_symmetric_max (default=1000000):\n    Size above which to stop checking for symmetry by default on the data.\nDatashader options\n------------------\naggregator (default=None):\n    Aggregator to use when applying rasterize or datashade operation\n    (valid options include 'mean', 'count', 'min', 'max' and more, and\n    datashader reduction objects)\ndynamic (default=True):\n    Whether to return a dynamic plot which sends updates on widget and\n    zoom/pan events or whether all the data should be embedded\n    (warning: for large groupby operations embedded data can become\n    very large if dynamic=False)\ndatashade (default=False):\n    Whether to apply rasterization and shading (colormapping) using\n    the Datashader library, returning an RGB object instead of\n    individual points\ndynspread (default=False):\n    For plots generated with datashade=True or rasterize=True,\n    automatically increase the point size when the data is sparse\n    so that individual points become more visible\nrasterize (default=False):\n    Whether to apply rasterization using the Datashader library,\n    returning an aggregated Image (to be colormapped by the\n    plotting backend) instead of individual points\nx_sampling/y_sampling (default=None):\n    Specifies the smallest allowed sampling interval along the x/y axis.\nGeographic options\n------------------\ncoastline (default=False):\n    Whether to display a coastline on top of the plot, setting\n    coastline='10m'/'50m'/'110m' specifies a specific scale.\ncrs (default=None):\n    Coordinate reference system of the data specified as Cartopy\n    CRS object, proj.4 string or EPSG code.\nfeatures (default=None): dict or list\n    A list of features or a dictionary of features and the scale\n    at which to render it. Available features include 'borders',\n    'coastline', 'lakes', 'land', 'ocean', 'rivers' and 'states'.\n    Available scales include '10m'/'50m'/'110m'.\ngeo (default=False):\n    Whether the plot should be treated as geographic (and assume\n    PlateCarree, i.e. lat/lon coordinates).\nglobal_extent (default=False):\n    Whether to expand the plot extent to span the whole globe.\nproject (default=False):\n    Whether to project the data before plotting (adds initial\n    overhead but avoids projecting data when plot is dynamically\n    updated).\nprojection (default=None): str or Cartopy CRS\n    Coordinate reference system of the plot specified as Cartopy\n    CRS object or class name.\ntiles (default=False):\n    Whether to overlay the plot on a tile source. Tiles sources\n    can be selected by name or a tiles object or class can be passed,\n    the default is 'Wikipedia'.\nStyle options\n-------------\nalpha\nannular_alpha\nannular_color\nannular_fill_alpha\nannular_fill_color\nannular_hover_alpha\nannular_hover_color\nannular_hover_fill_alpha\nannular_hover_fill_color\nannular_hover_line_alpha\nannular_hover_line_cap\nannular_hover_line_color\nannular_hover_line_dash\nannular_hover_line_join\nannular_hover_line_width\nannular_line_alpha\nannular_line_cap\nannular_line_color\nannular_line_dash\nannular_line_join\nannular_line_width\nannular_muted\nannular_muted_alpha\nannular_muted_color\nannular_muted_fill_alpha\nannular_muted_fill_color\nannular_muted_line_alpha\nannular_muted_line_cap\nannular_muted_line_color\nannular_muted_line_dash\nannular_muted_line_join\nannular_muted_line_width\nannular_nonselection_alpha\nannular_nonselection_color\nannular_nonselection_fill_alpha\nannular_nonselection_fill_color\nannular_nonselection_line_alpha\nannular_nonselection_line_cap\nannular_nonselection_line_color\nannular_nonselection_line_dash\nannular_nonselection_line_join\nannular_nonselection_line_width\nannular_selection_alpha\nannular_selection_color\nannular_selection_fill_alpha\nannular_selection_fill_color\nannular_selection_line_alpha\nannular_selection_line_cap\nannular_selection_line_color\nannular_selection_line_dash\nannular_selection_line_join\nannular_selection_line_width\nannular_visible\ncmap\ncolor\ndilate\nfill_alpha\nfill_color\nhover_alpha\nhover_color\nhover_fill_alpha\nhover_fill_color\nhover_line_alpha\nhover_line_cap\nhover_line_color\nhover_line_dash\nhover_line_join\nhover_line_width\nline_alpha\nline_cap\nline_color\nline_dash\nline_join\nline_width\nmuted\nmuted_alpha\nmuted_color\nmuted_fill_alpha\nmuted_fill_color\nmuted_line_alpha\nmuted_line_cap\nmuted_line_color\nmuted_line_dash\nmuted_line_join\nmuted_line_width\nnonselection_alpha\nnonselection_color\nnonselection_fill_alpha\nnonselection_fill_color\nnonselection_line_alpha\nnonselection_line_cap\nnonselection_line_color\nnonselection_line_dash\nnonselection_line_join\nnonselection_line_width\nselection_alpha\nselection_color\nselection_fill_alpha\nselection_fill_color\nselection_line_alpha\nselection_line_cap\nselection_line_color\nselection_line_dash\nselection_line_join\nselection_line_width\nticks_text_align\nticks_text_alpha\nticks_text_baseline\nticks_text_color\nticks_text_font\nticks_text_font_size\nticks_text_font_style\nvisible\nxmarks_alpha\nxmarks_color\nxmarks_hover_alpha\nxmarks_hover_color\nxmarks_hover_line_alpha\nxmarks_hover_line_cap\nxmarks_hover_line_color\nxmarks_hover_line_dash\nxmarks_hover_line_join\nxmarks_hover_line_width\nxmarks_line_alpha\nxmarks_line_cap\nxmarks_line_color\nxmarks_line_dash\nxmarks_line_join\nxmarks_line_width\nxmarks_muted\nxmarks_muted_alpha\nxmarks_muted_color\nxmarks_muted_line_alpha\nxmarks_muted_line_cap\nxmarks_muted_line_color\nxmarks_muted_line_dash\nxmarks_muted_line_join\nxmarks_muted_line_width\nxmarks_nonselection_alpha\nxmarks_nonselection_color\nxmarks_nonselection_line_alpha\nxmarks_nonselection_line_cap\nxmarks_nonselection_line_color\nxmarks_nonselection_line_dash\nxmarks_nonselection_line_join\nxmarks_nonselection_line_width\nxmarks_selection_alpha\nxmarks_selection_color\nxmarks_selection_line_alpha\nxmarks_selection_line_cap\nxmarks_selection_line_color\nxmarks_selection_line_dash\nxmarks_selection_line_join\nxmarks_selection_line_width\nxmarks_visible\nymarks_alpha\nymarks_color\nymarks_hover_alpha\nymarks_hover_color\nymarks_hover_line_alpha\nymarks_hover_line_cap\nymarks_hover_line_color\nymarks_hover_line_dash\nymarks_hover_line_join\nymarks_hover_line_width\nymarks_line_alpha\nymarks_line_cap\nymarks_line_color\nymarks_line_dash\nymarks_line_join\nymarks_line_width\nymarks_muted\nymarks_muted_alpha\nymarks_muted_color\nymarks_muted_line_alpha\nymarks_muted_line_cap\nymarks_muted_line_color\nymarks_muted_line_dash\nymarks_muted_line_join\nymarks_muted_line_width\nymarks_nonselection_alpha\nymarks_nonselection_color\nymarks_nonselection_line_alpha\nymarks_nonselection_line_cap\nymarks_nonselection_line_color\nymarks_nonselection_line_dash\nymarks_nonselection_line_join\nymarks_nonselection_line_width\nymarks_selection_alpha\nymarks_selection_color\nymarks_selection_line_alpha\nymarks_selection_line_cap\nymarks_selection_line_color\nymarks_selection_line_dash\nymarks_selection_line_join\nymarks_selection_line_width\nymarks_visible\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/hvplot/plotting/core.py\nType:      method\n\n\n\n\n\nTwo methods:\nWe want to plot ‚ÄòYEAR‚Äô on the x axis, ‚Äòstate‚Äô on the y axis, and specify ‚Äòincidence‚Äô as the values begin plotted in each heatmap bin.\n\nYou can use the by_state data frame which has already summed over weeks for each state\nUse the original, tidy data (measles_data) with columns for state, week, year, and incidence\n\nYou will need to use the reduce_function keyword to sum over weeks\n\n\n\nMethod #1\nUse our aggregated dataframe: by_state\n\n\nMethod #2\nPass in the original data and have hvplot do the aggregation for us, using the reduce_function keyword:\n\n\nJust like altair: save the file as html\n\nimport hvplot\n\nhvplot.save(heatmap, \"measles.html\")\n\n\n# load the html file and display it\nfrom IPython.display import HTML\n\nHTML(\"measles.html\")\n\n\n\n\n  \n    \n    measles\n    \n      \n\n\n\n\n\n\n  \n  \n    \n  \n    \n    \n  \n\n\n\n\n\n\nTwo more useful features:\n\nScatter matrix plots\nExplorer mode\n\n\n1. Scatter matrix plots\nVisualizing relationships between variables, as we have seen in seaborn and altair\nLet‚Äôs load the penguins data set from week 2\n\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/master/data/penguins.csv\"\npenguins = pd.read_csv(url)\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nUse the hvplot.scatter_matrix() function:\n\npenguins.hvplot.scatter?\n\n\n\nSignature:\npenguins.hvplot.scatter(\n    x=None,\n    y=None,\n    *,\n    alpha,\n    angle,\n    cmap,\n    color,\n    fill_alpha,\n    fill_color,\n    hover_alpha,\n    hover_color,\n    hover_fill_alpha,\n    hover_fill_color,\n    hover_line_alpha,\n    hover_line_cap,\n    hover_line_color,\n    hover_line_dash,\n    hover_line_join,\n    hover_line_width,\n    line_alpha,\n    line_cap,\n    line_color,\n    line_dash,\n    line_join,\n    line_width,\n    marker,\n    muted,\n    muted_alpha,\n    muted_color,\n    muted_fill_alpha,\n    muted_fill_color,\n    muted_line_alpha,\n    muted_line_cap,\n    muted_line_color,\n    muted_line_dash,\n    muted_line_join,\n    muted_line_width,\n    nonselection_alpha,\n    nonselection_color,\n    nonselection_fill_alpha,\n    nonselection_fill_color,\n    nonselection_line_alpha,\n    nonselection_line_cap,\n    nonselection_line_color,\n    nonselection_line_dash,\n    nonselection_line_join,\n    nonselection_line_width,\n    palette,\n    selection_alpha,\n    selection_color,\n    selection_fill_alpha,\n    selection_fill_color,\n    selection_line_alpha,\n    selection_line_cap,\n    selection_line_color,\n    selection_line_dash,\n    selection_line_join,\n    selection_line_width,\n    size,\n    visible,\n    s,\n    c,\n    scale,\n    logz,\n    width,\n    height,\n    shared_axes,\n    grid,\n    legend,\n    rot,\n    xlim,\n    ylim,\n    xticks,\n    yticks,\n    colorbar,\n    invert,\n    title,\n    logx,\n    logy,\n    loglog,\n    xaxis,\n    yaxis,\n    xformatter,\n    yformatter,\n    xlabel,\n    ylabel,\n    clabel,\n    padding,\n    responsive,\n    max_height,\n    max_width,\n    min_height,\n    min_width,\n    frame_height,\n    frame_width,\n    aspect,\n    data_aspect,\n    fontscale,\n    datashade,\n    rasterize,\n    x_sampling,\n    y_sampling,\n    aggregator,\n    **kwargs,\n)\nDocstring:\nThe `scatter` plot visualizes your points as markers in 2D space. You can visualize\none more dimension by using colors.\nThe `scatter` plot is a good first way to plot data with non continuous axes.\nReference: https://hvplot.holoviz.org/reference/pandas/scatter.html\nParameters\n----------\nx : string, optional\n    Field name(s) to draw x-positions from. If not specified, the index is\n    used. Can refer to continous and categorical data.\ny : string or list, optional\n    Field name(s) to draw y-positions from. If not specified, all numerical\n    fields are used.\nmarker : string, optional\n    The marker shape specified above can be any supported by matplotlib, e.g. s, d, o etc.\n    See https://matplotlib.org/stable/api/markers_api.html.\nc : string, optional\n    A color or a Field name to draw the color of the marker from\ns : int, optional, also available as 'size'\n    The size of the marker\nby : string, optional\n    A single field or list of fields to group by. All the subgroups are visualized.\ngroupby: string, list, optional\n    A single field or list of fields to group and filter by. Adds one or more widgets to\n    select the subgroup(s) to visualize.\nscale: number, optional\n    Scaling factor to apply to point scaling.\nlogz : bool\n    Whether to apply log scaling to the z-axis. Default is False.\ncolor : str or array-like, optional.\n    The color for each of the series. Possible values are:\n    A single color string referred to by name, RGB or RGBA code, for instance 'red' or\n    '#a98d19.\n    A sequence of color strings referred to by name, RGB or RGBA code, which will be used\n    for each series recursively. For instance ['green','yellow'] each field‚Äôs line will be\n    filled in green or yellow, alternatively. If there is only a single series to be\n    plotted, then only the first color from the color list will be used.\n**kwds : optional\n    Additional keywords arguments are documented in `hvplot.help('scatter')`.\nReturns\n-------\nA Holoviews object. You can `print` the object to study its composition and run\n.. code-block::\n    import holoviews as hv\n    hv.help(the_holoviews_object)\nto learn more about its parameters and options.\nExample\n-------\n.. code-block::\n    import hvplot.pandas\n    import pandas as pd\n    df = pd.DataFrame(\n        {\n            \"actual\": [100, 150, 125, 140, 145, 135, 123],\n            \"forecast\": [90, 160, 125, 150, 141, 141, 120],\n            \"numerical\": [1.1, 1.9, 3.2, 3.8, 4.3, 5.0, 5.5],\n            \"date\": pd.date_range(\"2022-01-03\", \"2022-01-09\"),\n            \"string\": [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n        },\n    )\n    scatter = df.hvplot.scatter(\n        x=\"numerical\",\n        y=[\"actual\", \"forecast\"],\n        ylabel=\"value\",\n        legend=\"bottom\",\n        height=500,\n        color=[\"#f16a6f\", \"#1e85f7\"],\n        size=100,\n    )\n    scatter\nYou can overlay the `scatter` markers on for example a `line` plot\n.. code-block::\n    line = df.hvplot.line(\n        x=\"numerical\", y=[\"actual\", \"forecast\"], color=[\"#f16a6f\", \"#1e85f7\"], line_width=5\n    )\n    scatter * line\nReferences\n----------\n- Bokeh: https://docs.bokeh.org/en/latest/docs/user_guide/plotting.html#scatter-markers\n- HoloViews: https://holoviews.org/reference/elements/matplotlib/Scatter.html\n- Pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.scatter.html\n- Plotly: https://plotly.com/python/line-and-scatter/\n- Matplotlib:  https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n- Seaborn: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n- Wiki: https://en.wikipedia.org/wiki/Scatter_plot\nGeneric options\n---------------\nclim: tuple\n    Lower and upper bound of the color scale\ncnorm (default='linear'): str\n    Color scaling which must be one of 'linear', 'log' or 'eq_hist'\ncolorbar (default=False): boolean\n    Enables a colorbar\nfontscale: number\n    Scales the size of all fonts by the same amount, e.g. fontscale=1.5\n    enlarges all fonts (title, xticks, labels etc.) by 50%\nfontsize: number or dict\n    Set title, label and legend text to the same fontsize. Finer control\n    by using a dict: {'title': '15pt', 'ylabel': '5px', 'ticks': 20}\nflip_xaxis/flip_yaxis: boolean\n    Whether to flip the axis left to right or up and down respectively\ngrid (default=False): boolean\n    Whether to show a grid\nhover : boolean\n    Whether to show hover tooltips, default is True unless datashade is\n    True in which case hover is False by default\nhover_cols (default=[]): list or str\n    Additional columns to add to the hover tool or 'all' which will\n    includes all columns (including indexes if use_index is True).\ninvert (default=False): boolean\n    Swaps x- and y-axis\nframe_width/frame_height: int\n    The width and height of the data area of the plot\nlegend (default=True): boolean or str\n    Whether to show a legend, or a legend position\n    ('top', 'bottom', 'left', 'right')\nlogx/logy (default=False): boolean\n    Enables logarithmic x- and y-axis respectively\nlogz (default=False): boolean\n    Enables logarithmic colormapping\nloglog (default=False): boolean\n    Enables logarithmic x- and y-axis\nmax_width/max_height: int\n    The maximum width and height of the plot for responsive modes\nmin_width/min_height: int\n    The minimum width and height of the plot for responsive modes\npadding: number or tuple\n    Fraction by which to increase auto-ranged extents to make\n    datapoints more visible around borders. Supports tuples to\n    specify different amount of padding for x- and y-axis and\n    tuples of tuples to specify different amounts of padding for\n    upper and lower bounds.\nrescale_discrete_levels (default=True): boolean\n    If `cnorm='eq_hist'` and there are only a few discrete values,\n    then `rescale_discrete_levels=True` (the default) decreases\n    the lower limit of the autoranged span so that the values are\n    rendering towards the (more visible) top of the `cmap` range,\n    thus avoiding washout of the lower values.  Has no effect if\n    `cnorm!=`eq_hist`.\nresponsive: boolean\n    Whether the plot should responsively resize depending on the\n    size of the browser. Responsive mode will only work if at\n    least one dimension of the plot is left undefined, e.g. when\n    width and height or width and aspect are set the plot is set\n    to a fixed size, ignoring any responsive option.\nrot: number\n    Rotates the axis ticks along the x-axis by the specified\n    number of degrees.\nshared_axes (default=True): boolean\n    Whether to link axes between plots\ntransforms (default={}): dict\n    A dictionary of HoloViews dim transforms to apply before plotting\ntitle (default=''): str\n    Title for the plot\ntools (default=[]): list\n    List of tool instances or strings (e.g. ['tap', 'box_select'])\nxaxis/yaxis: str or None\n    Whether to show the x/y-axis and whether to place it at the\n    'top'/'bottom' and 'left'/'right' respectively.\nxformatter/yformatter (default=None): str or TickFormatter\n    Formatter for the x-axis and y-axis (accepts printf formatter,\n    e.g. '%.3f', and bokeh TickFormatter)\nxlabel/ylabel/clabel (default=None): str\n    Axis labels for the x-axis, y-axis, and colorbar\nxlim/ylim (default=None): tuple or list\n    Plot limits of the x- and y-axis\nxticks/yticks (default=None): int or list\n    Ticks along x- and y-axis specified as an integer, list of\n    ticks positions, or list of tuples of the tick positions and labels\nwidth (default=700)/height (default=300): int\n    The width and height of the plot in pixels\nattr_labels (default=None): bool\n    Whether to use an xarray object's attributes as labels, defaults to\n    None to allow best effort without throwing a warning. Set to True\n    to see warning if the attrs can't be found, set to False to disable\n    the behavior.\nsort_date (default=True): bool\n    Whether to sort the x-axis by date before plotting\nsymmetric (default=None): bool\n    Whether the data are symmetric around zero. If left unset, the data\n    will be checked for symmetry as long as the size is less than\n    ``check_symmetric_max``.\ncheck_symmetric_max (default=1000000):\n    Size above which to stop checking for symmetry by default on the data.\nDatashader options\n------------------\naggregator (default=None):\n    Aggregator to use when applying rasterize or datashade operation\n    (valid options include 'mean', 'count', 'min', 'max' and more, and\n    datashader reduction objects)\ndynamic (default=True):\n    Whether to return a dynamic plot which sends updates on widget and\n    zoom/pan events or whether all the data should be embedded\n    (warning: for large groupby operations embedded data can become\n    very large if dynamic=False)\ndatashade (default=False):\n    Whether to apply rasterization and shading (colormapping) using\n    the Datashader library, returning an RGB object instead of\n    individual points\ndynspread (default=False):\n    For plots generated with datashade=True or rasterize=True,\n    automatically increase the point size when the data is sparse\n    so that individual points become more visible\nrasterize (default=False):\n    Whether to apply rasterization using the Datashader library,\n    returning an aggregated Image (to be colormapped by the\n    plotting backend) instead of individual points\nx_sampling/y_sampling (default=None):\n    Specifies the smallest allowed sampling interval along the x/y axis.\nGeographic options\n------------------\ncoastline (default=False):\n    Whether to display a coastline on top of the plot, setting\n    coastline='10m'/'50m'/'110m' specifies a specific scale.\ncrs (default=None):\n    Coordinate reference system of the data specified as Cartopy\n    CRS object, proj.4 string or EPSG code.\nfeatures (default=None): dict or list\n    A list of features or a dictionary of features and the scale\n    at which to render it. Available features include 'borders',\n    'coastline', 'lakes', 'land', 'ocean', 'rivers' and 'states'.\n    Available scales include '10m'/'50m'/'110m'.\ngeo (default=False):\n    Whether the plot should be treated as geographic (and assume\n    PlateCarree, i.e. lat/lon coordinates).\nglobal_extent (default=False):\n    Whether to expand the plot extent to span the whole globe.\nproject (default=False):\n    Whether to project the data before plotting (adds initial\n    overhead but avoids projecting data when plot is dynamically\n    updated).\nprojection (default=None): str or Cartopy CRS\n    Coordinate reference system of the plot specified as Cartopy\n    CRS object or class name.\ntiles (default=False):\n    Whether to overlay the plot on a tile source. Tiles sources\n    can be selected by name or a tiles object or class can be passed,\n    the default is 'Wikipedia'.\nStyle options\n-------------\nalpha\nangle\ncmap\ncolor\nfill_alpha\nfill_color\nhover_alpha\nhover_color\nhover_fill_alpha\nhover_fill_color\nhover_line_alpha\nhover_line_cap\nhover_line_color\nhover_line_dash\nhover_line_join\nhover_line_width\nline_alpha\nline_cap\nline_color\nline_dash\nline_join\nline_width\nmarker\nmuted\nmuted_alpha\nmuted_color\nmuted_fill_alpha\nmuted_fill_color\nmuted_line_alpha\nmuted_line_cap\nmuted_line_color\nmuted_line_dash\nmuted_line_join\nmuted_line_width\nnonselection_alpha\nnonselection_color\nnonselection_fill_alpha\nnonselection_fill_color\nnonselection_line_alpha\nnonselection_line_cap\nnonselection_line_color\nnonselection_line_dash\nnonselection_line_join\nnonselection_line_width\npalette\nselection_alpha\nselection_color\nselection_fill_alpha\nselection_fill_color\nselection_line_alpha\nselection_line_cap\nselection_line_color\nselection_line_dash\nselection_line_join\nselection_line_width\nsize\nvisible\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/hvplot/plotting/core.py\nType:      method\n\n\n\n\ncolumns = [\"flipper_length_mm\", \"bill_length_mm\", \"body_mass_g\", \"species\"]\nhvplot.scatter_matrix(penguins[columns], c=\"species\")\n\n\n\n\n\n\n  \n\n\n\n\nNote the ‚Äúbox select‚Äù and ‚Äúlasso‚Äù features on the tool bar for interactions\n\n\n2. Explorer mode\n\nAn interactive interface that allows you to easily generate customized plots, which makes it easy to explore both your data and hvPlot‚Äôs options, parameters, etc.\nNew feature just released!\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nhvexplorer = hvplot.explorer(penguins)\nhvexplorer\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGet the code for your customized plot!\nYou can export the current state of the explorer by running hvexplorer.plot_code()\n\nhvexplorer.plot_code()\n\n\"df.hvplot(x='index')\"\n\n\n\npenguins.hvplot(\n    by=[\"species\"],\n    colorbar=True,\n    kind=\"scatter\",\n    title=\"Bill Depth vs. Bill Length by Species\",\n    x=\"bill_depth_mm\",\n    xlabel=\"Bill Depth (mm)\",\n    y=[\"bill_length_mm\"],\n    ylabel=\"Bill Length (mm)\",\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\nRecap: altair vs hvplot\n\nBoth use a declarative syntax (altair more so than hvplot)\nUsers of ggplot might be more familiar with altair‚Äôs syntax\nhvplot integrates directly into pandas dataframes via the .hvplot() function\nBoth have support for cross-filtering and interactions\nBoth can be incorporated into web-based dashboard via HTML (later in course)\nhvplot has better support for large data (later in course)\n\nIt‚Äôs largely up to you which one you feel is easier to use"
  },
  {
    "objectID": "content/week-3/lecture-3A.html#documentation-references",
    "href": "content/week-3/lecture-3A.html#documentation-references",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "Documentation references",
    "text": "Documentation references\n\nHvplot user guide\nHoloViz tutorial: introduction to the HoloViz ecosystem\nHoloViews user guide and gallery\n\nSome very cool examples available in the galleries"
  },
  {
    "objectID": "content/week-3/lecture-3A.html#thats-it",
    "href": "content/week-3/lecture-3A.html#thats-it",
    "title": "Week 3A: More Interactive Data Viz",
    "section": "That‚Äôs it!",
    "text": "That‚Äôs it!\n\nWe‚Äôll get started with geopandas and geospatial data analysis next time\nSee you on Wednesday!"
  },
  {
    "objectID": "content/week-2/lecture-2A.html",
    "href": "content/week-2/lecture-2A.html",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "",
    "text": "Section 401\nSep 11, 2023"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#housekeeping",
    "href": "content/week-2/lecture-2A.html#housekeeping",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nEd Discussion: https://edstem.org/us/courses/42616/discussion/\nHW #1 due on Wednesday 9/20\nOffice hours:\n\nNick: Mondays, 8PM-10PM\nTeresa: Fridays, 10:30AM to 12:00PM\nSign-up for time slots on Canvas calendar\n\n\nQuestions / concerns? - Email: nhand@design.upenn.edu - Post questions on Ed Discussion"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#guides",
    "href": "content/week-2/lecture-2A.html#guides",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Guides",
    "text": "Guides\nCheck out the guides section of the course website for\n\nSetting up Python\nUsing mamba\nJupyter notebooks and JupyterLab\nTroubleshooting Python installation issues\nTips for managing the folder structure on your laptop\n\n\n\n\n\n\n\nReminder\n\n\n\nSign up for free DataCamp access if you haven‚Äôt yet. See the course website for instructions.\nGood courses for some of the data viz libraries we‚Äôll talk about today:\n\nIntroduction to Data Visualization with matplotlib\nIntroduction to Data Visualization with seaborn\nIntermediate Data Visualization with seaborn"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#git-github-resources",
    "href": "content/week-2/lecture-2A.html#git-github-resources",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Git & GitHub Resources",
    "text": "Git & GitHub Resources\n\nSetting up git\nManaging files on GitHub"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#reminder-following-along-with-lectures",
    "href": "content/week-2/lecture-2A.html#reminder-following-along-with-lectures",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Reminder: following along with lectures",
    "text": "Reminder: following along with lectures\n\nEasiest option: use Binder\n\n\n\nHarder option: downloading Github repository contents"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#todays-agenda",
    "href": "content/week-2/lecture-2A.html#todays-agenda",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Today‚Äôs agenda",
    "text": "Today‚Äôs agenda\n\nA brief overview of data visualization\nPractical tips on color in data vizualization\nThe Python landscape:\n\nmatplotlib\nseaborn\naltair"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#a-brief-history",
    "href": "content/week-2/lecture-2A.html#a-brief-history",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "A brief history",
    "text": "A brief history\nStarting with two of my favorite historical examples, and their modern renditions‚Ä¶"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#example-1-the-pioneering-work-of-w.-e.-b.-du-bois",
    "href": "content/week-2/lecture-2A.html#example-1-the-pioneering-work-of-w.-e.-b.-du-bois",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Example 1: the pioneering work of W. E. B. Du Bois",
    "text": "Example 1: the pioneering work of W. E. B. Du Bois\n\n\n\nRe-making the Du Bois Spiral with census data\nUsing the Du Bois spiral to show the demographics of whites in seven states:\n\n \n\nGreen is urban, blue suburban, yellow small town, red rural. Source"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#section",
    "href": "content/week-2/lecture-2A.html#section",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "",
    "text": "Additional references\n\nA 6-part blog series on W. E. B. Du Bois‚Äô Data Visualization\nAn article on historical data visualization\nW. E. B. Du Bois‚Äôs Data Portraits: Visualizing Black America"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#example-2-the-statistical-atlas-of-the-united-states",
    "href": "content/week-2/lecture-2A.html#example-2-the-statistical-atlas-of-the-united-states",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Example 2: the Statistical Atlas of the United States",
    "text": "Example 2: the Statistical Atlas of the United States\n\nFirst census: 1790\nFirst map for the census: 1850\nFirst Statistical Atlas: 1870\nLargely discontinued after 1890, except for the 2000 Census Atlas\n\n\nUsing modern data\nSee http://projects.flowingdata.com/atlas, by Nathan Yau\n\n\nIndustry and Earnings by Sex\n\n\n\nSource: American Community Survey, 5-Year, 2009-2013\n\n\nMedian Household Income\n\n\n\n\n\nMany more examples‚Ä¶"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#more-recentlytwo-main-movements",
    "href": "content/week-2/lecture-2A.html#more-recentlytwo-main-movements",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "More recently‚Ä¶two main movements:",
    "text": "More recently‚Ä¶two main movements:\n\n1st wave: clarity\n2nd wave: the grammar of visualization"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#section-1",
    "href": "content/week-2/lecture-2A.html#section-1",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "",
    "text": "Wave 1: Clarity\n\nPioneered by Edward Tufte and his release of The Visual Display of Quantitative Information in 1983\nFocuses on clarity, simplicity, and plain color schemes\nCharts should be immediately accessible and readable\n\n\nThe idea of ‚ÄúChartjunk‚Äù\n\nCoined by Tufte in Visual Display\nAny unnecessary information on a chart\n\n\n\nSome extreme examples\n\n\n\n\n\n\n\n\n\nWave 2: the grammar of visualization\n\nInfluenced by The Grammar of Graphics by Leland Wilkinson in 1999\nFocuses on encoding data via channels onto geometry\nMapping data attributes on to graphical channels, e.g., length, angle, color, or position (or any other graphical character)\nLess focus on clarity, more on the encoding system\nLeads to many, many (perhaps confusing) ways of visualizing data\nggplot2 provides an R implementation of The Grammar of Graphics\nA few different Python libraries available\n\n\n\nWhere are we now?\n\nBoth movements converging together\nMore visualization libraries available now than ever\n\n\n\nA survey of common tools\n\n\n\n\nFrom a 2017 survey by Elijah Meeks\nFormer data visualization engineer at Apple, Netflix; now at Noteable\nExcellent data viz resource\nFind him on Twitter or Medium: @Elijah_Meeks\nExecutive director of the Data Visualization Society\n\n\n\nCommunity-based data viz organization\nGreat resources for beginners\nCheck out the Nightingale: The Data Visualization Society‚Äôs Blog"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#the-7-kinds-of-data-viz-people",
    "href": "content/week-2/lecture-2A.html#the-7-kinds-of-data-viz-people",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "The 7 kinds of data viz people",
    "text": "The 7 kinds of data viz people\n\nFrom this blog post\nIllustrations by Susie Lu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee, e.g.¬†Data Sketches"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#data-visualization-as-communication",
    "href": "content/week-2/lecture-2A.html#data-visualization-as-communication",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Data visualization as communication",
    "text": "Data visualization as communication\n\nData visualization is primarily a communication and design problem, not a technical one\nTwo main modes:\n\nFast: quickly understood or quickly made (or both!)\nSlow: more advanced, focus on design, takes longer to understand and/or longer to make"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#fast-visualization",
    "href": "content/week-2/lecture-2A.html#fast-visualization",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Fast visualization",
    "text": "Fast visualization\n\nClassic trope: a report for busy executives created by subject experts \\(\\rightarrow\\) as clear and simplified as possible\nLeads readers to think that if the chart is not immediately understood then it must be a failure\nThe dominant method of data visualization\n\nE.g., Create a Tableau Dashboard in Just 10 Minutes"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#moving-beyond-fast-visualizations",
    "href": "content/week-2/lecture-2A.html#moving-beyond-fast-visualizations",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Moving beyond fast visualizations",
    "text": "Moving beyond fast visualizations\n\nThinking about what charts say, beyond what is immediately clear\nFocusing on colors, design choices"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#example-fatalities-in-the-iraq-war",
    "href": "content/week-2/lecture-2A.html#example-fatalities-in-the-iraq-war",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Example: Fatalities in the Iraq War",
    "text": "Example: Fatalities in the Iraq War\nby Simon Scarr in 2011\n\n\n\nQuestion: What design choices drive home the implicit message?"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#data-visualization-as-storytelling",
    "href": "content/week-2/lecture-2A.html#data-visualization-as-storytelling",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Data Visualization as Storytelling",
    "text": "Data Visualization as Storytelling\nThe same data, but different design choices‚Ä¶\n\nA negative portrayal\n\n\n\n\n\nA positive portrayal"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#design-choices-matter-data-viz-has-never-been-more-important",
    "href": "content/week-2/lecture-2A.html#design-choices-matter-data-viz-has-never-been-more-important",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Design choices matter & data viz has never been more important",
    "text": "Design choices matter & data viz has never been more important\nSome examples from the past few years‚Ä¶\n\n\n\nData Viz‚Äôs Breakthrough Moment in the COVID-19 Crisis\nInterview with John Burn-Murdoch About his COVID Data Viz\nJohn Burn-Murdoch‚Äôs Twitter\nCOVID-19 Data Viz from the Financial Times\n\nCoronavirus Tracker\nGlobal Economic Recovery Tracker"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#data-viz-style-guides",
    "href": "content/week-2/lecture-2A.html#data-viz-style-guides",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Data Viz Style Guides",
    "text": "Data Viz Style Guides\nLots of companies, cities, institutions, etc. have started design guidelines to improve and standardize their data visualizations.\nOne I particularly like: City of London Data Design Guidelines\nFirst few pages are listed in the ‚ÄúRecommended Reading‚Äù portion of this week‚Äôs README.\nLondon‚Äôs style guide includes some basic data viz principles that everyone should know and includes the following example:\n\nCity of London Data Design Guidelines\n\nCity of London Data Design Guidelines"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#good-rules",
    "href": "content/week-2/lecture-2A.html#good-rules",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Good rules",
    "text": "Good rules\n\nLess is more ‚Äî minimize ‚Äúchartjunk‚Äù\nDon‚Äôt use legends if you can label directly\nUse color / line weight to focus the reader on the data you want to emphasize\nDon‚Äôt make the viewer tilt their head ‚Äî Use titles/subtitles to explain what is being plotted"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#now-onto-colors",
    "href": "content/week-2/lecture-2A.html#now-onto-colors",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Now onto colors‚Ä¶",
    "text": "Now onto colors‚Ä¶\nChoose your colors carefully:\n\nSequential schemes: for continuous data that progresses from low to high\nDiverging schemes: for continuous data that emphasizes positive or negative deviations from a central value\nQualitative schemes: for data that has no inherent ordering, where color is used only to distinguish categories"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#colorbrewer-2.0",
    "href": "content/week-2/lecture-2A.html#colorbrewer-2.0",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "ColorBrewer 2.0",
    "text": "ColorBrewer 2.0\n\nThe classic tool for color selection\nHandles all three types of color schemes and provides a map-based visualization\nProvides explanations from Cynthia Brewer‚Äôs published works on color theory\n\n\n\n\n\nTests whether colors are colorblind safe, printer friendly, and photocopy safe\nColorBrewer palettes are included by default in matplotlib\n\nSee: http://colorbrewer2.org"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#perceptually-uniform-color-maps",
    "href": "content/week-2/lecture-2A.html#perceptually-uniform-color-maps",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Perceptually uniform color maps",
    "text": "Perceptually uniform color maps\n\nCreated for matplotlib and available by default\nperceptually uniform: equal steps in data are perceived as equal steps in the color space\nrobust to color blindness\ncolorful and beautiful\n\nFor quantitative data, these color maps are very strong options"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#need-more-colors",
    "href": "content/week-2/lecture-2A.html#need-more-colors",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Need more colors?",
    "text": "Need more colors?\nAlmost too many tools available‚Ä¶"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#some-of-my-favorites",
    "href": "content/week-2/lecture-2A.html#some-of-my-favorites",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Some of my favorites",
    "text": "Some of my favorites\n\nAdobe Color CC: allows you to explore other people‚Äôs color palettes and create new ones\nPaletton: similar to Adobe Color, but slightly more advanced\nChroma.js Color Scale Helper: create color palettes by interpolating between named HTML colors\nColorpicker for data: automatically generate new color palettes, but they aren‚Äôt always useful"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#making-sure-your-colors-work-viz-palette",
    "href": "content/week-2/lecture-2A.html#making-sure-your-colors-work-viz-palette",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Making sure your colors work: Viz Palette",
    "text": "Making sure your colors work: Viz Palette"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#wrapping-up-some-good-rules-to-live-by",
    "href": "content/week-2/lecture-2A.html#wrapping-up-some-good-rules-to-live-by",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "Wrapping up: some good rules to live by",
    "text": "Wrapping up: some good rules to live by\n\nOptimize your color map for your dataset\nThink about who your audience is\nAvoid palettes with too many colors: ColorBrewer stops at ~9 for a reason\nMaintain a theme and make it pretty\nThink about how color interacts with the other parts of the visualization\n\nNote: no easy way to get legend added to the plot in this case‚Ä¶"
  },
  {
    "objectID": "content/week-2/lecture-2A.html#thats-it",
    "href": "content/week-2/lecture-2A.html#thats-it",
    "title": "Week 2: Data Visualization Fundamentals",
    "section": "That‚Äôs it!",
    "text": "That‚Äôs it!\nSee you on Wednesday when we continue Data Viz Fundamentals"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Weekly Course Content",
    "section": "",
    "text": "As detailed on the schedule page, the course will be broken up into 14 weeks of content, and each week will have two lectures (cleverly labeled ‚ÄúA‚Äù and ‚ÄúB‚Äù) that cover that week‚Äôs unique topic. Each week will have a recommended set of readings that will help reinforce the content. As we progress through the semester, you will be able to access the weekly content on the sidebar of this page. For each week, you‚Äôll find information about readings and topics, as well as links to the lecture slides."
  },
  {
    "objectID": "content/index.html#lecture-slides",
    "href": "content/index.html#lecture-slides",
    "title": "Weekly Course Content",
    "section": "Lecture slides",
    "text": "Lecture slides\nThe lecture slides are Jupyter notebook files, a mix of executable Python cells, text, and images. Students have a few different options to access the slide materials:\n\nThe Jupyter notebook files for each week are stored in a repository that is available on the course‚Äôs GitHub homepage. On the content page for each week, you will see a link to this repository. Once you navigate to the repository on GitHub, you can download the contents of the repository to your computer and work locally with the notebook files by launching JupyterLab on your laptop. To download the repository contents, look for the green ‚ÄúCode‚Äù button and select ‚ÄúDownload ZIP‚Äù:\n\n\n\n\n\n\n\nYou can also open the lecture slides in an interactive, executable environment using the free Binder service. This service is hosted in the cloud, meaning that the code is not being executed locally on your laptop but instead on Binder‚Äôs servers. This can be very convenient, since it doesn‚Äôt require you to have the files downloaded to your laptop or your Python environment to be set up yet. On the content page for each week, look for the Binder logo  for links to launch the lectures on Binder.\nOn the content page for each week, there are also links to HTML versions of the lecture slides. Look for the  icon for this version of the slides. Since they are displayed in the browser, they are NOT executable and cannot be changed. This version of the slides is a useful reference tool to look up how we did something in class. During class periods, we recommend that you use options #1 or #2 to view the slides, so that you can follow along and execute slides and participate in any in-class exercises or labs."
  },
  {
    "objectID": "content/week-1/lecture-1B.html",
    "href": "content/week-1/lecture-1B.html",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "",
    "text": "Section 401\n9/6/2023"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#today",
    "href": "content/week-1/lecture-1B.html#today",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Today",
    "text": "Today\n\nIntroduction to Pandas\nKey data analysis concepts\nExample: Census + Zillow data"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#reminder-the-weekly-workflow",
    "href": "content/week-1/lecture-1B.html#reminder-the-weekly-workflow",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Reminder: The weekly workflow",
    "text": "Reminder: The weekly workflow\n\nYou‚Äôll set up your local Python environment as part of the first homework assignment\nEach week, you will have two options to follow along with lectures:\n\nUsing Binder in the cloud, launching via the button on the week‚Äôs repository\nDownload the week‚Äôs repository to your laptop and launch the notebook locally\n\nWork on homeworks locally on your laptop ‚Äî Binder is only a temporary environment (no save features)\n\nTo follow along today, go to https://github.com/MUSA-550-Fall-2023/week-1\n\n\n\n\n\n\n\nReminder: Free DataCamp Courses\n\n\n\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great ‚Äî the more the merrier!\nFor more info, including how to sign up, check out the resources section of the website."
  },
  {
    "objectID": "content/week-1/lecture-1B.html#python-data-analysis-the-pandas-package",
    "href": "content/week-1/lecture-1B.html#python-data-analysis-the-pandas-package",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Python Data Analysis: the pandas package",
    "text": "Python Data Analysis: the pandas package\nDocumentation is available at https://pandas.pydata.org\n\nThe old logo\n\n\n\n\n\nThe new, less fun logo\n\n\n\nThe following line imports the pandas package:\n\nimport pandas as pd"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#basic-pandas-concepts",
    "href": "content/week-1/lecture-1B.html#basic-pandas-concepts",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Basic pandas concepts",
    "text": "Basic pandas concepts\nThe primary objects in pandas are the:\n\nDataFrame, which is very similar to an Excel spreadsheet, and has rows and named columns\nSeries, which represents a single column of data. A DataFrame contains one or more Series and a name for each Series.\n\nThe data frame is a commonly used abstraction for data manipulation. Similar implementations exist in R.\nYou can think Series objects as fancier versions of Python‚Äôs built-in list data type.\nTo create a Series object, pass a list of values to the pd.Series() function:\n\n## Philadelphia metro counties and populations\n\ncounty_names = pd.Series(\n    [\n        \"Bucks\",\n        \"Chester\",\n        \"Delaware\",\n        \"Montgomery\",\n        \"Philadelphia\",\n        \"Burlington\",\n        \"Camden\",\n        \"Gloucester\",\n    ]\n)\n\npopulation = pd.Series(\n    [\n        645054.0,\n        545823.0,\n        575182.0,\n        864683.0,\n        1567258.0,\n        466103.0,\n        524907.0,\n        306601.0,\n    ]\n)\n\nDataFrame objects can be created by passing a dict mapping string column names to their respective Series.\n\ncounty_df = pd.DataFrame({\"County Name\": county_names, \"Population\": population})\n\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\n\n\n\n\n0\nBucks\n645054.0\n\n\n1\nChester\n545823.0\n\n\n2\nDelaware\n575182.0\n\n\n3\nMontgomery\n864683.0\n\n\n4\nPhiladelphia\n1567258.0\n\n\n5\nBurlington\n466103.0\n\n\n6\nCamden\n524907.0\n\n\n7\nGloucester\n306601.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlways try to give your variables meaningful names. It will help immensely when you are trying to debug problems or when you‚Äôre trying to understand the code you‚Äôve written months later.\n\n\n\n1. Accessing data\n\n# Access columns with a dict-like syntax\ncounty_df[\"Population\"]\n\n0     645054.0\n1     545823.0\n2     575182.0\n3     864683.0\n4    1567258.0\n5     466103.0\n6     524907.0\n7     306601.0\nName: Population, dtype: float64\n\n\nRows can be accessed using Python‚Äôs syntax for slicing:\n\n## Example: slicing a list slicing\n\n# Get the elements with indices 1 and 2 (but NOT 3)\ncounty_list = [\"Bucks\", \"Philadelphia\", \"Delaware\"]\n\ncounty_list[0:2]  # Same as county_list[:2] (omitting the zero)\n\n['Bucks', 'Philadelphia']\n\n\nUnfortunately the functionality for slicing lists is not that powerful‚Ä¶but pandas will have many more features!\n\n# Slicing a DataFrame is similar...but now we will get rows of the DataFrame back!\n\ncounty_df[0:2]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\n\n\n\n\n0\nBucks\n645054.0\n\n\n1\nChester\n545823.0\n\n\n\n\n\n\n\npandas includes functionality for many different ways of selecting data. See the documentation for many more examples.\n\n\n2. Manipulating data\n\ncounty_df[\"Population\"].median()\n\n560502.5\n\n\n\ncounty_df[\"Population\"].mean()\n\n686951.375\n\n\nNumPy (Numerical Python) is a popular toolkit for scientific computing. Among other things, it can calculate mathematical functions, like mean, median, min, max, etc.\npandas Series can be used as arguments to most NumPy functions:\n\nimport numpy as np\n\n\n# Calculate the median population value\nnp.median(county_df[\"Population\"])\n\n560502.5\n\n\n\n# Calculate the median population value\nnp.mean(county_df[\"Population\"])\n\n686951.375\n\n\n\n\n\n\n\n\nNote: DataCamp + NumPy\n\n\n\nWe won‚Äôt go through the specifics of NumPy in detail in this course. You will see it pop up throughout the course, particularly when we start talking about two-dimensional raster image data. NumPy specializes in multi-dimensional arrays, the same format of multi-band raster data.\nIn the meantime, if you want to know more about NumPy or just build your Python knowledge, check out the DataCamp course on NumPy. To get free access to DataCamp, see the instructions on the course website.\n\n\n\n\n3. Applying a function\nFor more complex single-column transformations, you can use Series.apply. It accepts a function that is applied to each value in the Series.\nAn apply operation allows you to call an arbitrary function to each value in a column. It is a core pandas concept and can be very powerful for complex calculations.\nAs an example, we can find which counties have a population greater than one million:\n\n# Define our function\ndef get_large_counties(population):\n    return population &gt; 1e6\n\n\nlarge_county_sel = county_df[\"Population\"].apply(get_large_counties)\n\n\n# This is a Series where the value is the returned value from the above function\nlarge_county_sel\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Population, dtype: bool\n\n\n\n# Add the new computed column to our original data frame\ncounty_df[\"Large Counties\"] = large_county_sel\n\n\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\n\n\n1\nChester\n545823.0\nFalse\n\n\n2\nDelaware\n575182.0\nFalse\n\n\n3\nMontgomery\n864683.0\nFalse\n\n\n4\nPhiladelphia\n1567258.0\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\n\n\n6\nCamden\n524907.0\nFalse\n\n\n7\nGloucester\n306601.0\nFalse\n\n\n\n\n\n\n\nWe could have also used a Python lambda function for our function. These are inline functions that start with a special Python keyword lambda, followed by the function argument, a colon, and then the contents of the function.\n\n\n\n\n\n\nTip\n\n\n\nSee this DataCamp tutorial for more info on lambda functions.\n\n\nAs an example, let‚Äôs add a small counties column using a lambda function. The lambda function below is equivalent to the more verbose traditional definition:\n# Equivalent to -&gt; lambda population: population &lt; 1e6\ndef small_counties_func(population):\n    return population &lt; 1e6\n\nsmall_counties_func = lambda population: population &lt; 1e6\n\n# we can also use lambda (unnamed, inline) functions\ncounty_df[\"Small Counties\"] = county_df[\"Population\"].apply(small_counties_func)\n\n# Print out\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n4. Data selection\nWe can select the ‚Äúlarge‚Äù counties by passing the boolean values to the .loc[] function of a DataFrame:\n\ncounty_df[\"Large Counties\"]\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Large Counties, dtype: bool\n\n\n\ncounty_df.loc[ county_df[\"Large Counties\"] ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBe sure to use square brackets when calling .loc.\n.loc[] ‚úì\n.loc() ‚úò\n\n\nBut: there is a faster, more concise, to achieve this!\n\ncounty_df[\"Population\"] &gt; 1e6\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Population, dtype: bool\n\n\n\n# We can pass the selection directory the .loc[] ‚Äî&gt; no need to define a new variable\n\ncounty_df.loc[ county_df[\"Population\"] &gt; 1e6 ] \n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n\n\n\n\n\nQuestion: What about getting rows where ‚ÄúLarge Counties‚Äù is False?\nAnswer: Use the Python tilde operator to do a logicial not operation:\n\n# Reverse the large counties boolean selection\ncounty_df.loc[ ~large_county_sel ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n# or equivalently:\n# NOTE: you need to put the whole expression in () and then apply the tilde!\n\ncounty_df.loc[ ~(county_df[\"Population\"] &gt; 1e6) ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nAh! An even faster way!\nWe can use the pandas query function.\nThe query() function will return a subset of your dataframe based on a string version of the boolean expression.\n\ncounty_df.query(\"Population &lt; 1e6\")\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nWhat else can .loc[] do? So much!\nOne of the more common uses is use the DataFrame‚Äôs index labels to select particular rows. What is the index?\nEach row has a label that idenitifes it. By default the label is an integer ranging from 0 (first row) to 1 less than the number of rows (last row). The label is displayed in bold before each row when you look at a DataFrame:\n\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nWe can pass a label to the .loc[] function to select a particular row. For example, to get the Philadelphia row, we could use the ‚Äú4‚Äù label:\n\ncounty_df.loc[4]\n\nCounty Name       Philadelphia\nPopulation           1567258.0\nLarge Counties            True\nSmall Counties           False\nName: 4, dtype: object\n\n\nOr maybe we want the suburb counties in NJ. In this case, we can pass a list of multiple labels (5, 6, 7)‚Äù\n\nnj_suburbs = county_df.loc[ [5, 6, 7] ]\n\nnj_suburbs\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n::: {.callout-important}\nNote that this is different than the .iloc[] function, which can also be used to select rows from a DataFrame. However, it uses the integer value of the row, regardless of what the labels in the index are. For example, the first row of a dataframe can always be accessed using:\ndf.iloc[0]\n\nFor more details on the .iloc[] function, see the documentation on indexing by position.\nIn the case of the nj_suburbs dataframe, we can get the first row, which has a label of ‚Äú5‚Äù, using:\n\nnj_suburbs.iloc[0]\n\nCounty Name       Burlington\nPopulation          466103.0\nLarge Counties         False\nSmall Counties          True\nName: 5, dtype: object\n\n\nWe can also reset the index labels so they range from 0 to the length of the dataframe, using the reset_index() function. For example\n\nnj_suburbs_reset = nj_suburbs.reset_index(drop=True)\n\nnj_suburbs_reset\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBurlington\n466103.0\nFalse\nTrue\n\n\n1\nCamden\n524907.0\nFalse\nTrue\n\n\n2\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nNeed more help?\nHow to remember the specifics of all of these functions?\nThe documentation is your best friend: The Pandas user guide\nYou can also use the question mark operator in the notebook!\n\n# Use the question mark\npd.DataFrame.loc?\n\n\nType:        property\nString form: &lt;property object at 0x13712c360&gt;\nDocstring:  \nAccess a group of rows and columns by label(s) or a boolean array.\n``.loc[]`` is primarily label based, but may also be used with a\nboolean array.\nAllowed inputs are:\n- A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n  interpreted as a *label* of the index, and **never** as an\n  integer position along the index).\n- A list or array of labels, e.g. ``['a', 'b', 'c']``.\n- A slice object with labels, e.g. ``'a':'f'``.\n  .. warning:: Note that contrary to usual python slices, **both** the\n      start and the stop are included\n- A boolean array of the same length as the axis being sliced,\n  e.g. ``[True, False, True]``.\n- An alignable boolean Series. The index of the key will be aligned before\n  masking.\n- An alignable Index. The Index of the returned selection will be the input.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above)\nSee more at :ref:`Selection by Label &lt;indexing.label&gt;`.\nRaises\n------\nKeyError\n    If any items are not found.\nIndexingError\n    If an indexed key is passed and its index is unalignable to the frame index.\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column label pair.\nDataFrame.iloc : Access group of rows and columns by integer position(s).\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n    Series/DataFrame.\nSeries.loc : Access group of values using labels.\nExamples\n--------\n**Getting values**\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\nSingle label. Note this returns the row as a Series.\n&gt;&gt;&gt; df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\nList of labels. Note using ``[[]]`` returns a DataFrame.\n&gt;&gt;&gt; df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\nSingle label for row and column\n&gt;&gt;&gt; df.loc['cobra', 'shield']\n2\nSlice with labels for row and single label for column. As mentioned\nabove, note that both the start and stop of the slice are included.\n&gt;&gt;&gt; df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\nBoolean list with the same length as the row axis\n&gt;&gt;&gt; df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\nAlignable boolean Series:\n&gt;&gt;&gt; df.loc[pd.Series([False, True, False],\n...        index=['viper', 'sidewinder', 'cobra'])]\n            max_speed  shield\nsidewinder          7       8\nIndex (same behavior as ``df.reindex``)\n&gt;&gt;&gt; df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\nConditional that returns a boolean Series\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 6]\n            max_speed  shield\nsidewinder          7       8\nConditional that returns a boolean Series with column labels specified\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 6, ['max_speed']]\n            max_speed\nsidewinder          7\nCallable that returns a boolean Series\n&gt;&gt;&gt; df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n**Setting values**\nSet value for all items matching the list of labels\n&gt;&gt;&gt; df.loc[['viper', 'sidewinder'], ['shield']] = 50\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\nSet value for an entire row\n&gt;&gt;&gt; df.loc['cobra'] = 10\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\nSet value for an entire column\n&gt;&gt;&gt; df.loc[:, 'max_speed'] = 30\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\nSet value for rows matching callable condition\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 35] = 0\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n**Getting values on a DataFrame with an index that has integer labels**\nAnother example using integers for the index\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n&gt;&gt;&gt; df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\nSlice with integer labels for rows. As mentioned above, note that both\nthe start and stop of the slice are included.\n&gt;&gt;&gt; df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n**Getting values with a MultiIndex**\nA number of examples using a DataFrame with a MultiIndex\n&gt;&gt;&gt; tuples = [\n...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...    ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n&gt;&gt;&gt; index = pd.MultiIndex.from_tuples(tuples)\n&gt;&gt;&gt; values = [[12, 2], [0, 4], [10, 20],\n...         [1, 4], [7, 1], [16, 36]]\n&gt;&gt;&gt; df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n&gt;&gt;&gt; df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\nSingle label. Note this returns a DataFrame with a single index.\n&gt;&gt;&gt; df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\nSingle index tuple. Note this returns a Series.\n&gt;&gt;&gt; df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\nSingle label for row and column. Similar to passing in a tuple, this\nreturns a Series.\n&gt;&gt;&gt; df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\n&gt;&gt;&gt; df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\nSingle tuple for the index with a single label for the column\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'), 'shield']\n2\nSlice from index tuple to single label\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\nSlice from index tuple to index tuple\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1\nPlease see the :ref:`user guide&lt;advanced.advanced_hierarchical&gt;`\nfor more details and explanations of advanced indexing.\n\n\n\n\n# Use the question mark\npd.DataFrame.iloc?\n\n\nType:        property\nString form: &lt;property object at 0x136a307c0&gt;\nDocstring:  \nPurely integer-location based indexing for selection by position.\n``.iloc[]`` is primarily integer position based (from ``0`` to\n``length-1`` of the axis), but may also be used with a boolean\narray.\nAllowed inputs are:\n- An integer, e.g. ``5``.\n- A list or array of integers, e.g. ``[4, 3, 0]``.\n- A slice object with ints, e.g. ``1:7``.\n- A boolean array.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above).\n  This is useful in method chains, when you don't have a reference to the\n  calling object, but would like to base your selection on some value.\n- A tuple of row and column indexes. The tuple elements consist of one of the\n  above inputs, e.g. ``(0, 1)``.\n``.iloc`` will raise ``IndexError`` if a requested indexer is\nout-of-bounds, except *slice* indexers which allow out-of-bounds\nindexing (this conforms with python/numpy *slice* semantics).\nSee more at :ref:`Selection by Position &lt;indexing.integer&gt;`.\nSee Also\n--------\nDataFrame.iat : Fast integer location scalar accessor.\nDataFrame.loc : Purely label-location based indexer for selection by label.\nSeries.iloc : Purely integer-location based indexing for\n               selection by position.\nExamples\n--------\n&gt;&gt;&gt; mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n&gt;&gt;&gt; df = pd.DataFrame(mydict)\n&gt;&gt;&gt; df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n**Indexing just the rows**\nWith a scalar integer.\n&gt;&gt;&gt; type(df.iloc[0])\n&lt;class 'pandas.core.series.Series'&gt;\n&gt;&gt;&gt; df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64\nWith a list of integers.\n&gt;&gt;&gt; df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n&gt;&gt;&gt; type(df.iloc[[0]])\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&gt;&gt;&gt; df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400\nWith a `slice` object.\n&gt;&gt;&gt; df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\nWith a boolean mask the same length as the index.\n&gt;&gt;&gt; df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\nWith a callable, useful in method chains. The `x` passed\nto the ``lambda`` is the DataFrame being sliced. This selects\nthe rows whose index label even.\n&gt;&gt;&gt; df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n**Indexing both axes**\nYou can mix the indexer types for the index and columns. Use ``:`` to\nselect the entire axis.\nWith scalar integers.\n&gt;&gt;&gt; df.iloc[0, 1]\n2\nWith lists of integers.\n&gt;&gt;&gt; df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000\nWith `slice` objects.\n&gt;&gt;&gt; df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000\nWith a boolean array whose length matches the columns.\n&gt;&gt;&gt; df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000\nWith a callable function that expects the Series or DataFrame.\n&gt;&gt;&gt; df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#a-more-interesting-example-the-donut-effect",
    "href": "content/week-1/lecture-1B.html#a-more-interesting-example-the-donut-effect",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "A more interesting example: The Donut Effect",
    "text": "A more interesting example: The Donut Effect\nThe pandemic and growth in work from home led to a phenomenon known as the ‚ÄúDonut Effect‚Äù. With more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\n\n1. Population change\nI‚Äôve stored more data on population for Philadelphia and its suburbs in a comma-separated value (CSV) file in the data folder: data/phila-metro-counties-population.csv.\nThis data comes from the Census Population Estimates series for counties in the Philadelphia metropolitan region, which includes Philadelphia County, four suburb counties in PA, and 3 suburb counties in NJ. Later in the course, you‚Äôll learn how to download data from the Census directly using Python, but for now, I‚Äôve downloaded and formatted the data already.\nLet‚Äôs use this data to see if the donut effect occurred in Philadelphia and its suburbs.\nGoal: Calculate the percent population change from 2020 to 2022 in Philadelphia and the total percent change in the suburb counties and compare!\nFirst, use the pd.read_csv() function from pandas. The first argument to pd.read_csv() is the file path to load.\n\n# Load the data from the CSV file\npop_df = pd.read_csv(\"./data/phila-metro-counties-population.csv\")\n\n\n# How many rows are in the dataframe? --&gt; use the len() operator\nlen(pop_df)\n\n8\n\n\nPeak at the first 5 rows:\n\npop_df.head()\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n0\nBucks\nPA\n646112.0\n645054.0\n\n\n1\nChester\nPA\n534783.0\n545823.0\n\n\n2\nDelaware\nPA\n576323.0\n575182.0\n\n\n3\nMontgomery\nPA\n856938.0\n864683.0\n\n\n4\nPhiladelphia\nPA\n1600600.0\n1567258.0\n\n\n\n\n\n\n\n\nExercise: Calculate the percent change in population for Philadelphia\nLet‚Äôs select the data for Philadelphia to get its population change.\nSteps:\n\nSelect the data for Philadelphia\nSelect the 2020 and 2022 population columns (see the tip below about the squeeze() function)\nCalculate the percent change\n\n\n\n\n\n\n\nTip: The squeeze() function\n\n\n\nIf we select the data for Philadelphia, it will be DataFrame with only a single row. So if we access a column, it will return a Series of length one, rather than just a number.\nWe can use the .squeeze() function to get rid of this extra dimension of length one. It does just one it sounds like: if you have a DataFrame with only one row, it will ‚Äúsqueeze‚Äù the row dimension by removing it, returning just a Series object:\n\n\nThe selector for Philadelphia:\n\npop_df[\"county_name\"] == \"Philadelphia\"\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: county_name, dtype: bool\n\n\nNow, let‚Äôs use the .loc[] function to select the data for Philadelphia\n\nphilly_pop = pop_df.loc[ pop_df['county_name'] == 'Philadelphia' ]\n\nphilly_pop\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n4\nPhiladelphia\nPA\n1600600.0\n1567258.0\n\n\n\n\n\n\n\n\nphilly_pop = pop_df.loc[pop_df[\"county_name\"] == \"Philadelphia\"].squeeze()\n\nphilly_pop\n\ncounty_name    Philadelphia\nstate_name               PA\npop_2020          1600600.0\npop_2022          1567258.0\nName: 4, dtype: object\n\n\nNow we can calculate the percent change:\n\nphilly_pop_change = 100 * (philly_pop['pop_2022'] / philly_pop['pop_2020'] - 1)\n\nphilly_pop_change\n\n-2.0830938398100685\n\n\nA little more than 2% decline in population from 2020 to 2022!\nA note about string formatting in Python\nWe can format this into a string using an ‚Äúf-string‚Äù, strings that have an ‚Äúf‚Äù before them.\nLook for curly braces in these strings. The syntax is: {variable_name : format string}. For example:\n\nprint(f\"Philadelphia's percent change in population from 2020 to 2022: {philly_pop_change:.1f}%\")\n\nPhiladelphia's percent change in population from 2020 to 2022: -2.1%\n\n\nIn this case, we told Python to format the philly_pop_change as a floating point number with one decimal using .1f.\n\n\n\n\n\n\nTip\n\n\n\nFor more practice with f-strings, see this DataCamp tutorial.\n\n\nNext up: let‚Äôs calculate the total population change across all the suburb counties. First select every county except for Philadelphia, using the != (not equals) operator.\n\n# Now do the selection!\nsuburb_pop = pop_df.loc[ pop_df[\"county_name\"] != 'Philadelphia' ]  # select the valid rows\n\nsuburb_pop\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n0\nBucks\nPA\n646112.0\n645054.0\n\n\n1\nChester\nPA\n534783.0\n545823.0\n\n\n2\nDelaware\nPA\n576323.0\n575182.0\n\n\n3\nMontgomery\nPA\n856938.0\n864683.0\n\n\n5\nBurlington\nNJ\n461648.0\n466103.0\n\n\n6\nCamden\nNJ\n523074.0\n524907.0\n\n\n7\nGloucester\nNJ\n302554.0\n306601.0\n\n\n\n\n\n\n\nTo calculate the total percent change, we can sum up the population in 2020 and 2022 and calculate the percent change.\nIn this case, we can use the built-in .sum() function for a column Series to sum up the values in a column:\n\nsuburb_pop[\"pop_2022\"].sum()\n\n3928353.0\n\n\n\nsuburb_pop_change = 100 * (suburb_pop[\"pop_2022\"].sum() / suburb_pop[\"pop_2020\"].sum() - 1)\n\nsuburb_pop_change\n\n0.6900286869026662\n\n\n\nprint(f\"Percent change in population from 2020 to 2022 in the Philadelphia suburbs: {suburb_pop_change:.1f}%\")\n\nPercent change in population from 2020 to 2022 in the Philadelphia suburbs: 0.7%\n\n\nTakeaway\nA slight 0.7% increase in population change in the suburb counties from 2020 to 2022, compared to a 2.1% decline in Philadelphia.\nThis tracks with the idea of a ‚Äúdonut effect‚Äù in Philadelphia during the pandemic!\n\n\n\n2. Home & rental prices\nLet‚Äôs investigate changes in home and rental prices in Philadelphia and its suburbs during the pandemic. If there was a strong ‚Äúdonut effect‚Äù, we would expect to see higher price appreciation in the suburbs compared to Philadelphia itself.\nFor this part, we will use Zillow data, specifically data by county for the Zillow Observed Rent Index (ZORI) and Zillow Home Value Index (ZHVI).\nI‚Äôve already downloaded the data and stored it in the data/ folder. Let‚Äôs read it into pandas using the pd.read_csv() function.\n\nzhvi = pd.read_csv(\"data/County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\nzori = pd.read_csv(\"data/County_zori_sm_month.csv\")\n\nPeek at the first few rows of the rent data:\n\nzori.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nMetro\nStateCodeFIPS\nMunicipalCodeFIPS\n2015-01-31\n...\n2022-10-31\n2022-11-30\n2022-12-31\n2023-01-31\n2023-02-28\n2023-03-31\n2023-04-30\n2023-05-31\n2023-06-30\n2023-07-31\n\n\n\n\n0\n3101\n0\nLos Angeles County\ncounty\nCA\nCA\nLos Angeles-Long Beach-Anaheim, CA\n6\n37\n1859.636263\n...\n2884.685434\n2878.934549\n2875.026052\n2871.083085\n2877.814065\n2884.805738\n2898.603156\n2902.637331\n2911.922927\n2918.754134\n\n\n1\n139\n1\nCook County\ncounty\nIL\nIL\nChicago-Naperville-Elgin, IL-IN-WI\n17\n31\n1509.519799\n...\n1953.033934\n1948.209459\n1951.135298\n1957.004902\n1971.312435\n1985.335169\n2003.414964\n2029.555166\n2052.142638\n2065.170019\n\n\n2\n1090\n2\nHarris County\ncounty\nTX\nTX\nHouston-The Woodlands-Sugar Land, TX\n48\n201\n1244.972009\n...\n1605.890305\n1604.357134\n1603.265557\n1612.486921\n1618.611045\n1628.098030\n1630.157224\n1638.558882\n1648.540280\n1655.318245\n\n\n3\n2402\n3\nMaricopa County\ncounty\nAZ\nAZ\nPhoenix-Mesa-Chandler, AZ\n4\n13\n1001.214534\n...\n1937.026071\n1927.513924\n1918.344571\n1913.116142\n1917.415652\n1930.233484\n1939.647281\n1945.676380\n1945.808056\n1948.880852\n\n\n4\n2841\n4\nSan Diego County\ncounty\nCA\nCA\nSan Diego-Chula Vista-Carlsbad, CA\n6\n73\n1797.996849\n...\n3094.460551\n3074.724631\n3061.535358\n3052.480138\n3057.236678\n3079.509599\n3115.522418\n3154.972888\n3185.039118\n3205.105049\n\n\n\n\n5 rows √ó 112 columns\n\n\n\nAnd do the same for the home price data:\n\nzhvi.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nMetro\nStateCodeFIPS\nMunicipalCodeFIPS\n2000-01-31\n...\n2022-10-31\n2022-11-30\n2022-12-31\n2023-01-31\n2023-02-28\n2023-03-31\n2023-04-30\n2023-05-31\n2023-06-30\n2023-07-31\n\n\n\n\n0\n3101\n0\nLos Angeles County\ncounty\nCA\nCA\nLos Angeles-Long Beach-Anaheim, CA\n6\n37\n205982.244116\n...\n841135.925058\n838631.538883\n835366.307179\n828060.815041\n818172.457895\n809992.297565\n806193.000835\n808305.144458\n815021.466146\n826958.461096\n\n\n1\n139\n1\nCook County\ncounty\nIL\nIL\nChicago-Naperville-Elgin, IL-IN-WI\n17\n31\n136354.916313\n...\n284950.857552\n284143.669431\n283375.276172\n284008.891628\n284769.843988\n286109.760510\n286873.595748\n287996.202478\n290010.594940\n292281.611623\n\n\n2\n1090\n2\nHarris County\ncounty\nTX\nTX\nHouston-The Woodlands-Sugar Land, TX\n48\n201\n107050.239739\n...\n283073.624205\n282597.185986\n281442.805088\n279727.962356\n278170.043854\n277582.191393\n277509.897059\n277852.705574\n278500.865874\n279400.674485\n\n\n3\n2402\n3\nMaricopa County\ncounty\nAZ\nAZ\nPhoenix-Mesa-Chandler, AZ\n4\n13\n146824.674809\n...\n478373.220270\n472003.290822\n465224.903046\n459750.984210\n455757.981512\n454203.508557\n453192.923509\n453262.846630\n454625.973499\n457065.330187\n\n\n4\n2841\n4\nSan Diego County\ncounty\nCA\nCA\nSan Diego-Chula Vista-Carlsbad, CA\n6\n73\n217214.415906\n...\n881918.327354\n873416.861727\n866066.237738\n858962.865592\n854294.612077\n853873.557956\n856932.860092\n862757.750601\n871180.725221\n881746.559143\n\n\n\n\n5 rows √ó 292 columns\n\n\n\n\nConvert from ‚Äúwide‚Äù to ‚Äútidy‚Äù format\nCurrently, our data is in wide format: each observation has its own column. This usually results in many columns but few rows.\nUsually it‚Äôs better to have data in tidy (also known as long) format. Tidy datasets are arranged such that each variable is a column and each observation is a row.\nIn our case, we want to have a column called ZORI and one called ZHVI and a row for each month that the indices were measured.\npandas provides the melt() function for converting from wide formats to tidy formats. melt() doesn‚Äôt aggregate or summarize the data. It transforms it into a different shape, but it contains the exact same information as before.\nImagine you have 6 rows of data (each row is a unique county) with 10 columns of home values (each column is a different month). That is wide data and is the format usually seen in spreadsheets or tables in a report.\nIf you melt() that wide data, you would get a table with 60 rows and 3 columns. Each row would contain the county name, the month, and the home value that county and month. This tidy-formatted data contains the same info as the wide data, but in a different form.\nThis animation shows the transformation from wide to long / long to wide. You can ignore gather() and spread() - those are the R versions of the pandas functions.\n\n\npd.melt?\n\n\n\nSignature:\npd.melt(\n    frame: 'DataFrame',\n    id_vars=None,\n    value_vars=None,\n    var_name=None,\n    value_name='value',\n    col_level=None,\n    ignore_index: 'bool' = True,\n) -&gt; 'DataFrame'\nDocstring:\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.\nThis function is useful to massage a DataFrame into a format where one\nor more columns are identifier variables (`id_vars`), while all other\ncolumns, considered measured variables (`value_vars`), are \"unpivoted\" to\nthe row axis, leaving just two non-identifier columns, 'variable' and\n'value'.\nParameters\n----------\nid_vars : tuple, list, or ndarray, optional\n    Column(s) to use as identifier variables.\nvalue_vars : tuple, list, or ndarray, optional\n    Column(s) to unpivot. If not specified, uses all columns that\n    are not set as `id_vars`.\nvar_name : scalar\n    Name to use for the 'variable' column. If None it uses\n    ``frame.columns.name`` or 'variable'.\nvalue_name : scalar, default 'value'\n    Name to use for the 'value' column.\ncol_level : int or str, optional\n    If columns are a MultiIndex then use this level to melt.\nignore_index : bool, default True\n    If True, original index is ignored. If False, the original index is retained.\n    Index labels will be repeated as necessary.\n    .. versionadded:: 1.1.0\nReturns\n-------\nDataFrame\n    Unpivoted DataFrame.\nSee Also\n--------\nDataFrame.melt : Identical method.\npivot_table : Create a spreadsheet-style pivot table as a DataFrame.\nDataFrame.pivot : Return reshaped DataFrame organized\n    by given index / column values.\nDataFrame.explode : Explode a DataFrame from list-like\n        columns to long format.\nNotes\n-----\nReference :ref:`the user guide &lt;reshaping.melt&gt;` for more examples.\nExamples\n--------\n&gt;&gt;&gt; df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n&gt;&gt;&gt; df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6\n&gt;&gt;&gt; pd.melt(df, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n&gt;&gt;&gt; pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\nThe names of 'variable' and 'value' columns can be customized:\n&gt;&gt;&gt; pd.melt(df, id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5\nOriginal index values can be kept around:\n&gt;&gt;&gt; pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6\nIf you have multi-index columns:\n&gt;&gt;&gt; df.columns = [list('ABC'), list('DEF')]\n&gt;&gt;&gt; df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6\n&gt;&gt;&gt; pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n&gt;&gt;&gt; pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/pandas/core/reshape/melt.py\nType:      function\n\n\n\nThe syntax for pd.melt():\n\n‚Äúid_vars‚Äù: A list of column names that are the identifier variables, the fields that uniquely identify each observation.\n‚Äúvalue_vars‚Äù: A list of column names that get ‚Äúunpivoted‚Äù and become the values in the new dataframe.\n‚Äúvalue_name‚Äù: The name to use for the new value column created; by default, this is ‚Äúvalue‚Äù\n‚Äúvar_name‚Äù: The name of the column that holds the unpivoted column names (the ‚Äúkey‚Äù above); by default, this is ‚Äúvariable‚Äù\n\nNow let‚Äôs look at our dataframes.\nFor ‚Äúid_vars‚Äù, we want to use ‚ÄòRegionName‚Äô (county name) and ‚ÄòStateName‚Äô. That uniquely identifies each row.\nBut what about ‚Äúvalue_vars‚Äù?\nThe columns holding the values we want to unpivot are the monthly date strings. So‚Ä¶we need to figure out how to select all of the column names that look like dates.\n\nzhvi.columns\n\nIndex(['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName',\n       'State', 'Metro', 'StateCodeFIPS', 'MunicipalCodeFIPS', '2000-01-31',\n       ...\n       '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31', '2023-02-28',\n       '2023-03-31', '2023-04-30', '2023-05-31', '2023-06-30', '2023-07-31'],\n      dtype='object', length=292)\n\n\nThe filter() function to the rescue!\nThis function will take a function that returns True/False and list of values and return a new list with values that only return True in the function.\n\n\n\n\n\n\nTip\n\n\n\nSee this tutorial for more info on the built-in filter() function.\n\n\nIn our case, let‚Äôs write a function that tests if a string column name starts with ‚Äò20‚Äô ‚Äî this should give us all of the column names that look like dates.\n\ndef looks_like_a_date(column_name):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n\n    return column_name.startswith(\"20\")\n\n\nlooks_like_a_date(\"2000-01-31\")\n\nTrue\n\n\n\n## Now do the filter:\n## First argument is the function\n## Second argument is the list of values we want to filter\n\nfilter(looks_like_a_date, zhvi.columns)\n\n&lt;filter at 0x137b5ec80&gt;\n\n\nUgh, this doesn‚Äôt look like what we wanted!\nThis is what‚Äôs called an ‚Äúiterable‚Äù in Python (something that is ready to be iterated over). In order to actually get the values, we need to explicitly pass this to a list() function.\n\nlist(\n    filter(looks_like_a_date, zhvi.columns)\n)\n\n['2000-01-31',\n '2000-02-29',\n '2000-03-31',\n '2000-04-30',\n '2000-05-31',\n '2000-06-30',\n '2000-07-31',\n '2000-08-31',\n '2000-09-30',\n '2000-10-31',\n '2000-11-30',\n '2000-12-31',\n '2001-01-31',\n '2001-02-28',\n '2001-03-31',\n '2001-04-30',\n '2001-05-31',\n '2001-06-30',\n '2001-07-31',\n '2001-08-31',\n '2001-09-30',\n '2001-10-31',\n '2001-11-30',\n '2001-12-31',\n '2002-01-31',\n '2002-02-28',\n '2002-03-31',\n '2002-04-30',\n '2002-05-31',\n '2002-06-30',\n '2002-07-31',\n '2002-08-31',\n '2002-09-30',\n '2002-10-31',\n '2002-11-30',\n '2002-12-31',\n '2003-01-31',\n '2003-02-28',\n '2003-03-31',\n '2003-04-30',\n '2003-05-31',\n '2003-06-30',\n '2003-07-31',\n '2003-08-31',\n '2003-09-30',\n '2003-10-31',\n '2003-11-30',\n '2003-12-31',\n '2004-01-31',\n '2004-02-29',\n '2004-03-31',\n '2004-04-30',\n '2004-05-31',\n '2004-06-30',\n '2004-07-31',\n '2004-08-31',\n '2004-09-30',\n '2004-10-31',\n '2004-11-30',\n '2004-12-31',\n '2005-01-31',\n '2005-02-28',\n '2005-03-31',\n '2005-04-30',\n '2005-05-31',\n '2005-06-30',\n '2005-07-31',\n '2005-08-31',\n '2005-09-30',\n '2005-10-31',\n '2005-11-30',\n '2005-12-31',\n '2006-01-31',\n '2006-02-28',\n '2006-03-31',\n '2006-04-30',\n '2006-05-31',\n '2006-06-30',\n '2006-07-31',\n '2006-08-31',\n '2006-09-30',\n '2006-10-31',\n '2006-11-30',\n '2006-12-31',\n '2007-01-31',\n '2007-02-28',\n '2007-03-31',\n '2007-04-30',\n '2007-05-31',\n '2007-06-30',\n '2007-07-31',\n '2007-08-31',\n '2007-09-30',\n '2007-10-31',\n '2007-11-30',\n '2007-12-31',\n '2008-01-31',\n '2008-02-29',\n '2008-03-31',\n '2008-04-30',\n '2008-05-31',\n '2008-06-30',\n '2008-07-31',\n '2008-08-31',\n '2008-09-30',\n '2008-10-31',\n '2008-11-30',\n '2008-12-31',\n '2009-01-31',\n '2009-02-28',\n '2009-03-31',\n '2009-04-30',\n '2009-05-31',\n '2009-06-30',\n '2009-07-31',\n '2009-08-31',\n '2009-09-30',\n '2009-10-31',\n '2009-11-30',\n '2009-12-31',\n '2010-01-31',\n '2010-02-28',\n '2010-03-31',\n '2010-04-30',\n '2010-05-31',\n '2010-06-30',\n '2010-07-31',\n '2010-08-31',\n '2010-09-30',\n '2010-10-31',\n '2010-11-30',\n '2010-12-31',\n '2011-01-31',\n '2011-02-28',\n '2011-03-31',\n '2011-04-30',\n '2011-05-31',\n '2011-06-30',\n '2011-07-31',\n '2011-08-31',\n '2011-09-30',\n '2011-10-31',\n '2011-11-30',\n '2011-12-31',\n '2012-01-31',\n '2012-02-29',\n '2012-03-31',\n '2012-04-30',\n '2012-05-31',\n '2012-06-30',\n '2012-07-31',\n '2012-08-31',\n '2012-09-30',\n '2012-10-31',\n '2012-11-30',\n '2012-12-31',\n '2013-01-31',\n '2013-02-28',\n '2013-03-31',\n '2013-04-30',\n '2013-05-31',\n '2013-06-30',\n '2013-07-31',\n '2013-08-31',\n '2013-09-30',\n '2013-10-31',\n '2013-11-30',\n '2013-12-31',\n '2014-01-31',\n '2014-02-28',\n '2014-03-31',\n '2014-04-30',\n '2014-05-31',\n '2014-06-30',\n '2014-07-31',\n '2014-08-31',\n '2014-09-30',\n '2014-10-31',\n '2014-11-30',\n '2014-12-31',\n '2015-01-31',\n '2015-02-28',\n '2015-03-31',\n '2015-04-30',\n '2015-05-31',\n '2015-06-30',\n '2015-07-31',\n '2015-08-31',\n '2015-09-30',\n '2015-10-31',\n '2015-11-30',\n '2015-12-31',\n '2016-01-31',\n '2016-02-29',\n '2016-03-31',\n '2016-04-30',\n '2016-05-31',\n '2016-06-30',\n '2016-07-31',\n '2016-08-31',\n '2016-09-30',\n '2016-10-31',\n '2016-11-30',\n '2016-12-31',\n '2017-01-31',\n '2017-02-28',\n '2017-03-31',\n '2017-04-30',\n '2017-05-31',\n '2017-06-30',\n '2017-07-31',\n '2017-08-31',\n '2017-09-30',\n '2017-10-31',\n '2017-11-30',\n '2017-12-31',\n '2018-01-31',\n '2018-02-28',\n '2018-03-31',\n '2018-04-30',\n '2018-05-31',\n '2018-06-30',\n '2018-07-31',\n '2018-08-31',\n '2018-09-30',\n '2018-10-31',\n '2018-11-30',\n '2018-12-31',\n '2019-01-31',\n '2019-02-28',\n '2019-03-31',\n '2019-04-30',\n '2019-05-31',\n '2019-06-30',\n '2019-07-31',\n '2019-08-31',\n '2019-09-30',\n '2019-10-31',\n '2019-11-30',\n '2019-12-31',\n '2020-01-31',\n '2020-02-29',\n '2020-03-31',\n '2020-04-30',\n '2020-05-31',\n '2020-06-30',\n '2020-07-31',\n '2020-08-31',\n '2020-09-30',\n '2020-10-31',\n '2020-11-30',\n '2020-12-31',\n '2021-01-31',\n '2021-02-28',\n '2021-03-31',\n '2021-04-30',\n '2021-05-31',\n '2021-06-30',\n '2021-07-31',\n '2021-08-31',\n '2021-09-30',\n '2021-10-31',\n '2021-11-30',\n '2021-12-31',\n '2022-01-31',\n '2022-02-28',\n '2022-03-31',\n '2022-04-30',\n '2022-05-31',\n '2022-06-30',\n '2022-07-31',\n '2022-08-31',\n '2022-09-30',\n '2022-10-31',\n '2022-11-30',\n '2022-12-31',\n '2023-01-31',\n '2023-02-28',\n '2023-03-31',\n '2023-04-30',\n '2023-05-31',\n '2023-06-30',\n '2023-07-31']\n\n\nHooray, it worked!\nNow, it‚Äôs time to melt our datasets:\n\npd.melt(\n    zhvi,\n    id_vars=[\"RegionName\", \"StateName\"],\n    value_vars=list(\n        filter(looks_like_a_date, zhvi.columns)\n    ),  # Notice I'm filtering zhvi columns here\n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZHVI\n\n\n\n\n0\nLos Angeles County\nCA\n2000-01-31\n205982.244116\n\n\n1\nCook County\nIL\n2000-01-31\n136354.916313\n\n\n2\nHarris County\nTX\n2000-01-31\n107050.239739\n\n\n3\nMaricopa County\nAZ\n2000-01-31\n146824.674809\n\n\n4\nSan Diego County\nCA\n2000-01-31\n217214.415906\n\n\n...\n...\n...\n...\n...\n\n\n871069\nThomas County\nNE\n2023-07-31\n133745.901961\n\n\n871070\nMcPherson County\nNE\n2023-07-31\n273759.383253\n\n\n871071\nArthur County\nNE\n2023-07-31\n237980.396932\n\n\n871072\nPetroleum County\nMT\n2023-07-31\n195882.207523\n\n\n871073\nKenedy County\nTX\n2023-07-31\n104976.588235\n\n\n\n\n871074 rows √ó 4 columns\n\n\n\n\nzhvi_tidy = zhvi.melt(\n    id_vars=[\"RegionName\", \"StateName\"],\n    value_vars=list(filter(looks_like_a_date, zhvi.columns)), # Notice I'm filtering zhvi columns here\n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n\nzori_tidy = zori.melt(\n    id_vars=[\"RegionName\", \"StateName\"],\n    value_vars=list(filter(looks_like_a_date, zori.columns)), # Notice I'm filtering zori columns here\n    var_name=\"Date\",\n    value_name=\"ZORI\",\n)\n\nand take a look:\n\nzori_tidy.head()\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\n\n\n\n\n0\nLos Angeles County\nCA\n2015-01-31\n1859.636263\n\n\n1\nCook County\nIL\n2015-01-31\n1509.519799\n\n\n2\nHarris County\nTX\n2015-01-31\n1244.972009\n\n\n3\nMaricopa County\nAZ\n2015-01-31\n1001.214534\n\n\n4\nSan Diego County\nCA\n2015-01-31\n1797.996849\n\n\n\n\n\n\n\n\nzhvi_tidy.head()\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZHVI\n\n\n\n\n0\nLos Angeles County\nCA\n2000-01-31\n205982.244116\n\n\n1\nCook County\nIL\n2000-01-31\n136354.916313\n\n\n2\nHarris County\nTX\n2000-01-31\n107050.239739\n\n\n3\nMaricopa County\nAZ\n2000-01-31\n146824.674809\n\n\n4\nSan Diego County\nCA\n2000-01-31\n217214.415906\n\n\n\n\n\n\n\n\n\nMerge the data frames\nAnother common operation is merging, also known as joining, two datasets.\nWe can use the merge() function to merge observations that have the same Date, RegionName, and StateName values.\n\n# Left dataframe is ZORI\n# Right dataframe is ZHVI\n\nzillow_data = pd.merge(\n    zori_tidy, zhvi_tidy, on=[\"Date\", \"RegionName\", \"StateName\"], how=\"outer\"\n)\n\nLet‚Äôs sort the data by ‚ÄòDate‚Äô:\n\nzillow_data = zillow_data.sort_values(\n    by=\"Date\",         # Sort by Data column\n    ascending=True,    # Ascending order\n    ignore_index=True, # Reset the index to 0 to N\n)\n\nLet‚Äôs take a peak at the end of the dataframe using the .tail() function:\n\nzillow_data.head(n=20)\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\nZHVI\n\n\n\n\n0\nDooly County\nGA\n2000-01-31\nNaN\n43205.386681\n\n\n1\nMoniteau County\nMO\n2000-01-31\nNaN\nNaN\n\n\n2\nJefferson County\nGA\n2000-01-31\nNaN\n52304.968485\n\n\n3\nHenry County\nKY\n2000-01-31\nNaN\n83564.841863\n\n\n4\nPawnee County\nOK\n2000-01-31\nNaN\n37124.230106\n\n\n5\nClarke County\nMS\n2000-01-31\nNaN\nNaN\n\n\n6\nFloyd County\nIA\n2000-01-31\nNaN\n65476.850601\n\n\n7\nNottoway County\nVA\n2000-01-31\nNaN\nNaN\n\n\n8\nDewitt County\nIL\n2000-01-31\nNaN\nNaN\n\n\n9\nGrand County\nCO\n2000-01-31\nNaN\n176407.811425\n\n\n10\nSequatchie County\nTN\n2000-01-31\nNaN\n57679.387017\n\n\n11\nBarbour County\nWV\n2000-01-31\nNaN\n41933.827293\n\n\n12\nCarroll County\nIL\n2000-01-31\nNaN\nNaN\n\n\n13\nWashington County\nAL\n2000-01-31\nNaN\nNaN\n\n\n14\nFloyd County\nVA\n2000-01-31\nNaN\nNaN\n\n\n15\nMarquette County\nWI\n2000-01-31\nNaN\nNaN\n\n\n16\nVermillion County\nIN\n2000-01-31\nNaN\nNaN\n\n\n17\nLawrence County\nIL\n2000-01-31\nNaN\n14805.795158\n\n\n18\nBrown County\nIN\n2000-01-31\nNaN\nNaN\n\n\n19\nRedwood County\nMN\n2000-01-31\nNaN\nNaN\n\n\n\n\n\n\n\nMerging is very powerful and the merge can be done in a number of ways. In this case, we did a outer merge in order to keep all parts of each dataframe: even if a county doesn‚Äôt have both a ZORI and ZHVI value, the row will be in the merged dataframe with NaN to represent missing values.\nBy contrast, the inner merge only keeps the overlapping intersection of the merge: counties must have both a ZORI and ZHVI value for a given month for the row to make it into the merged dataframe.\nThe below infographic is helpful for figuring out the differences between merge types:\n\n\n\nTrim to the counties we want\nWhen we select the counties in the Philly metro region, we need to be careful because if we select just based on county name, we run the risk of selecting counties with the same name in other states. So let‚Äôs do our selection in two parts:\n\nTrim to counties in NJ or PA\nTrim based on county name\n\n\n# Get the rows with state == 'PA' or state == 'NJ'\nin_nj_pa = zillow_data[\"StateName\"].isin([\"PA\", \"NJ\"])\n\n# Save the trimmed dataframe\nzillow_trim_tmp = zillow_data.loc[in_nj_pa]\n\n\ncounty_names = [\n    \"Bucks County\",\n    \"Chester County\",\n    \"Delaware County\",\n    \"Montgomery County\",\n    \"Burlington County\",\n    \"Camden County\",\n    \"Gloucester County\",\n    \"Philadelphia County\",\n]\n\n# Trim based on county name\ncounty_sel = zillow_trim_tmp['RegionName'].isin(county_names)\n\n# Trim temp dataframe from last step\nprices_philly_metro = zillow_trim_tmp.loc[county_sel]\n\n\nprices_philly_metro\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\nZHVI\n\n\n\n\n1939\nCamden County\nNJ\n2000-01-31\nNaN\n104472.922223\n\n\n1946\nChester County\nPA\n2000-01-31\nNaN\n181113.160285\n\n\n1948\nDelaware County\nPA\n2000-01-31\nNaN\n113558.833735\n\n\n1960\nBucks County\nPA\n2000-01-31\nNaN\n166888.007583\n\n\n2008\nBurlington County\nNJ\n2000-01-31\nNaN\n141309.912108\n\n\n...\n...\n...\n...\n...\n...\n\n\n870781\nCamden County\nNJ\n2023-07-31\n1840.157047\n305607.084299\n\n\n870875\nGloucester County\nNJ\n2023-07-31\n2156.799027\n333625.235271\n\n\n870881\nBucks County\nPA\n2023-07-31\n2165.660642\n464753.874925\n\n\n870987\nMontgomery County\nPA\n2023-07-31\n2058.804292\n443222.467804\n\n\n871037\nPhiladelphia County\nPA\n2023-07-31\n1747.206167\n221021.754706\n\n\n\n\n2264 rows √ó 5 columns\n\n\n\nLet‚Äôs verify it worked‚Ä¶how many unique values in the ‚ÄúRegionName‚Äù column?\n\nprices_philly_metro['RegionName'].nunique()\n\n8\n\n\n\n\nGroup by: split-apply-combine\npandas is especially useful for grouping and aggregating data via the groupby() function.\nFrom the pandas documentation, groupby means: - Splitting the data into groups based on some criteria. - Applying a function to each group independently. - Combining the results into a data structure.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the documentation for more info.\n\n\nWe will take advantage of the pandas ‚Äúgroup by‚Äù to calculate the percent increase in price values from March 2020 to July 2023 (the latest available data) for each county.\nTo do this, we‚Äôll do the following:\n\nGroup by county name (the ‚ÄúRegionName‚Äù column)\nApply a function that does the following for each county‚Äôs data:\n\nSelect the March 2020 data\nSelect the July 2023 data\nCalculates the percent change for ZHVI and ZORI\n\n\nFirst, do the groupby operation:\n\ngrouped = prices_philly_metro.groupby(\"RegionName\")\n\ngrouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x137c4a800&gt;\n\n\nThe groupby() function returns a DataFrameGroupBy object. This object knows what the groups are, but we still need to apply() a function to this object in order to get a DataFrame back!\nLet‚Äôs set up our function:\n\ndef calculate_percent_increase(group_df):\n    \"\"\"\n    Calculate the percent increase from 2020-03-31 to 2023-07-31.\n    \n    Note that `group_df` is the DataFrame for each group, in this\n    case, the data for each county.\n    \"\"\"\n    # Create selections for the march 2020 and july 2023 data\n    march_sel = group_df[\"Date\"] == \"2020-03-31\"\n    july_sel = group_df[\"Date\"] == \"2023-07-31\"\n    \n    # Get the data for each month (only 1 row, so squeeze it!)\n    march_2020 = group_df.loc[march_sel].squeeze()\n    july_2023 = group_df.loc[july_sel].squeeze()\n\n    # Columns to calculate percent change for\n    columns = [\"ZORI\", \"ZHVI\"]\n    \n    # Return the percent change for both columns\n    return 100 * (july_2023[columns] / march_2020[columns] - 1)\n\nNow, let‚Äôs apply this function to our group object:\n\nresult = grouped.apply(calculate_percent_increase)\n\nresult\n\n\n\n\n\n\n\n\nZORI\nZHVI\n\n\nRegionName\n\n\n\n\n\n\nBucks County\n28.943525\n33.028304\n\n\nBurlington County\n35.053677\n37.403941\n\n\nCamden County\n35.580024\n51.834765\n\n\nChester County\n27.913963\n39.692418\n\n\nDelaware County\n29.269362\n41.453732\n\n\nGloucester County\n36.979221\n44.089155\n\n\nMontgomery County\n27.045037\n39.111692\n\n\nPhiladelphia County\n15.020128\n20.048094\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the index (shown in bold in the notebook output) is the column that we grouped over. That is always the case when we use groupby() in pandas. The resulting DataFrame‚Äôs index is the groupby columns.\n\n\nSort by ZHVI and take a look:\n\nresult.sort_values(by=\"ZHVI\", ascending=True)\n\n\n\n\n\n\n\n\nZORI\nZHVI\n\n\nRegionName\n\n\n\n\n\n\nPhiladelphia County\n15.020128\n20.048094\n\n\nBucks County\n28.943525\n33.028304\n\n\nBurlington County\n35.053677\n37.403941\n\n\nMontgomery County\n27.045037\n39.111692\n\n\nChester County\n27.913963\n39.692418\n\n\nDelaware County\n29.269362\n41.453732\n\n\nGloucester County\n36.979221\n44.089155\n\n\nCamden County\n35.580024\n51.834765\n\n\n\n\n\n\n\nTakeaway\nPhiladelphia has had much lower price appreciation for home values and rent values since the pandemic started compared to its suburbs.\nThis is evidence in favor of the ‚Äúdonut effect‚Äù, with higher demand in the suburbs driving prices higher!\n\n\n\n\n\n\nNote\n\n\n\nThere are a number of built-in, convenience functions that we can use on the groupby object, such as .mean(), .sum(), .size(), etc.\n\n\nFor example, if we wanted to calculate the average ZHVI and ZORI values since March 2020 for every county, we could do the following:\n\n# First trim to dates from March 2020 onwards, then group by county name\ngroup_example = prices_philly_metro.query(\"Date &gt;= '2020-03-31'\").groupby(\"RegionName\")\n\n\n# Select the columns we want, and then use the built-in mean function\navg_prices = group_example[['ZHVI', 'ZORI']].mean()\n\navg_prices.sort_values(\"ZHVI\")\n\n\n\n\n\n\n\n\nZHVI\nZORI\n\n\nRegionName\n\n\n\n\n\n\nPhiladelphia County\n208756.603528\n1620.775839\n\n\nCamden County\n255742.478151\n1612.282080\n\n\nGloucester County\n283831.654654\n1868.459836\n\n\nDelaware County\n285825.916694\n1532.245821\n\n\nBurlington County\n315162.461555\n1924.282700\n\n\nMontgomery County\n382385.207204\n1831.782312\n\n\nBucks County\n413498.955059\n1907.576110\n\n\nChester County\n440770.335977\n1880.804478\n\n\n\n\n\n\n\nNote: Philadelphia also had the lowest average price (about $209k), in addition to the lowest growth in prices, since the pandemic started.\n\n\nBonus: a matplotlib preview\nThe week 2 lectures will dive into (probably too much) detail about matplotlib and data visualization in Python. For now, let‚Äôs do a quick and dirty plot for illustration purposes only. The matplotlib details here aren‚Äôt important\n\n# The import statement matplotlib\nfrom matplotlib import pyplot as plt\n\nWe‚Äôll plot the ZORI/ZHVI values for each county from March 2020 through July 2023. One way to do this is to use the same groupby operation we used earlier.\nAs shown below, if you iterate over the groupby object, you will get two things back each time:\n\nThe value of the thing you grouped over\nThe dataframe holding the data for that group.\n\nWe will use this trick to group by ‚ÄúRegionName‚Äù and when we iterate over the groups, we can easily plot the data for each group on the same axes. It will look like this:\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Make our plots, each time using group_df\n    ...\nHere we go‚Ä¶plot the ZORI values.\nTo better see the growth since March 2020, we will normalize the y-axis to 1 in March 2020:\n\n# Create the figure and axes\nfig, ax = plt.subplots()\n\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Trim to where Date &gt; March 2020\n    # Sort by Date in ascending order\n    group_df = group_df.sort_values(\"Date\").query(\"Date &gt;= '2020-03-01'\")\n\n    # Date vs. ZORI\n    x = group_df[\"Date\"]\n    y = group_df[\"ZORI\"]\n    \n    # Trime\n    ax.plot(x, y / y.iloc[0], label=countyName)\n\n# Format and add a legend\nax.set_title(\"ZORI Values, Normalized to 1 in March 2020\")\nax.set_xticks([\"2020-03-31\", \"2023-07-31\"])\nax.legend(fontsize=10);\n\n\n\n\nPlot the ZHVI values, normalized to 1 in March 2020:\n\n# Create the figure and axes\nfig, ax = plt.subplots()\n\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Trim to where Date &gt; March 2020\n    # Sort by Date in ascending order\n    group_df = group_df.sort_values(\"Date\").query(\"Date &gt;= '2020-03-01'\")\n\n    # Date vs. ZHVI\n    x = group_df[\"Date\"]\n    y = group_df[\"ZHVI\"]\n    \n    # Trime\n    ax.plot(x, y / y.iloc[0], label=countyName)\n\n# Format and add a legend\nax.set_title(\"ZHVI Values, Normalized to 1 in March 2020\")\nax.set_xticks([\"2020-03-31\", \"2023-07-31\"])\nax.legend(fontsize=10);\n\n\n\n\nTakeaway: You can clearly see that price growth in Philadelphia significantly lags the growth in its suburb counties."
  },
  {
    "objectID": "content/week-1/lecture-1B.html#first-homework-assignment",
    "href": "content/week-1/lecture-1B.html#first-homework-assignment",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "First Homework Assignment",
    "text": "First Homework Assignment\nAvailable on GitHub:\nhttps://github.com/MUSA-550-Fall-2023/assignment-1\nTwo parts:\n\nDownload and install Python locally on your computer. Instructions in the assignment README!\nExplore the ‚ÄúDonut Effect‚Äù using Zillow ZHVI data by ZIP code in Philadelphia, and submit your Jupyter notebook.\n\n\n\n\n\n\n\nImportant\n\n\n\nDue date: Monday 9/25 by the end of the day (11:59 PM)"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#thats-it",
    "href": "content/week-1/lecture-1B.html#thats-it",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "That‚Äôs it!",
    "text": "That‚Äôs it!\n\nNext week: Data Visualization Fundamentals\nOffice hours:\n\nNick: Mondays, 8:00PM - 10:00PM, remote\nTeresa: Fridays, 10:30AM - 12:00PM, remote\nSign up for 15-minute time slots on Canvas (Zoom info in calendar invite)\n\nPost any questions on Ed Discussion!\nEmail questions/concerns to nhand@design.upenn.edu"
  },
  {
    "objectID": "assignment/overview.html",
    "href": "assignment/overview.html",
    "title": "Overview",
    "section": "",
    "text": "There are six homework assignments due throughout the semester and one final project that is due at the end of the finals period. For grading purposes, the assignment with the lowest grade will not count towards your final grade."
  },
  {
    "objectID": "assignment/overview.html#guidelines",
    "href": "assignment/overview.html#guidelines",
    "title": "Overview",
    "section": "Guidelines",
    "text": "Guidelines\nWe will be using Jupyter notebook files for assignments. Below are a few guidelines to make this process as smooth as possible.\n1. Notebook files should be a polished, finished product.\n\nPlease be sure that your name (as well as the names of anyone you worked with) is listed at the top of the notebook file you submit.\nPlease remove any extra or unneccesary code.\nWhen possible, use markdown cells for comments or add comments directly to your Python code to help explain the steps in your analysis.\nSection headers in markdown can also be helpful to mark different sections of the analysis.\n\n2. Notebook files should be reproducible and executable.\n\nEnsure that your assignments can be executed from top to bottom and produce the desired result. This is how our course TAs will grade each assignment.\nIf your analysis loads a dataset, make sure you either A) Add the dataset to the repository, or B) document the original source of the data so it can be downloaded during grading.\nMake sure you are using relative file paths when loading your data so that the TA can download your repository and the file structure will still work. For more information, check out the guide on file paths."
  },
  {
    "objectID": "assignment/overview.html#submission",
    "href": "assignment/overview.html#submission",
    "title": "Overview",
    "section": "Submission",
    "text": "Submission\nWe‚Äôll be using GitHub Classroom to submit homework assignments. For each assignment, be on the look out for the submission url, which will always start with the following: https://classroom.github.com/.\nWhen you click on this submission link for each assignment, you will be prompted to log in to GitHub, and then a unique, private repository will be created on the course‚Äôs GitHub page. You will create a new private repository for each assignment. Only you and the instructors will be able to view these repositories.\nWe will be using Jupyter notebook files for assignments, and you should add your notebook to your newly created assignment repository before the deadline. You can add files to the repository through the browser (github.com) interface or using the command line locally on your machine. For more help, see these instructions."
  },
  {
    "objectID": "assignment/401/final-project.html",
    "href": "assignment/401/final-project.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Due on Wednesday, December 20 at 11:59 PM\nThe final project is to replicate the pipeline approach on a dataset (or datasets) of your choosing.\nThe final deliverable will be a web-based data visualization and accompanying description including a summary of the results and the methods used in each step of the process (collection, analysis and visualization)."
  },
  {
    "objectID": "assignment/401/final-project.html#due-dates",
    "href": "assignment/401/final-project.html#due-dates",
    "title": "Final Project Proposal",
    "section": "Due dates",
    "text": "Due dates\nNote: Due dates are tentative and subject to change\nWritten proposal due date: Monday, December 4th (or earlier)\nProject due date: Wednesday, December 20th at 11:59 PM\nFinal projects must receive prior approval in the form of a written proposal."
  },
  {
    "objectID": "assignment/401/assignment-5.html",
    "href": "assignment/401/assignment-5.html",
    "title": "HW #5",
    "section": "",
    "text": "Assigned on Monday, November 6\n\n\nDue on Monday, November 20 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-5\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/401/assignment-2.html",
    "href": "assignment/401/assignment-2.html",
    "title": "HW #2",
    "section": "",
    "text": "Assigned on Monday, September 25\n\n\nDue on Monday, October 9 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-2\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/401/assignment-1.html",
    "href": "assignment/401/assignment-1.html",
    "title": "HW #1",
    "section": "",
    "text": "Assigned on Wednesday, September 6\n\n\nDue on Monday, September 25 at 11:59 PM"
  },
  {
    "objectID": "assignment/401/assignment-1.html#due-date",
    "href": "assignment/401/assignment-1.html#due-date",
    "title": "HW #1",
    "section": "Due Date",
    "text": "Due Date\n\nSection 401: Monday, September 25 at 11:59 PM\n\nThis week‚Äôs assignment will be broken into two parts:"
  },
  {
    "objectID": "assignment/401/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "href": "assignment/401/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "title": "HW #1",
    "section": "Part 1: Installing Python locally and launching a Jupyter notebook",
    "text": "Part 1: Installing Python locally and launching a Jupyter notebook\nFollow the initial installation guide on the course website for instructions on how to set up Python locally and launch JupyterLab\nPlease see the list of recommended readings for tutorials and background reading to get familiar with Python, mamba/conda, and the Jupyter notebook.\nProblems? Post your question to Ed Discussion!\n\nWorking Locally\nIf you‚Äôve successfully followed the installation guide, you should be able to launch JupyterLab by running jupyter lab (after activating the course environment) from the command line. This should launch the JupyterLab dashboard. Now you can launch the assignment notebook ‚Äúassignment-1.ipynb‚Äù and get started on the homework!\nThe notebook will execute code from the current working directory (the folder that the notebook was launched from). This folder is usually your home folder on your laptop. If you are using relative file paths to load the data, the path should be relative to this working directory. From within the Jupyter notebook, you can find out the current working directory by running the following command in a cell:\npwd\nIf you‚Äôve downloaded the assignment-1 repository to your computer, it usually makes sense to launch the Jupyter notebook from this folder instead of the default folder. You can change the start-up folder by first navigating to your assignment folder in the command line: see instructions here."
  },
  {
    "objectID": "assignment/401/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "href": "assignment/401/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "title": "HW #1",
    "section": "Part 2: Exploring the Donut Effect for Philadelphia ZIP codes",
    "text": "Part 2: Exploring the Donut Effect for Philadelphia ZIP codes\nIn part #2, you will explore the Donut Effect for home values in different parts of Philadelphia. This part will be submitted as a Jupyter notebook (a .ipynb file). There is already a starter notebook in this repository to help guide you through this part.\n\nSubmission\nNote: Be sure to read the documentation for assignment guidelines and submission.\nYou will submit your assignment through GitHub. For each assignment, I will provide a GitHub link that can be used to create a new repository. Each student will have their own private repository on GitHub where the assignment can be submitted. Only the student and instructors will have access to the private repository.\nThe invitation link depends on your section. For this assignments, the links are:\n\nSection 401: https://classroom.github.com/a/jpzX7AF4\n\nIf you do not have a GitHub account yet, you should be prompted to make an account. After clicking on this link, GitHub will create a new private repo with permissions such that only you and the instructors can view the commits.\nYour assignment should be added to this newly crearted GitHub repository before the deadline. You can add files to the repository through the web (github.com) interface or using the command line on your laptop.\nImportant: Files should be committed to the newly created private repository (after following the above link) and not to your forked version of the assignment-1 repository.\nYour Jupyter notebook should be submitted to your private repository by the deadline."
  },
  {
    "objectID": "assignment/402/final-project-proposal.html",
    "href": "assignment/402/final-project-proposal.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Due on Monday, December 4 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/final-project-proposal\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/402/assignment-4.html",
    "href": "assignment/402/assignment-4.html",
    "title": "HW #4",
    "section": "",
    "text": "Assigned on Thursday, October 19\n\n\nDue on Thursday, November 2 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-4\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/402/assignment-6.html",
    "href": "assignment/402/assignment-6.html",
    "title": "HW #6",
    "section": "",
    "text": "Assigned on Tuesday, November 21\n\n\nDue on Monday, December 4 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-6\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/402/assignment-3.html",
    "href": "assignment/402/assignment-3.html",
    "title": "HW #3",
    "section": "",
    "text": "Assigned on Thursday, September 28\n\n\nDue on Thursday, October 19 at 11:59 PM\n\n\n\n\n¬†View materials:¬†MUSA-550-Fall-2023/assignment-3\n\n\n¬†Submission link:¬†GitHub classroom\n\n\nCheck back later for details!"
  },
  {
    "objectID": "assignment/402/schedule.html",
    "href": "assignment/402/schedule.html",
    "title": "Assignments: Section 402",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n\n\n\n\n\n   Assigned on\n\n\n   Due on\n\n\n\n\n\n\nHW #1\n\n\nThursday, August 31\n\n\nThursday, September 14\n\n\n\n\nHW #2\n\n\nThursday, September 14\n\n\nThursday, September 28\n\n\n\n\nHW #3\n\n\nThursday, September 28\n\n\nThursday, October 19\n\n\n\n\nHW #4\n\n\nThursday, October 19\n\n\nThursday, November 2\n\n\n\n\nHW #5\n\n\nThursday, November 2\n\n\nTuesday, November 21\n\n\n\n\nHW #6\n\n\nTuesday, November 21\n\n\nMonday, December 4\n\n\n\n\nFinal project proposal\n\n\n\n\nMonday, December 4\n\n\n\n\nFinal project\n\n\n\n\nWednesday, December 20"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to the course syllabus for MUSA 550, Geospatial Data Science in Python, taught at the University of Pennsylvania in fall 2023."
  },
  {
    "objectID": "syllabus.html#overview",
    "href": "syllabus.html#overview",
    "title": "Syllabus",
    "section": "Overview",
    "text": "Overview\nThis course will provide students with the knowledge and tools to turn data into meaningful insights, with a focus on real-world case studies in the urban planning and public policy realm. Focusing on the latest Python software tools, the course will outline the ‚Äúpipeline‚Äù approach to data science. It will teach students the tools to gather, visualize, and analyze datasets, providing the skills to effectively explore large datasets and transform results into understandable and compelling narratives. The course is organized into five main sections:\n\nExploratory Data Science: Students will be introduced to the main tools needed to get started analyzing and visualizing data using Python.\nIntroduction to Geospatial Data Science: Building on the previous set of tools, this module will teach students how to work with geospatial datasets using a range of modern Python toolkits.\nData Ingestion & Big Data: Students will learn how to collect new data through web scraping and APIs, as well as how to work effectively with the large datasets often encountered in real-world applications.\nFrom Exploration to Storytelling: With a solid foundation, students will learn the latest tools to present their analysis results using web-based formats to transform their insights into interactive stories.\nGeospatial Data Science in the Wild: Armed with the necessary data science tools, the final module introduces a range of advanced analytic and machine learning techniques using a number of innovative examples from modern researchers."
  },
  {
    "objectID": "syllabus.html#logistics",
    "href": "syllabus.html#logistics",
    "title": "Syllabus",
    "section": "Logistics",
    "text": "Logistics\nThere are two sections for this course, 401 and 402. Info for these sections is below.\n\nLecture\nThe course will be conducted in weekly sessions devoted to lectures, interactive demonstrations, and in-class labs.\n\nSection 401\n\nMonday & Wednesday, 8:30 AM to 10:00 AM\nWilliams Hall, Room 202\n\n\n\nSection 402\n\nThursday, 1:45 PM to 4:45 PM\nMeyerson Hall, Room B2\n\n\n\n\nContact Info\n\nSection 401\n\nInstructor: Nick Hand, nhand@design.upenn.edu\nTeaching Assistant: Teresa Chang, thchang@design.upenn.edu\n\n\n\nSection 402\n\nInstructor: Eric Delmelle, ericdel@design.upenn.edu\nTeaching Assistant: Jinze Wang, wangjz@seas.upenn.edu\n\n\n\n\nOffice Hours\nOffice hours will be by appointment via Zoom ‚Äî you should be able to sign up for 1 (or more) 15-minute time slot via the Canvas calendar.\n\nSection 401\nNick:\nMondays, 8:00PM-10:00PM ‚Äî remote, sign up for slots on Canvas calendar\nTeresa:\nFridays, 10:30AM-12:00PM ‚Äî remote, sign up for slots on Canvas calendar\n\n\nSection 402\nEric:\nThursday 10am - 12pm (2nd floor conference room)\nJinze:\nWednesdays, 11:00AM-12:30PM ‚Äî remote, sign up for slots on Canvas calendar\n\n\n\nCourse Websites\n\nMain website: https://musa-550-fall-2023.github.io\nGitHub: https://www.github.com/MUSA-550-Fall-2023\nCanvas: https://canvas.upenn.edu/courses/1740535\nEd Discussion https://edstem.org/us/courses/42616/discussion/\n\nThe course‚Äôs main website will be the main source of information, including the course schedule, weekly content, and guides/resources.\nThe course‚Äôs GitHub page will have repositories for each week‚Äôs lectures as well as assignments. Students will also submit their assignments through GitHub.\nWe will use Canvas signing up for office hours and tracking grades.\nEd Discussion is a Q&A forum that allows students to ask questions related to lecture materials and assignments.\n\n\nAssignments\nThere are six homework assignments and one required final project at the end of the semester. While you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nFor the final project, students will replicate the pipeline approach on a dataset (or datasets) of their choosing. Students will be required to use several of the analysis techniques taught in the class and produce a web-based data visualization that effectively communicates the empirical results to a non-technical audience. The final product should also include a description of the methods used in each step of the data science process (collection, analysis, and visualization).\nFor more details on the final project, see the GitHub repository.\n\n\nGrading\nThe grading breakdown is as follows: 50% for homework; 45% for final project, 5% for participation. Your participation grade will be determined by your activity on Ed Discussion ‚Äî both asking, answering, and reading questions.\nWhile you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nThere‚Äôs no penalty for late assignments. I would highly recommend staying caught up on lectures and assignments as much as possible, but if you need to turn something in a few days late, that‚Äôs fine ‚Äî there‚Äôs no penalty. If you turn in something late, you‚Äôll be missing out on valuable feedback, but that‚Äôs the only practical penalty, there‚Äôs no extra penalty to your grade.\n\n\nSoftware\nThis course relies on use of Python and various related packages and for geospatial topics. All software is open-source and freely available. The course will require a working installation of Python on your local computer. See the Installation Setup Guide for instructions on how to setup your computer for use in this course."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Syllabus",
    "section": "Policies",
    "text": "Policies\nMUSA 550 is a fast-paced course that covers a lot of topics in a short amount of time. I know that it can be overwhelming and frustrating, particularly as you are trying to learn Python syntax and the topics in the course at the same time. But I firmly believe that all students can succeed in this class.\nYou‚Äôll get the most out of the course if you stay up to date on the lectures and assignments. If you fall behind, I know there can be a desire to copy code from the Internet or others to help you complete assignments. Ultimately, this will be detrimental to your progress as an analytics wizard. My goal for this course is for everyone to learn and engage with the material to the best of their ability.\n\nIf you find yourself falling behind or struggling with Python issues, please ask for help by:\n\nPost a question on Ed Discussion ‚Äî the fix for your problem might be quick and other students are probably experiencing similar issues.\nCome to office hours and discuss issues or larger conceptual questions you are having.\nTake advantage of the free resources to help fine-tune your Python skills.\n\n\nAnd if you are still struggling, reach out and let me know and we‚Äôll figure out a strategy to make things work!\n\nCommunication Policies\n\nPlease add the following text into the subject line of emails to us: [MUSA550]. This will help us make sure we don‚Äôt miss your email!\nWe will use the Ed Discussion Q&A forum for questions related to lecture material and assignments.\nTo prevent code copying, please do not post long, complete code examples to Ed Discussion.\nAnonymous posting is enabled on Ed Discussion ‚Äî if you have a question that requires a full code example, please use the anonymous feature to post the question.\nWe will also use Ed Discussion for announcements ‚Äî please make sure your notifications are turned on and you check the website frequently. This will be the primary method of communication for course-wide announcements.\nIf you have larger-scale or conceptual questions on assignments or lecture material, please set up a time to discuss during office hours.\n\n\n\nGroup Work\nStudents are allowed (and encouraged!) to collaborate when working through lecture materials or assignments. If you work closely with other students, please list the members of your group at the top of your assignment.\n\n\nSpecial Accommodations\nThere are a number of ongoing situations in the world that may take precedence over the course work. If you are experiencing any difficulties outside the course, please contact me and accommodations can be made. Similarly, if you are having any difficulties with the course schedule, attending lectures, or similar, please let us know.\n\n\nAcademic Integrity\nStudents are expected to be familiar with and comply with Penn‚Äôs Code of Academic Integrity, which is available in the Pennbook, or online at https://catalog.upenn.edu/pennbook/code-of-academic-integrity."
  },
  {
    "objectID": "resource/mamba.html",
    "href": "resource/mamba.html",
    "title": "Using mamba",
    "section": "",
    "text": "In this guide, we‚Äôll outline some of the key concepts and common uses to get you up and running with mamba. A conda cheatsheet is also available under the ‚ÄúCheatsheets‚Äù section in the left sidebar."
  },
  {
    "objectID": "resource/mamba.html#key-concepts",
    "href": "resource/mamba.html#key-concepts",
    "title": "Using mamba",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nCommands\nWe‚Äôll be using the mamba tool through its command-line interface rather than a typical graphical user interface (GUI) application. If you are unfamiliar with the command line, this will take some getting used to but once you get the hang of it, it will make working with mamba and Python much easier.\nThe mamba command is the main interface for using the mamba tool for managing your Python packages. From the command line, you simply run:\nconda command [optional arguments will go here]\nwhere ‚Äúcommand‚Äù is the name of the command you want to run. Commands exist to install new packages, create new environments, and much more.\n\n\nStarting and running mamba\nWe will run mamba from the command line but the specifics of this will depend on your operating system.\n\nWindows\nOpen the Start menu, search for and open the ‚ÄúMiniforge Prompt‚Äù. This application provides a command line interface where the mamba tool is properly load, initialized, and ready to be used. Note that you cannot use the default ‚ÄúCommand Prompt‚Äù application to use mamba because it doesn‚Äôt know how to load mamba properly.\nMacOS\nThe Terminal app should be used on MacOS to use mamba. You can also use any Terminal emulator (such as iTerm2). Simply open the Terminal application and the mamba command should be ready to use.\n\n\n\nChannels\n‚ÄúChannels‚Äù are the locations where packages are located. Channels are typically remote and hosted in the cloud. When you specify a channel, mamba will search the remote database for the right package and download it to your local computer.\nBy default, conda usually downloads packages from the defaults channel, which hosts thousands of packages and is managed by the makers of the Anaconda distribution. A full list of packages is available here.\nThe ‚Äúmambaforge‚Äù distribution that we installed in this course is pre-configured to use a community-managed channel known as ‚Äúconda-forge‚Äù instead of the ‚Äúdefaults‚Äù channel. Conda forge includes many of the packages on the ‚Äúdefaults‚Äù channel but also popular packages that are widely-used but not quite essential enough for the ‚Äúdefaults‚Äù channel. A list of maintained packages is available here.\nFor less well known packages, there is a higher likelihood the package will be hosted on conda forge. For that reason, we will prefer downloading and installing packages from conda forge in this course.\n\n\nEnvironments\nThe mamba tool not only lets you download and install packages, but you can group those packages together into environments. By default, the ‚Äúmambaforge‚Äù Python distribution creates an environment named base. We will create a new environment specifically for this course that will hold all of the packages needed for the entire semester.\nEnvironments become particularly useful when working with lots of packages, packages that have a lot of dependencies, or packages that are difficult to install. When environments become too large, it can be difficult to install a new package that satisfies all of the existing package dependencies. For that reason, we will create a fresh, new environment to install the packages we need to use during this course.\n\n\nConda/mamba vs.¬†pip\nThe other widely used method for installing packages is via the pip command. The commands are similar in a lot of ways but with some key differences. The pip command installs packages from the Python Package Index and is designed to install Python-only packages.\nThe main advantage of conda/mamba is that it is cross-platform and can handle dependencies that are written in C (or other languages) and will automatically handle the compiling process during installation. Many of the packages we use in this course have complex dependencies written in C, and mamba will make installation of these packages much easier.\nIn this course, we‚Äôll be using mamba to install packages. Generally speaking, if you already are using mamba to manage environments, it‚Äôs best to try to install packages with mamba and if the package is not available, then try using pip.\nSee this article for more information about conda and pip."
  },
  {
    "objectID": "resource/mamba.html#common-uses",
    "href": "resource/mamba.html#common-uses",
    "title": "Using mamba",
    "section": "Common Uses",
    "text": "Common Uses\nManaging environments and installing packages will be done by executing the mamba command in the command line. Below are some of the most common commands that we will use in this class.\n\n\n\n\n\n\nImportant\n\n\n\nAll of the examples below should be run in the Terminal app (MacOS) or Miniforge Prompt (Windows). See the Starting and running mamba section above for more detail.\n\n\n\nGetting help with the mamba command\nThe mamba command has a built-in help function. From the command line, run,\nmamba --help\nwhich will print out info about individual commands:\nusage: mamba [-h] [-V] command ...\n\nconda is a tool for managing and deploying applications, environments and packages.\n\nOptions:\n\npositional arguments:\n  command\n    clean             Remove unused packages and caches.\n    compare           Compare packages between conda environments.\n    config            Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .condarc file\n                      (/Users/nhand/.condarc) by default. Use the --show-sources flag to display all identified configuration locations on your computer.\n    create            Create a new conda environment from a list of specified packages.\n    info              Display information about current conda install.\n    init              Initialize conda for shell interaction.\n    install           Installs a list of packages into a specified conda environment.\n    list              List installed packages in a conda environment.\n    package           Low-level conda package utility. (EXPERIMENTAL)\n    remove (uninstall)\n                      Remove a list of packages from a specified conda environment. Use `--all` flag to remove all packages and the environment itself.\n    rename            Renames an existing environment.\n    run               Run an executable in a conda environment.\n    search            Search for packages and display associated information.The input is a MatchSpec, a query language for conda packages. See examples\n                      below.\n    update (upgrade)  Updates conda packages to the latest compatible version.\n    notices           Retrieves latest channel notifications.\n    repoquery         Query repositories using mamba.\n\noptional arguments:\n  -h, --help          Show this help message and exit.\n  -V, --version       Show the conda version number and exit.\nTo find out more info about a specific sub-command, you can run:\nmamba command --help\nFor example, for more info about the arguments (both required and optional) needed to install packages, use: mamba install --help.\n\n\nListing the available environments\nThe default environment when first installing mamba is called 'base'. You can list the currently installed Python environments by running the following command from the command line:\nmamba env list\nThe currently active environment will have a '*' next to it. You should see the 'base' environment as well as any other environments you have created.\n\n\nActivating your environment\nEnvironments must first be ‚Äúactivated‚Äù before the packages are available to use. To activate the environment for this course, you can run the following from the command line:\nmamba activate musa-550-fall-2023\nNow, all of the packages in this environment will be available when we run Python.\n\n\nFinding the active environment\nTo see the active environment, list the available environments. The active environment will be listed with a ‚Äò*‚Äô next to its name.\nFrom the command line, run:\nmamba env list\n\n\nListing the installed packages\nIf you have already activated the musa-550-fall-2023 environment, you can list all of the installed packages.\nFrom the command line:\nmamba list\n\n\nActivating the base environment\nTo activate the 'base' default environment, run from the command line:\nmamba activate base\n\n\n\n\n\n\nNote\n\n\n\nYou should always use the ‚Äòmusa-550-fall-2023‚Äô environment to do the analysis in this course. Make sure it is the activated environment when using Python.\n\n\n\n\nDeleting an environment\nNote that you cannot create a new environment with the same name as an existing environment. If your environment becomes corrupted or you run into issues, it is often easiest to delete the environment and start over. To do, you can run the following commands from the command line:\nmamba deactivate\nmamba env remove --name musa-550-fall-2023\n\n\nUpdating an existing environment\nThe environment we are using throughout the course might be need to be updated during the course. For example, we might want to update to include a newly released version of a package.\nYou can update your local environment via the following command. From the command line:\nmamba env update pennmusa/musa-550-fall-2023\nThis command will ensure that the ‚Äòmusa-550-fall-2023‚Äô environment on your local computer matches the environment specified by the ‚Äúenvironment.yml‚Äù file stored in the cloud for the course.\n\n\nInstalling specific packages\nYou shouldn‚Äôt need to install any individual packages into the ‚Äòmusa-550-fall-2023‚Äô environment. But for reference, you could install specific packages into the active environment using from the command line:\nmamba install package_name"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Helpful resources",
    "section": "",
    "text": "This section includes a number of extra resources, cheatsheets, and guides related to software installation, Python, and other relevant topics."
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing Python and inital set-up",
    "section": "",
    "text": "MUSA 550 relies on freely available software from the Python  open-source ecosystem. This guide will walk you through how to set up your computer for the course, including downloading and installing Python as well as the various packages you will need throughout the semester.\nBy the end of this guide, you‚Äôll have Python installed, be able to launch a Jupyter notebook (the interface for running Python code), and will be writing your first Python code. Let‚Äôs get started!"
  },
  {
    "objectID": "resource/install.html#step-1.-installing-python",
    "href": "resource/install.html#step-1.-installing-python",
    "title": "Installing Python and inital set-up",
    "section": "Step 1. Installing Python",
    "text": "Step 1. Installing Python\nThere are a number of different tools and ways to install Python for new users. In this course, we‚Äôll be using a package manager called mamba to install Python and manage dependencies. In the Python ecosystem, package managers are especially useful, as they greatly simplify the installation process and ensure all of your dependencies will function properly.\nmamba is a drop-in replacement (meaning it has the same functionality) for a very popular Python package manager called conda. We‚Äôll use mamba instead of conda because it has signficiantly better performance. In my experience, it‚Äôs not uncommon for it to sometimes take more than hour to install a list of Python packages with conda but only a few seconds for mamba to do the same.\n\n\n\n\n\n\nTip\n\n\n\nAnything you can do with the conda tool, you can do with the mamba tool. Outside this class, you‚Äôll likely hear about conda more often, since it‚Äôs the more popular tool at the moment.\nAs you will see later, we will often refer to the conda documentation to learn about the key concepts behind conda/mamba and its main functionality.\n\n\nThe mamba tool allows you to easily install Python packages on your laptop using environments. An environment allows you to install packages for specific purposes and keep those packages isolated from any other Python packages that might be installed on your laptop. This is very useful, since different versions of packages often don‚Äôt work nicely together. We will create an environment for use during this class that includes all of the Python packages you will need in the course.\nIn this course, we will use the ‚Äúmambaforge‚Äù distribution of Python, which includes Python, mamba, and a few other essential packages and dependencies. It also comes pre-configured with ‚Äúconda-forge‚Äù, a popular, community-maintained server that makes the most popular Python packages available for download for free.\n\n\n\n\n\n\nNote\n\n\n\nThe mambaforge distribution is the mamba version of the popular ‚ÄúMiniconda‚Äù distribution, which is a free, minimal Python installation that just includes conda. Miniconda is a lightweight version of the full Anaconda distribution. The differences between the ‚Äúmini‚Äù and ‚Äúfull‚Äù versions are outlined here.\nThe major difference is that the Anaconda distribution will install more than 1,500 of the most common scientific Python packages (many more than we need in this course) and will take up about 3 GB of disk space. Mambaforge/Miniconda will only install core Python dependencies (as well as mamba/conda) and will only take up a much smaller amount of disk space.\n\n\nThe following page contains the installation files for the mambaforge distribution:\nhttps://github.com/conda-forge/miniforge#mambaforge\nSelect the appropriate file for your computer‚Äôs operating system and click to download the file. The file should be named something like Mambaforge-Linux-*, Mambaforge-MacOSX-*, or Mambaforge-Windows-*.\nThe rest of the installation instructions will vary based on your operating system:\n\nWindows\n\nIn the file browser, double-click the .exe file that you downloaded.\nFollow the instructions on the screen. If you are unsure about any setting, accept the defaults. You can change them later.\n\nMacOS\n\nOpen the Terminal application.\nChange to the folder to the directory where the .sh installer file was downloaded (this is usually the ‚ÄúDownloads‚Äù folder) by running the following command in the Terminal app:\nbash Mambaforge-MacOSX-x86_64.sh\nor if you have a Mac with the new M2 chips, use:\nbash Mambaforge-MacOSX-arm64.sh\nFollow the instructions on the screen. If you are asked about if you ‚Äúwish the installer to initialize Mambaforge by running conda init?‚Äù, type ‚Äúyes‚Äù. This will ensure that mamba is an available command when you open up Terminal.\nIf you are unsure about any setting, accept the defaults. You can change them later.\nTo make the changes take effect, close and then re-open your terminal window."
  },
  {
    "objectID": "resource/install.html#verify-that-your-python-installation-is-working",
    "href": "resource/install.html#verify-that-your-python-installation-is-working",
    "title": "Installing Python and inital set-up",
    "section": "2. Verify that your Python installation is working",
    "text": "2. Verify that your Python installation is working\nTo verify that the install worked, we will run mamba from the command line. The specifics of this will depend on your operating system:\n\nWindows\nOpen the Start menu, search for and open the ‚ÄúMiniforge Prompt‚Äù. This application provides a command line interface where the mamba tool is properly loaded, initialized, and ready to be used.\nNote that you cannot use the default ‚ÄúCommand Prompt‚Äù application to use mamba because it doesn‚Äôt know how to load mamba properly.\nMacOS\nThe Terminal app should be used on MacOS to use mamba. You can also use any Terminal emulator (such as iTerm2). Simply open the Terminal application and the mamb command should be ready to use.\n\nNow, let‚Äôs test your installation. From the command line, run: mamba list. A list of installed packages should be printed to the screen if the installation was successful.\nAfter you‚Äôve sucessfully installed the mambaforge distribution, you will have Python 3.10 installed with a default environment called ‚Äúbase‚Äù."
  },
  {
    "objectID": "resource/install.html#create-your-first-python-environment",
    "href": "resource/install.html#create-your-first-python-environment",
    "title": "Installing Python and inital set-up",
    "section": "3. Create your first Python environment",
    "text": "3. Create your first Python environment\nThe mamba/conda tool allows us to easily install new Python packages and keep track of which ones we‚Äôve already installed. I‚Äôve put together a list of the packages we‚Äôll need in this course (a group of packages is known as an environment in mamba-speak). Note that you‚Äôll be using the command line (either the Miniforge Prompt in Windows or Terminal app in MacOS) to run mamba and create your environment.\nThroughout this course, we will maintain an environment called ‚Äúmusa-550-fall-2023‚Äù to install and manage all of the packages needed throughout the semester.\nThe packages in an environment are specified in a file typically called ‚Äúenvironment.yml‚Äù. The environment file for this course is stored in the course-materials repository on Github and a copy is also stored in the cloud on anaconda.org.\nIt is recommended to create the ‚Äòmusa-550-fall-2023‚Äô environment on your local computer using the environment file stored in the cloud on anaconda.org. The instructions to do so are as follows:\n\nFirst, we need to make sure the anaconda-client package is installed locally. This will ensure that mamba can interface with anaconda.org. From the command line (Miniforge Prompt on Windows or Terminal on MacOS), run:\nmamba install anaconda-client -n base\nThis will install the anaconda-client package into the default ‚Äúbase‚Äù environment.\nThen, create the musa-550-fall-2023 environment by running:\nmamba env create pennmusa/musa-550-fall-2023\nAfter this command finishes, all of the packages we need for the course should be installed. To verify this, you can run mamba env list from the command line to see the installed environments. If everything worked, you should now see the 'musa-550-fall-2023' environment listed.\n\n\n\n\n\n\n\nNote\n\n\n\nThis semester, we will be using Python version 3.10. More information about the different versions of Python is available here."
  },
  {
    "objectID": "resource/install.html#activate-the-courses-environment",
    "href": "resource/install.html#activate-the-courses-environment",
    "title": "Installing Python and inital set-up",
    "section": "4. Activate the course‚Äôs environment",
    "text": "4. Activate the course‚Äôs environment\nOnce you‚Äôve created your new environment and installed the Python packages for the course, you need to tell mamba to activate it (more mamba-speak) so that you can actually use the packages when you are writing Python code.\nTo activate the environment for this course, you can run the following from the command line (Miniforge Prompt on Windows or Terminal on MacOS):\nmamba activate musa-550-fall-2023\nNow, all of the packages in this environment will be available when we run Python.\n\n\n\n\n\n\nImportant\n\n\n\nIf you forget to activate the course‚Äôs environment, you will be using the default ‚Äúbase‚Äù environment. This has some of the packages we will need, but many will be missing. If you are trying to import Python packages and get a ‚ÄúModuleNotFoundError‚Äù error, the active environoment is likely the issue!"
  },
  {
    "objectID": "resource/install.html#launching-a-jupyter-notebook",
    "href": "resource/install.html#launching-a-jupyter-notebook",
    "title": "Installing Python and inital set-up",
    "section": "5. Launching a Jupyter notebook",
    "text": "5. Launching a Jupyter notebook\nThroughout the course, we will write, edit, and execute Python code in files called Jupyter notebooks. These files have an .ipynb extension. Notebooks are documents that combine live runnable code with narrative text, images, and interactive visualizations.\nAn application called JupyterLab is the recommended way to work with these notebook files. JupyterLab is a browser-based interface that allows users to execute Python in notebook files. It can also handle all sorts of additional file formats and even has a built-in command-line feature. The JupyterLab guide provides much more information about the features of JupyterLab ‚Äî¬†for the moment, we will just focus on launching our first notebook file.\n\n\n\n\n\n\nNote\n\n\n\nThere are other interfaces for working with Jupyter notebooks. The original Jupyter notebook application has since been replaced by the more powerful JupyterLab application. Popular code editors such as VS Code have also added support for Jupyter notebooks, although it is important to note that VS Code does not support all features of notebooks. For this reason, JupyterLab remains the recommended notebook interface.\n\n\nThe recommended approach for starting a notebook is to use the Miniforge Prompt on Windows or the Terminal app on MacOS. To do so, we simply need to activate our 'musa-550-fall-2023' environment and then start JupyterLab, which is included in the course‚Äôs environment.\nFrom the command line, first activate the environment:\nmamba activate musa-550-fall-2023\nand then launch JupyterLab\njupyter lab\nThis will create the local Jupyter server and should launch the JupyterLab dashboard in a browser. If it does not open in a browser, copy the link that is output by the command into your favorite browser. Typically, the server will be running at http://localhost:8888. The dashboard should like look something like this:\n\nOn the left, you‚Äôll see a file browser for the files in the folder where you ran the jupyter lab command. On the right you will see the ‚ÄúLauncher‚Äù, which allows you to easily create various types of new files. Click on the ‚ÄúPython 3‚Äù button under the ‚ÄúNotebook‚Äù section and you‚Äôll create your first notebook. Alternatively, you can use the File -&gt; New -&gt; Notebook option from the menu bar. The new notebook, entitled ‚ÄúUntitled.ipynb‚Äù, is created within the same directory.\nNow, let‚Äôs type the following into the first cell:\nprint(\"Hello, World!\")\nClick the ‚èµ button in the menu bar of the notebook and you will run your first Python code in a notebook!\n\n\n\n\n\n\n\nTip\n\n\n\nThe Jupyter notebook section of the JupyterLab walks you through each step of working with notebook files in JupyterLab. Check it out here, or find more information in the JupyterLab guide."
  }
]