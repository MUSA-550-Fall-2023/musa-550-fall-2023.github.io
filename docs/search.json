[
  {
    "objectID": "resource/file-paths.html",
    "href": "resource/file-paths.html",
    "title": "File paths and working directories",
    "section": "",
    "text": "Note\n\n\n\nBelow is a guide from one of our course’s previous TAs, Eugene Chong. It does a great job laying out the common issues associated with file paths and Python.\nA common challenge in collaborative data science work is dealing with file paths. A csv on your computer has a different file path than a csv on your colleague’s computer, and if you don’t plan ahead for that, your colleague won’t be able run your code on their computer without making (possibly many) changes.\nThis post contains a suggestion for how to organize your files and a glossary of terms. I’ll update it as things come up. Please let us know any questions!"
  },
  {
    "objectID": "resource/file-paths.html#absolute-v.-relative-file-paths",
    "href": "resource/file-paths.html#absolute-v.-relative-file-paths",
    "title": "File paths and working directories",
    "section": "Absolute v. relative file paths",
    "text": "Absolute v. relative file paths\nProblem: I write pd.read_csv(\"C:/Users/eugene/data.csv\") in my code and send it to a project partner, Gritty. Gritty tries to run it, but it fails, and they have to change the code to pd.read_csv(\"C:/Users/gritty/data.csv\") to make it work. These are known as absolute file paths, which point to the exact location of a file on a computer, and generally they make it difficult to share code.\nOne way to deal with this is to organize your projects (e.g., a homework assignment) into self-contained folders. Something like the below:\nC:\n|__ /Users\n    |__ /eugene\n        |__ /MUSA550\n            |__ /Homework1\n                 |__ homework1_notebook.ipynb\n                 |__ /Data\n                      |__ data.csv\nWith this, I could instead write pd.read_csv(\"Data/data.csv\"). Then, I could send Gritty my entire Homework1 folder or upload that folder to GitHub as a repository, and Gritty can run the notebook without making any changes. These are relative file paths, which point to the location of a file on a computer relative to the working directory (i.e., the folder where homework1_notebook.ipynb is saved).\nFor your homework assignments, the most straightforward way to structure your files is to download the entire GitHub repository for that assignment (see below). Then, you can create a Jupyter notebook in that folder and, when you’re finished, upload it to GitHub. This way, when we download the repository for grading, or you open the notebook on a different computer, everything will run without changes (assuming you used relative file paths)."
  },
  {
    "objectID": "resource/file-paths.html#glossary",
    "href": "resource/file-paths.html#glossary",
    "title": "File paths and working directories",
    "section": "Glossary",
    "text": "Glossary\nThese terms/commands work in Jupyter Notebooks, and they also apply to any command line tools you might encounter, like the Terminal, git bash, etc. except the Windows Command Prompt, Miniforge Prompt (which is actually just a wrapper around the Command Prompt), and Windows PowerShell.\nhome directory: Also referred to as ~. This is the directory for your particular user on your computer. In Windows, it’s usually something like C:/Users/eugene. If you open Terminal/Miniforge Prompt, it will be at this location by default.\nroot directory: Also referred to as /. This is the very highest level directory in your computer, where your operating system folders and such are located. We won’t be doing anything here, since deleting files can mess things up (I don’t think you can even open the root directory in the Windows File Explorer).\nworking directory: Also referred to as .. Relative file paths will be relative to this location on your computer. You can run the command pwd (“print working directory”) in either your Jupyter Notebook or the Terminal to see your current working directory. Note that ./subfolder/data.csv and subfolder/data.csv are the same; the first explicitly references the working directory, whereas it’s only implied in the second."
  },
  {
    "objectID": "resource/common-issues.html",
    "href": "resource/common-issues.html",
    "title": "Troubleshooting common installation issues",
    "section": "",
    "text": "Having trouble with mamba/conda or your Python installation? You’ve come to the right place. Below, we outline some of the most common issues encountered during local installation of Python packages, as well as the troubleshooting steps to try to fix the issues."
  },
  {
    "objectID": "resource/common-issues.html#common-problems",
    "href": "resource/common-issues.html#common-problems",
    "title": "Troubleshooting common installation issues",
    "section": "Common Problems",
    "text": "Common Problems\nBelow we list some of the most common issues encountered when installing packages with mamba.\n\nMissing package errors\nIf you have successfully followed the steps outlined in the installation guide to create your environment, but receive an ImportError when importing packages, you might have launched the notebook from the 'base' environment instead of the ‘musa-550-fall-2023’ environment. Be sure to activate the ‘musa-550-fall-2023’ environment before launching the notebook.s\n\n\nThe file extension of the environment file on Windows\nBe sure that Windows does not automatically add an .txt extension to your environment.yml file. This will sometimes cause mamba to fail, with a cryptic error:\nSpecNotFound: environment with requirements.txt needs a name\nThe environment file needs to end in .yml. You can change the extension for a file on Windows following these instructions.\n\n\nMixing pip and mamba\nThe command pip can also be used to install Python packages. However, using pip to install packages into a mamba environment can lead to issues. It’s best to stick to using the mamba env update command to update your environment or mamba install package_name to install specific packages.\n\n\nImport errors for geopandas\nWhen importing geopandas, you can sometimes receive errors about missing libraries. This is usually because package versions got mixed up during installation. This can sometimes happen, and geopandas is particularly sensitive to the versions of its dependencies.\nThe best and easiest thing to do to try to solve it is use the steps above to create a fresh environment.\n\n\nNumpy errors\nIf you receive the following error:\nImportError: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/path/to/old/version/of/numpy/'']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.\nFrom the Miniforge Prompt (Windows) or Terminal (Mac), run:\nmamba install --force-reinstall --clobber numpy"
  },
  {
    "objectID": "resource/common-issues.html#most-common-fix-install-a-fresh-environment",
    "href": "resource/common-issues.html#most-common-fix-install-a-fresh-environment",
    "title": "Troubleshooting common installation issues",
    "section": "Most common fix: install a fresh environment",
    "text": "Most common fix: install a fresh environment\nUnfortunately, mamba/conda environments can sometimes become corrupted, preventing new packages from being installed or imported into Python properly. Most issues like this can be solved by simply deleting your current environment and starting fresh with a new version.\nThe following steps can be used to try to solve common issues:\n\n\n\n\n\n\nNote\n\n\n\nThe commands here should be executed via the command line, either using the Miniforge Prompt on Windows or the Terminal app on MacOS.\n\n\n\nStep 1: Delete any existing environment\nWe want to create a fresh environment, so you can delete any environment that was giving you issues. If that environment was called ‘musa-550-fall-2023’, you can run the following commands to delete it:\nmamba deactivate\nmamba env remove --name musa-550-fall-2023\nThe first command will make sure the environment we are deleting isn’t active, and then the second command will perform the deletion.\n\n\nStep 2: Create a fresh environment\nFollow the instructions outlined here to create a fresh version of the course website."
  },
  {
    "objectID": "resource/python.html",
    "href": "resource/python.html",
    "title": "Python resources",
    "section": "",
    "text": "MUSA 550 assumes some general familiarity with programming concepts, but there aren’t any formal Python prerequisites. However, we recommend all students that they use some of the free, online resources for learning Python’s core concepts. Below, we include a number of online resources that are freely available for students in the course."
  },
  {
    "objectID": "resource/python.html#datacamp-courses",
    "href": "resource/python.html#datacamp-courses",
    "title": "Python resources",
    "section": "DataCamp courses",
    "text": "DataCamp courses\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great — the more the merrier!\n\n\n\n\n\n\nImportant\n\n\n\nTo gain access, use this unique invite link. You will need to create or sign in to a DataCamp account with an “upenn.edu” email address.\n\n\nIntroductory DataCamp courses include:\n\nIntroduction to Python for Data Science\nPython Data Science Toolbox, Part 1\nPython DataScience Toolbox, Part 2\nIntroduction to NumPy\n\nAnd there are also shorter, free tutorials available on some core Python concepts:\n\nIf/else statements\nFor loops\nWhile loops\n\nA few courses covering more advanced topics include:\n\nIntermediate Python\nWriting functions in Python\n\nThere are also courses available to help reinforce topics we will cover in detail during the semester, including:\n\nData manipulation with pandas\nJoining data with pandas\nIntroduction to Data Visualization with seaborn\nIntroduction to Data Visualization with matplotlib\nIntermediate Data Visualization with seaborn\nSupervised Learning with scikit-learn\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the full list of available Python courses on DataCamp’s website."
  },
  {
    "objectID": "resource/python.html#introductory-python-tutorials",
    "href": "resource/python.html#introductory-python-tutorials",
    "title": "Python resources",
    "section": "Introductory Python tutorials",
    "text": "Introductory Python tutorials\nThe following tutorials assume no background in Python and provide a fairly comprehensive introduction to Python and its core concepts.\n\nPractical Python Programming by David Beazley\nPython for Social Science (in particular, the first four chapters)\nScientific Python Basics from the Berkeley Institute for Data Science (notebook version)"
  },
  {
    "objectID": "resource/python.html#more-advanced-tutorials",
    "href": "resource/python.html#more-advanced-tutorials",
    "title": "Python resources",
    "section": "More advanced tutorials",
    "text": "More advanced tutorials\nThe The Python Data Science Handbook by Jake VanderPlas is a free, online textbook covering the Python basics needed for this course. It is a bit more advanced than the resources in the previous section and assumes some familiarity with Python.\nIn particular, the first four chapters are excellent:\n\nChapter 1: IPython/Jupyter\nChapter 2: Numpy\nChapter 3: Pandas\nChapter 4: matplotlib\n\nThe data analysis library pandas and the visualization library matplotlib will be covered extensively in this course, but the above chapters provide additional background material on this foundational Python tools.\nNote: You can click on the “Open in Colab” button for each chapter and run the examples interactively in a cloud computing environment directly in the browser (using Google Colab)."
  },
  {
    "objectID": "resource/python.html#additional-resources",
    "href": "resource/python.html#additional-resources",
    "title": "Python resources",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Berkeley Institute for Data Science has compiled a number of Python resources\nThe subreddit r/learnpython is a good place for Python resources — it maintains a comprehensive wiki of resources and tutorials."
  },
  {
    "objectID": "resource/jupyter.html",
    "href": "resource/jupyter.html",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "",
    "text": "In this course, we will perform most of our Python data analysis in files known as Jupyter notebooks. These files, which have an extension of “.ipynb”, combine live runnable code with narrative text (via the Markdown language), images, interactive visualizations and other rich output.\nTo work with notebook files, we will use an application called JupyterLab. JupyterLab is a browser-based interface that allows users to edit and execute notebook files. It can also handle all sorts of additional file formats and even has a built-in command-line feature.\nOn this page, I’ll discuss two common issues when starting out with Jupyter notebooks and JupyterLab: launching JupyterLab and ensuring the right files are available."
  },
  {
    "objectID": "resource/jupyter.html#launching-jupyterlab",
    "href": "resource/jupyter.html#launching-jupyterlab",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "Launching JupyterLab",
    "text": "Launching JupyterLab\nThe recommended approach for starting JupyterLab is to use the Miniforge Prompt on Windows or the Terminal app on MacOS. To do so, we simply need to activate our ‘musa-550-fall-2023’ environment and then launch the notebook.\nFrom the command line, run:\nmamba activate musa-550-fall-2023\njupyter lab\nThis will create the local Jupyter server and should launch the JupyterLab dashboard in a browser. If it does not open in a browser, copy the link that is output by the command into your favorite browser. Typically, the server will be running at http://localhost:8888. The dashboard should like look something like this:\n\nOn the left, you’ll see a file browser for the files in the folder where you ran the jupyter lab command. On the right you will see the “Launcher”, which allows you to easily create various types of new files. Click on the “Python 3” button under the “Notebook” section and you’ll create your first notebook. Alternatively, you can use the File -&gt; New -&gt; Notebook option from the menu bar. The new notebook, entitled “Untitled.ipynb”, is created within the same directory.\nThe “Launcher” also shows you what other actions you can take with JupyterLab, including creating text files, Python files (“.py” files), Markdown files, new terminals or Python consoles. One of the most powerful features of JupyterLab is its ability to handle multiple file formats at once. You can have multiple file types open in the main work area and drag and resize these files to view them all at once, as described here.\n\n\n\n\n\n\nTip\n\n\n\nMore info on the various components of the JupyterLab interface, with several useful videos, is available here."
  },
  {
    "objectID": "resource/jupyter.html#changing-the-jupyterlab-start-up-folder",
    "href": "resource/jupyter.html#changing-the-jupyterlab-start-up-folder",
    "title": "Using Jupyter notebooks and JupyterLab",
    "section": "Changing the JupyterLab start-up folder",
    "text": "Changing the JupyterLab start-up folder\nBy default, JupyterLab launches from the home directory. When you see the file browser on the left of the dashboard, you should see all of the files in this folder.\nWhen working with weekly lectures or assignments, it is easiest to launch JupyterLab from the specific assignment or week folder that you are working on.\nThere are two options to do this:\n\nChange to the desired folder before launching JupyterLab\nUse the “notebook-dir” option to specify the desired folder when launching JupyterLab\n\n\nOption 1\n\nStep 1: Change to the desired directory\nLet’s imagine we want to change to a folder named:\n/Users/YourUserName/MUSA_550 (on a Mac),\nor\nC:\\Users\\YourUserName\\MUSA_550 (on Windows)\nIf you need help finding the folder name’s path, this guide for Windows. (I usually use Method #2). On MacOS, you can use this guide to copy a folder’s path name.\nNext, use the following steps:\nStep 1. On Windows, open the Miniforge Prompt, or on Mac, open the Terminal.\nStep 2. Navigate to the folder where the environment file is located. From the Prompt or Terminal run:\n\nWindows\ncd C:\\Users\\YourUserName\\MUSA_550\nMac\ncd /Users/YourUserName/MUSA_550/\n\n\n\nStep 2: Launch JupyterLab\nNow, type the following command, either in Anaconda Prompt or the Terminal:\njupyter lab\nAnd you should now see the desired files in the file browser on the left sidebar of the JupyterLab interface!\n\n\n\nOption 2\nFrom the command line (Miniforge Prompt or Terminal), we can use the “notebook-dir” option to specify what working directory we want JupyterLab to use. For example, if we want to start from “/Users/YourUserName/MUSA_550/”, we could do:\n\nWindows\njupyter lab --notebook-dir=C:\\Users\\YourUserName\\MUSA_550\nMac\njupyter lab --notebook-dir=/Users/YourUserName/MUSA_550/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Geospatial Data Science\n    \n      Use Python to gather, visualize, and analyze geospatial data with an urban planning and public policy focus\n    \n    \n      MUSA 550 • Fall 2023Masters of Urban Spatial AnalyticsWeitzman School of Design, University of Pennsylvania\n    \n  \n  \n    \n      \n        \n      \n    \n  \n\n\n\n\n\nInstructors\n\nSection 401\n\n   Dr. Nick Hand\n   nhand@design.upenn.edu\n   nicholashand\n\n\n\nSection 402\n\n   Dr. Eric Delmelle\n   ericdel@design.upenn.edu \n\n\n\n\nCourse details\n\nSection 401\n\n   Mondays and Wednesdays\n   8:30AM-10:00AM\n   Williams Hall 202\n\n\n\nSection 402\n\n   Thursdays\n   1:45PM-4:45PM\n   Meyerson Hall B2\n\n\n\n\nContacting us\nE-mail is the best way to get in contact with us. We will try to respond to all course-related e-mails within 24 hours, although we do have a few tiny humans 👶 at home, so it may be a bit longer on occassion. Please be patient!"
  },
  {
    "objectID": "content/week-1/lecture-1A.html",
    "href": "content/week-1/lecture-1A.html",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "",
    "text": "Aug. 30, 2023"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#today",
    "href": "content/week-1/lecture-1A.html#today",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Today",
    "text": "Today\n\nCourse logistics\nUsing Jupyter Notebooks and Jupyter Lab\nIntroduction to Python & Pandas"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#who-am-i",
    "href": "content/week-1/lecture-1A.html#who-am-i",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Who am I?",
    "text": "Who am I?\n\nMy day job\n\nMy name is Nick Hand\nFor the past 5+ years, I have led a small data science team in the City Controller’s Office\nWhat did we do?:\n\nObjective, data-driven analysis of financial policies impacting Philadelphia\nIncreasing transparency through data releases and interactive reports\n\nIn two weeks, I start as a data scientist working at the Consumer Finance Protection Bureau in the federal government\n\nIn my time at the Controller’s Office, we covered a range of policy issues in the city:\n\nAnalysis of the fairness and accuracy of property assessments\nInteractive reports for the City’s cash levels\nAnalysis of the 10-Year Tax Abatement program\nInteractive dashboard of Soda Tax spending\nVisualization of paving & potholes\nSeries on the impact of COVID-19 on Philadelphia’s small businesses and neighborhoods\nVisualization of the City’s most recent budget\nInteractive report on redlining in Philadelphia\nAnalysis of the impact of gun violence on housing prices\nInteractive dashboard of shooting victims in Philadelphia\nInteractive dashboard of neighborhood well-being\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo see more of our work, check out: https://controller.phila.gov/policy-analysis\n\n\n\n\nPreviously:Astrophysics Ph.D. at Berkeley\n\n\n\n\n\n\n\nHow did I get here?\n\nAstrophysics/physics to data science is becoming increasingly common\nLanded a job through Twitter: https://www.parkingjawn.com\n\nDashboard visualization of monthly parking tickets in Philadelphia\nData from OpenDataPhilly\n\n\n\nA nice example of how exploratory analysis + a well-designed dashbord can lead to insights: - The power of cross-filtering: different views of the same data across multiple dimensions - See drop in parking tickets over Jan 24-26, 2016 due to snowstorm\nParking Jawn is not Python based, but dovetails nicely with one of the main goals of the course: &gt; How can we effectively explore and extract insight from complex datasets?"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#course-logistics",
    "href": "content/week-1/lecture-1A.html#course-logistics",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Course logistics",
    "text": "Course logistics\n\nGeneral Info\n\nTwo 90-minute lectures per week — mix of lecturing, interactive demos, and in-class lab time\nMy email: nhand@design.upenn.edu\n\nOffice Hours:\n\n2-hours during the week\nOffice hours will be by appointment and remote via Zoom. You will be able to sign up for 1 (or more) 15-minute time slot via the Canvas calendar.\nTime is to be determined\n\n\nTeaching Assistant: Teresa Chang\n\nEmail: thchang@design.upenn.edu\nOffice hours: TBD\n\n\n\n\nCourse Websites\nCourse has four websites (sorry!). They are:\n\nMain Course: https://musa-550-fall-2023.github.io\nGithub: https://github.com/MUSA-550-Fall-2023\nCanvas: https://canvas.upenn.edu/courses/1740535\nEd Discussion: https://edstem.org/us/courses/42616/discussion/\n\nEach will have its own purpose:\n\nMain course website\n\nCourse schedule with links to weekly slides\nResources for learning Python, setting up software, and dealing with common issues\nGeneral course info and policies\nQuick links to the other websites for the course\n\n\n\nGithub\n\nGithub organization set up for the course\nEach week and assignment will have its own Github repository\nAssignments will also be submitted through Github\n\n\n\nCanvas\n\nWill be used sign up for remote office hours and provide Zoom links for office hours\nGrading will also be tracked here\n\n\n\nEd Discussion\n\nWill be used for question & answer forum for course materials and assignments\nAnnouncements will also be made here so make sure you check frequently or turn on your notifications!\nMain method of communication will be through announcements on this site\nParticipation grade (5% of total grade) will also be determined by user activity on the forum"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#main-course-website-1",
    "href": "content/week-1/lecture-1A.html#main-course-website-1",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Main course website",
    "text": "Main course website\n\n  \n\n\nHighlights\n\nSyllabus\nSchedule\nResources & Guides:\n\nPython resources\nInitial installation guide\n\nWeekly content\nAssignments\nQuick links to Canvas, Ed Discussion, GitHub homepage"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#course-github",
    "href": "content/week-1/lecture-1A.html#course-github",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Course Github",
    "text": "Course Github"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-goals-of-this-course",
    "href": "content/week-1/lecture-1A.html#the-goals-of-this-course",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The goals of this course",
    "text": "The goals of this course\n\nProvide students with the knowledge and tools to turn data into meaningful insights and stories\nFocus on the modern data science tools within the Python ecosystem\nThe pipeline approach to data science:\n\ngathering, storing, analyzing, and visualizing data to tell stories\n\nReal-world applications of analysis techniques in the urban planning and public policy realm"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#what-well-cover",
    "href": "content/week-1/lecture-1A.html#what-well-cover",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "What we’ll cover",
    "text": "What we’ll cover\n\nModule 1\nExploratory Data Science: Students will be introduced to the main tools needed to get started analyzing and visualizing data using Python\n\n\nModule 2\nIntroduction to Geospatial Data Science: Building on the previous set of tools, this module will teach students how to work with geospatial datasets using a range of modern Python toolkits.\n\n\nModule 3\nData Ingestion & Big Data: Students will learn how to collect new data through web scraping and APIs, as well as how to work effectively with the large datasets often encountered in real-world applications.\n\n\nModule 4\nFrom Exploration to Storytelling: With a solid foundation, students will learn the latest tools to present their analysis results using web-based formats to transform their insights into interactive stories.\n\n\nModule 5\nGeospatial Data Science in the Wild: Armed with the necessary data science tools, the final module introduces a range of advanced analytic and machine learning techniques using a number of innovative examples from modern researchers."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#assignments-and-grading",
    "href": "content/week-1/lecture-1A.html#assignments-and-grading",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Assignments and grading",
    "text": "Assignments and grading\n\nGrading:\n\n55% homework\n45% final project\n5% participation (based on class and Ed Discussion participation)\n\nWhile you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nThere’s no penalty for late assignments. I would highly recommend staying caught up on lectures and assignments as much as possible, but if you need to turn something in a few days late, there won’t be a penalty.\n\nNote: Homeworks will be assigned (roughly) every two and a half weeks."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-course-schedule",
    "href": "content/week-1/lecture-1A.html#the-course-schedule",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The course schedule",
    "text": "The course schedule\nCheck out the schedule page for the most up-to-date details on lectures, assignment due dates, etc."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#final-project",
    "href": "content/week-1/lecture-1A.html#final-project",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Final project",
    "text": "Final project\nThe final project is to replicate the pipeline approach on a dataset (or datasets) of your choosing.\nStudents will be required to use several of the analysis techniques taught in the class and produce a web-based data visualization that effectively communicates the empirical results to a non-technical audience.\nMore info will be posted here: https://github.com/MUSA-550-Fall-2023/final-project"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#any-questions-so-far",
    "href": "content/week-1/lecture-1A.html#any-questions-so-far",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Any questions so far?",
    "text": "Any questions so far?"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#initial-surveys",
    "href": "content/week-1/lecture-1A.html#initial-surveys",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Initial surveys",
    "text": "Initial surveys\nRoll call: https://bit.ly/musa550-roll-call\nSome initial feedback: https://bit.ly/musa550-initial-feedback"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-incredible-growth-of-python",
    "href": "content/week-1/lecture-1A.html#the-incredible-growth-of-python",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The Incredible Growth of Python",
    "text": "The Incredible Growth of Python\nExample #1: A 2017 analysis of StackOverflow posts"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-rise-of-the-jupyter-notebook",
    "href": "content/week-1/lecture-1A.html#the-rise-of-the-jupyter-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The rise of the Jupyter notebook",
    "text": "The rise of the Jupyter notebook\n\nThe engine of collaborative data science\n\nFirst started by a physics grad student around 2001\nKnown as the IPython notebook originally\nStarting getting popular in ~2011\nFirst funding received in 2015 — the Jupyter notebook was born\n\n\n\nGoogle searches for Jupyter notebook\n\n\n\nKey features\n\nAimed at “computational narratives” — telling stories with data\nInteractive, reproducible, shareable, user-friendly, visualization-focused\nFully open-source and managed by the community\n\nVery versatile: good for both exploratory data analysis and polished finished products\n\n\n\n\n\n\nImportant\n\n\n\nThe lecture slides in the course will all be Jupyter notebooks. The preferred interface for editing and executing them will be JupyterLab. That’s what I’m using now!\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more info on Jupyter notebooks and JupyterLab, check out the guide on the course website.\nIn particulary, I strongly encourage you to go through the official documentation for JupyterLab and Jupyter notebooks:\n\nStarting JupyterLab\nThe JupyterLab interface\nThe structure of a notebook document\nThe notebook workflow\nWorking with notebooks in JupyterLab\nWorking with files"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#beyond-the-jupyter-notebook",
    "href": "content/week-1/lecture-1A.html#beyond-the-jupyter-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Beyond the Jupyter notebook",
    "text": "Beyond the Jupyter notebook\nGoogle Colab is the most popular alternative to Jupyter notebooks.\n\n\nA fancier notebook experience built on top of Jupyter notebook\nRunning in the cloud on Google’s servers\nAn internal Google product that was released publicly\nVery popular for Python-based machine learning, since it provides low-barrier access to GPU resources which can be very helpful for training machine learning models\nWe’ll focus on the open-source Jupyter notebook as the foundation for this course\n\nSee, for example: https://colab.research.google.com/notebooks/welcome.ipynb"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#the-binder-service",
    "href": "content/week-1/lecture-1A.html#the-binder-service",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "The Binder service",
    "text": "The Binder service\nhttps://mybinder.org\n\n\nA free, open-source service, supported by donors\nAllows you to launch a repository of Jupyter notebooks on GitHub in an executable environment in the cloud\nAmazing if you want to make your code immediately reproducible by anyone, anywhere.\nNote: as a free service, it can be a bit slow sometimes"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#class-lectures",
    "href": "content/week-1/lecture-1A.html#class-lectures",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Class lectures",
    "text": "Class lectures\nWeekly lectures are available on Binder! In the README for each week’s repository on GitHub, you will see badges to launch the lecture slides on Binder.\n\n\n\n\nYou can also access these links from the content section of the course website. For example, here is:\nhttps://musa-550-fall-2023.github.io/content/week-1/\n\n\n\n\n\n\n\n\n\nSuggested weekly workflow\n\n\n\n\nSet up local Python environment as part of first homework assignment\nEach week, you will have two options to follow along with lectures:\n\nUsing Binder in the cloud, launching via the button on the week’s repository\nDownload the week’s repository to your laptop and launch the notebook locally\n\nWork on homeworks locally on your laptop — Binder is only a temporary environment (no save features)\n\n\n\nCheck out the content overview page on the main course website for more info!"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#now-to-the-fun-stuff",
    "href": "content/week-1/lecture-1A.html#now-to-the-fun-stuff",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Now to the fun stuff…",
    "text": "Now to the fun stuff…\nJupyter notebooks are a mix of code cells and text cells in Markdown. You can change the type of cell in the top menu bar.\nThis cell is a Markdown cell.\n\n# Comments begin with a \"#\" character in Python\n# A simple code cell\n# SHIFT-ENTER to execute\n\n\nx = 10\nprint(x)\n\n10\n\n\n\nPython data types\n\n# integer\na = 10\n\n# float\nb = 10.5\n\n# string\nc = \"this is a test string\"\n\n# lists\nd = list(range(0, 10))\n\n# booleans\ne = True\n\n# dictionaries\nf = {\"key1\": 1, \"key2\": 2}\n\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)\nprint(f)\n\n10\n10.5\nthis is a test string\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nTrue\n{'key1': 1, 'key2': 2}\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike R, you’ll need to use quotes more often in Python, particularly around strings and keys of dictionaries.\n\n\n\n\nAlternative method for creating a dictionary\nWe can use the dict() function, which is built in to the Python language. More on functions in a bit…\n\nf = dict(key1=1, key2=2, key3=3)\n\nf\n\n{'key1': 1, 'key2': 2, 'key3': 3}\n\n\n\n\nAccessing dictionary values\n\n# Access the value with key 'key1'\nf['key1']\n\n1\n\n\n\n\nAccessing list values\n\nd\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n# Access the second list entry (0 is the first index)\nd[1]  \n\n1\n\n\n\n\nAccessing characters of a string\n\nc\n\n'this is a test string'\n\n\n\n# the first character\nc[0]\n\n't'\n\n\n\n\nIterators and for loops\n\n\n\n\n\n\nImportant\n\n\n\nBe sure to use the right indentation in for loops!\n\n\n\n# Variable that will track the sum\nresult_sum = 0\n\n# Variable i takes on values [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor i in range(10):\n    \n    # Indented, so it runs for each iteration of the loop\n    print(i)\n    result_sum = result_sum + i\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nprint(result_sum)\n\n45\n\n\n\n\nPython’s inline syntax\n\na = range(10) # this is an iterator\n\n\nprint(a)\n\nrange(0, 10)\n\n\nUse the list() function to iterate over it and make it into a list:\n\n# convert it to a list explicitly\na = list(range(10))\n\n# Output it from the cell\na\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n# or use the INLINE syntax; this is the SAME\na = [i for i in range(10)]\n\na\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\nPython functions\ndef function_name(arg1, arg2, arg3):\n    \n    .\n    .\n    .\n    code lines (indented)\n    .\n    .\n    .\n    \n    return result\n\ndef compute_square(x):\n    return x * x\n\n\nsq = compute_square(5)\nprint(sq)\n\n25\n\n\n\n\nKeywords: arguments with a default!\n\ndef compute_product(x, y=5):\n    return x * y\n\n\n# use the default value for y\nprint(compute_product(5))\n\n25\n\n\n\n# specify a y value other than the default\nprint(compute_product(5, 10))\n\n50\n\n\n\n# can also explicitly tell Python which arguments are which\nprint(compute_product(5, y=2))\nprint(compute_product(y=2, x=5))\n\n10\n10\n\n\n\nprint(compute_product(x=5, y=4))\n\n20\n\n\n\n# argument names must match the function signature though!\nprint(compute_product(x=5, z=5))\n\nTypeError: compute_product() got an unexpected keyword argument 'z'\n\n\n\n\nGetting help in the notebook\nUse tab auto-completion and the ? and ?? operators\n\nthis_variable_has_a_long_name = 5\n\n\n# try hitting tab after typing this_ \nthis_variable_has_a_long_name\n\n5\n\n\n\n# Forget how to create a range? --&gt; use the help message\nrange?\n\n\nInit signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     \n\n\n\n\n\nPeeking at the source code for a function\nUse the ?? operator\n\n# Lets re-define compute_product() and add a docstring between \"\"\" \"\"\"\ndef compute_product(x, y=5):\n    \"\"\"\n    This computes the product of x and y\n    \n    \n    This is all part of the comment.\n    \"\"\"\n    return x * y\n\n\ncompute_product??\n\n\nSignature: compute_product(x, y=5)\nSource:   \ndef compute_product(x, y=5):\n    \"\"\"\n    This computes the product of x and y\n    \n    \n    This is all part of the comment.\n    \"\"\"\n    return x * y\nFile:      /var/folders/49/ntrr94q12xd4rq8hqdnx96gm0000gn/T/ipykernel_63388/3896936014.py\nType:      function\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe question mark operator gives you access to the help message for any variable or function.\nI use this frequently and it is a great way to understand what a function actually does.\n\n\n\n\nThe JupyterLab Debugger\nYou can enable Debugging mode in JupyterLab by clicking on the “bug” icon in the top right:\n\nThis should open the Debugger panel on the right side of JupyterLab. One of the most useful parts of this panel is the “Variables” section, which gives you the current values of all defined variables in the notebook.\n\n\n\n\n\n\n\nTip\n\n\n\nFor more information on the debugger, see the JupyterLab docs."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#getting-more-python-help",
    "href": "content/week-1/lecture-1A.html#getting-more-python-help",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Getting more Python help",
    "text": "Getting more Python help\nThis was a very brief introduction to Python and Python syntax. We’ll continue practicing and reinforcing the proper syntax throughout the next few weeks, but it can definitely be frustrating. Hang in there!\n\n\n\n\n\n\nImportant: DataCamp Tutorials\n\n\n\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great — the more the merrier!\nFor more info, including how to sign up, check out the resources section of the website.\n\n\nAdditional Python resources are listed on our course website under “Resources”\nhttps://musa-550-fall-2023.github.io/resource/python/\nIn addition to the DataCamp videos, there are links to lots of online tutorials:\n\nIntroductory level tutorials\nMore advanced tutorials\nThe r/learnpython subreddit has a great wiki of resources\nThe Berkeley Institute for Data Science has also compiled a number of Python resources"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#one-more-thing-working-outside-the-notebook",
    "href": "content/week-1/lecture-1A.html#one-more-thing-working-outside-the-notebook",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "One more thing: working outside the notebook",
    "text": "One more thing: working outside the notebook\nIn this class, we will almost exclusively work inside Jupyter notebooks — you’ll be writing Python code and doing data analysis directly in the notebook.\nThe more traditional method of using Python is to put your code into a .py file and execute it via the command line (known as the Miniforge/Anaconda Prompt on Windows or Terminal app on MacOS).\nSee this section of the Practical Python Programming tutorial for more info.\n\nThe JupyterLab text editor\nThere is a file called hello_world.py in the repository for week 1. If we execute it, it should print out “Hello, World” to the command line.\nFirst, let’s open up the .py file in the JupyterLab text editor. Double click on the “hello_world.py” item in the file browser on the left:\n\nThis will open the file and allow you to make edits. You should see the following:\n# Our first Python program\nprint(\"Hello World!\")\n\n\n\n\n\n\nTip\n\n\n\nSee the JupyterLab docs for more info on the text editor.\n\n\n\n\nUsing the JupyterLab Terminal\nTo execute the file, we can use the built-in Terminal feature in JupyterLab using the following steps:\n\nBring up the “Launcher” tab by clicking on the blue button with a plus sign in the upper left.\nClick on the “Terminal” button”\nWhen the terminal opens, type the following:\n\npython hello_world.py\nAnd you should see the following output:\nHello World!\nIt should look something like this:"
  },
  {
    "objectID": "content/week-1/lecture-1A.html#code-editors",
    "href": "content/week-1/lecture-1A.html#code-editors",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "Code editors",
    "text": "Code editors\nThe JupyterLab text editor will work in a pinch, but it’s not usually the best option when writing software outside the notebook. Other code editors will provide a nice interface for writing Python code and some even have fancy features, like real-time syntax checking and syntax highlighting.\nMy recommended option is Visual Studio Code."
  },
  {
    "objectID": "content/week-1/lecture-1A.html#see-you-next-week",
    "href": "content/week-1/lecture-1A.html#see-you-next-week",
    "title": "Welcome to MUSA 550:Geospatial Data Science in Python",
    "section": "See you next week!",
    "text": "See you next week!\n\nNo lecture on Monday next week due to Labor Day\nNext class a week from today on Wednesday 9/6\nIn the meantime:\n\nFollow [the guide] for setting up your local Python environment\nCheck out Python resources + DataCamp courses in the meantime\nHomework #1 will be posted a week from today (Wednesday 9/6, due on 9/20)"
  },
  {
    "objectID": "content/week-1/index.html",
    "href": "content/week-1/index.html",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "",
    "text": "Course Logistics\nUsing Jupyter Notebooks\nIntroduction to Pandas"
  },
  {
    "objectID": "content/week-1/index.html#topics",
    "href": "content/week-1/index.html#topics",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "",
    "text": "Course Logistics\nUsing Jupyter Notebooks\nIntroduction to Pandas"
  },
  {
    "objectID": "content/week-1/index.html#initial-survey",
    "href": "content/week-1/index.html#initial-survey",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Initial Survey",
    "text": "Initial Survey\nPlease fill out this anonymous survey for some initial feedback:\nhttps://bit.ly/musa550-initial-feedback"
  },
  {
    "objectID": "content/week-1/index.html#recommended-readings",
    "href": "content/week-1/index.html#recommended-readings",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nCourse syllabus\nGuide for installing Python and initial set up\nGuide for mamba/conda\n\nCheck out this 20-minute guide on the conda docs\n\nGuide for Jupyter notebooks and JupyterLab\n\nGood readings on the Jupyter docs include:\n\nStarting JupyterLab\nThe JupyterLab interface\nThe structure of a notebook document\nThe notebook workflow\nWorking with notebooks in JupyterLab\nWorking with files\n\n\nRecommended tutorial for students with little Python background: Practical Python Programming\n\n\nPython tutorials & DataCamp access\nA number of Python resources are listed on the course website. In particular, DataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great — the more the merrier!\nCheck out the resources page for more information, including how to sign up."
  },
  {
    "objectID": "content/week-1/index.html#additional-materials",
    "href": "content/week-1/index.html#additional-materials",
    "title": "Week 1: Exploratory Data Science in Python",
    "section": "Additional materials",
    "text": "Additional materials\n\nDocumentation\n\nGit and GitHub:\n\nSetting up git\nManaging files on GitHub\n\nConda user guide\nJupyter notebook\nGuide to Markdown in notebook cells\nBinder Documentation\npandas documentation\nmatplotlib documentation\n\n\n\nReadings/reference\n\nStackOverflow: The Incredible Growth of Python\n2015 Funding Proposal for the Jupyter Notebook\nA history of Jupyter Notebooks by its founder\nGoogle’s Colab\nTidy Data\nJoining Datasets\nZIllow Data\nIntroduction to writing your first Python program\nVisual Studio Code"
  },
  {
    "objectID": "schedule/401.html",
    "href": "schedule/401.html",
    "title": "Schedule: Section 401",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n/Users/nhand/Teaching/PennMUSA/Fall2023/musa-550-fall-2023.github.io/python/schedule.py:257: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Repo\n\n\n\nHTML Slides\n\n\n\nExecutable Slides\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\nExploratory Data Science in Python\n\n\n\n\n\n\n\n\n\n\nWednesday, August 30\n\n\n\nLecture 1A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 6\n\n\n\nLecture 1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\nData Visualization Fundamentals\n\n\n\n\n\n\n\n\n\n\nMonday, September 11\n\n\n\nLecture 2A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 13\n\n\n\nLecture 2B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\nGeospatial Data Analysis and GeoPandas\n\n\n\n\n\n\n\n\n\n\nMonday, September 18\n\n\n\nLecture 3A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 20\n\n\n\nLecture 3B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\nMore Interactive Data Viz, Working with Raster Datasets\n\n\n\n\n\n\n\n\n\n\nMonday, September 25\n\n\n\nLecture 4A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 27\n\n\n\nLecture 4B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\nOpenStreetMap, Urban Networks, and Interactive Web Maps\n\n\n\n\n\n\n\n\n\n\nMonday, October 2\n\n\n\nLecture 5A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 4\n\n\n\nLecture 5B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\nWeb Scraping\n\n\n\n\n\n\n\n\n\n\nMonday, October 9\n\n\n\nLecture 6A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 11\n\n\n\nLecture 6B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\nWorking with APIs\n\n\n\n\n\n\n\n\n\n\nMonday, October 16\n\n\n\nLecture 7A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 18\n\n\n\nLecture 7B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\nAnalyzing and Visualizing Large Datasets\n\n\n\n\n\n\n\n\n\n\nMonday, October 23\n\n\n\nLecture 8A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 25\n\n\n\nLecture 8B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\nFrom Notebooks to the Web: Part 1\n\n\n\n\n\n\n\n\n\n\nMonday, October 30\n\n\n\nLecture 9A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 1\n\n\n\nLecture 9B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\nFrom Notebooks to the Web: Part 2\n\n\n\n\n\n\n\n\n\n\nMonday, November 6\n\n\n\nLecture 10A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 8\n\n\n\nLecture 10B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\nCase Study: Predicting Home Prices in Philadelphia\n\n\n\n\n\n\n\n\n\n\nMonday, November 13\n\n\n\nLecture 11A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 15\n\n\n\nLecture 11B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\nCase Study: Predicting Rideshare Demand\n\n\n\n\n\n\n\n\n\n\nMonday, November 20\n\n\n\nLecture 12A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 assigned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, November 27\n\n\n\nLecture 12B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\nCase Study: Clustering Analysis in Python\n\n\n\n\n\n\n\n\n\n\nWednesday, November 29\n\n\n\nLecture 13A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 4\n\n\n\nLecture 13B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal project proposal due\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\nCase Study: Advanced Raster Analysis\n\n\n\n\n\n\n\n\n\n\nWednesday, December 6\n\n\n\nLecture 14A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 11\n\n\n\nLecture 14B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, December 20\n\n\n\nFinal project due"
  },
  {
    "objectID": "schedule/402.html",
    "href": "schedule/402.html",
    "title": "Schedule: Section 402",
    "section": "",
    "text": "Note\n\n\n\nThe schedule is tentative and could change in the future.\n\n\n\n\n/Users/nhand/Teaching/PennMUSA/Fall2023/musa-550-fall-2023.github.io/python/schedule.py:257: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Repo\n\n\n\nHTML Slides\n\n\n\nExecutable Slides\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\nExploratory Data Science in Python\n\n\n\n\n\n\n\n\n\n\nThursday, August 31\n\n\n\nLecture 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 6\n\n\n\nHW #1 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\nData Visualization Fundamentals\n\n\n\n\n\n\n\n\n\n\nThursday, September 7\n\n\n\nLecture 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\nGeospatial Data Analysis and GeoPandas\n\n\n\n\n\n\n\n\n\n\nThursday, September 14\n\n\n\nLecture 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 20\n\n\n\nHW #1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #2 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\nMore Interactive Data Viz, Working with Raster Datasets\n\n\n\n\n\n\n\n\n\n\nThursday, September 21\n\n\n\nLecture 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\nOpenStreetMap, Urban Networks, and Interactive Web Maps\n\n\n\n\n\n\n\n\n\n\nThursday, September 28\n\n\n\nLecture 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 4\n\n\n\nHW #2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #3 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\nWeb Scraping\n\n\n\n\n\n\n\n\n\n\nThursday, October 5\n\n\n\nLecture 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, October 18\n\n\n\nHW #3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #4 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\nWorking with APIs\n\n\n\n\n\n\n\n\n\n\nThursday, October 19\n\n\n\nLecture 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\nAnalyzing and Visualizing Large Datasets\n\n\n\n\n\n\n\n\n\n\nThursday, October 26\n\n\n\nLecture 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, November 1\n\n\n\nHW #4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #5 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\nFrom Notebooks to the Web: Part 1\n\n\n\n\n\n\n\n\n\n\nThursday, November 2\n\n\n\nLecture 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\nFrom Notebooks to the Web: Part 2\n\n\n\n\n\n\n\n\n\n\nThursday, November 9\n\n\n\nLecture 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\nCase Study: Predicting Home Prices in Philadelphia\n\n\n\n\n\n\n\n\n\n\nThursday, November 16\n\n\n\nLecture 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, November 20\n\n\n\nHW #5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW #6 assigned\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\nCase Study: Predicting Rideshare Demand\n\n\n\n\n\n\n\n\n\n\nTuesday, November 21\n\n\n\nLecture 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\nCase Study: Clustering Analysis in Python\n\n\n\n\n\n\n\n\n\n\nThursday, November 30\n\n\n\nLecture 13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday, December 4\n\n\n\nHW #6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal project proposal due\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\nCase Study: Advanced Raster Analysis\n\n\n\n\n\n\n\n\n\n\nThursday, December 7\n\n\n\nLecture 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday, December 20\n\n\n\nFinal project due"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Weekly Course Content",
    "section": "",
    "text": "As detailed on the schedule page, the course will be broken up into 14 weeks of content, and each week will have two lectures (cleverly labeled “A” and “B”) that cover that week’s unique topic. Each week will have a recommended set of readings that will help reinforce the content. As we progress through the semester, you will be able to access the weekly content on the sidebar of this page. For each week, you’ll find information about readings and topics, as well as links to the lecture slides."
  },
  {
    "objectID": "content/index.html#lecture-slides",
    "href": "content/index.html#lecture-slides",
    "title": "Weekly Course Content",
    "section": "Lecture slides",
    "text": "Lecture slides\nThe lecture slides are Jupyter notebook files, a mix of executable Python cells, text, and images. Students have a few different options to access the slide materials:\n\nThe Jupyter notebook files for each week are stored in a repository that is available on the course’s GitHub homepage. On the content page for each week, you will see a link to this repository. Once you navigate to the repository on GitHub, you can download the contents of the repository to your computer and work locally with the notebook files by launching JupyterLab on your laptop. To download the repository contents, look for the green “Code” button and select “Download ZIP”:\n\n\n\n\n\n\n\nYou can also open the lecture slides in an interactive, executable environment using the free Binder service. This service is hosted in the cloud, meaning that the code is not being executed locally on your laptop but instead on Binder’s servers. This can be very convenient, since it doesn’t require you to have the files downloaded to your laptop or your Python environment to be set up yet. On the content page for each week, look for the Binder logo  for links to launch the lectures on Binder.\nOn the content page for each week, there are also links to HTML versions of the lecture slides. Look for the  icon for this version of the slides. Since they are displayed in the browser, they are NOT executable and cannot be changed. This version of the slides is a useful reference tool to look up how we did something in class. During class periods, we recommend that you use options #1 or #2 to view the slides, so that you can follow along and execute slides and participate in any in-class exercises or labs."
  },
  {
    "objectID": "content/week-1/lecture-1B.html",
    "href": "content/week-1/lecture-1B.html",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "",
    "text": "9/6/2023"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#today",
    "href": "content/week-1/lecture-1B.html#today",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Today",
    "text": "Today\n\nIntroduction to Pandas\nKey data analysis concepts\nExample: Census + Zillow data"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#reminder-the-weekly-workflow",
    "href": "content/week-1/lecture-1B.html#reminder-the-weekly-workflow",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Reminder: The weekly workflow",
    "text": "Reminder: The weekly workflow\n\nYou’ll set up your local Python environment as part of the first homework assignment\nEach week, you will have two options to follow along with lectures:\n\nUsing Binder in the cloud, launching via the button on the week’s repository\nDownload the week’s repository to your laptop and launch the notebook locally\n\nWork on homeworks locally on your laptop — Binder is only a temporary environment (no save features)\n\nTo follow along today, go to https://github.com/MUSA-550-Fall-2023/week-1\n\n\n\n\n\n\n\nReminder: Free DataCamp Courses\n\n\n\nDataCamp is providing 6 months of complimentary access to its courses for students in MUSA 550. Whether you have experience with Python or not, this is a great opportunity to learn the basics of Python and practice your skills.\nIt is strongly recommended that you watch some or all of the introductory videos below to build a stronger Python foundation for the semester. The more advanced, intermediate courses are also great — the more the merrier!\nFor more info, including how to sign up, check out the resources section of the website."
  },
  {
    "objectID": "content/week-1/lecture-1B.html#python-data-analysis-the-pandas-package",
    "href": "content/week-1/lecture-1B.html#python-data-analysis-the-pandas-package",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Python Data Analysis: the pandas package",
    "text": "Python Data Analysis: the pandas package\nDocumentation is available at https://pandas.pydata.org\n\nThe old logo\n\n\n\n\n\nThe new, less fun logo\n\n\n\nThe following line imports the pandas package:\n\nimport pandas as pd"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#basic-pandas-concepts",
    "href": "content/week-1/lecture-1B.html#basic-pandas-concepts",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "Basic pandas concepts",
    "text": "Basic pandas concepts\nThe primary objects in pandas are the:\n\nDataFrame, which is very similar to an Excel spreadsheet, and has rows and named columns\nSeries, which represents a single column of data. A DataFrame contains one or more Series and a name for each Series.\n\nThe data frame is a commonly used abstraction for data manipulation. Similar implementations exist in R.\nYou can think Series objects as fancier versions of Python’s built-in list data type.\nTo create a Series object, pass a list of values to the pd.Series() function:\n\n## Philadelphia metro counties and populations\n\ncounty_names = pd.Series(\n    [\n        \"Bucks\",\n        \"Chester\",\n        \"Delaware\",\n        \"Montgomery\",\n        \"Philadelphia\",\n        \"Burlington\",\n        \"Camden\",\n        \"Gloucester\",\n    ]\n)\n\npopulation = pd.Series([\n    645054.0,\n    545823.0,\n    575182.0,\n    864683.0,\n    1567258.0,\n    466103.0,\n    524907.0,\n    306601.0,\n])\n\nDataFrame objects can be created by passing a dict mapping string column names to their respective Series.\n\ncounty_df = pd.DataFrame({\"County Name\": county_names, \"Population\": population})\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\n\n\n\n\n0\nBucks\n645054.0\n\n\n1\nChester\n545823.0\n\n\n2\nDelaware\n575182.0\n\n\n3\nMontgomery\n864683.0\n\n\n4\nPhiladelphia\n1567258.0\n\n\n5\nBurlington\n466103.0\n\n\n6\nCamden\n524907.0\n\n\n7\nGloucester\n306601.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlways try to give your variables meaningful names. It will help immensely when you are trying to debug problems or when you’re trying to understand the code you’ve written months later.\n\n\n\n1. Accessing data\n\n# Access columns with a dict-like syntax\ncounty_df[\"Population\"]\n\n0     645054.0\n1     545823.0\n2     575182.0\n3     864683.0\n4    1567258.0\n5     466103.0\n6     524907.0\n7     306601.0\nName: Population, dtype: float64\n\n\nRows can be accessed using Python’s syntax for slicing:\n\n## Example: slicing a list slicing\n\n# Get the elements with indices 1 and 2 (but NOT 3)\ncounty_list = [\"Bucks\", \"Philadelphia\", \"Delaware\"]\n\ncounty_list[0:2]  # Same as county_list[:2] (omitting the zero)\n\n['Bucks', 'Philadelphia']\n\n\nUnfortunately the functionality for slicing lists is not that powerful…but pandas will have many more features!\n\n# Slicing a DataFrame is similar...but now we will get rows of the DataFrame back!\n\ncounty_df[0:2]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\n\n\n\n\n0\nBucks\n645054.0\n\n\n1\nChester\n545823.0\n\n\n\n\n\n\n\npandas includes functionality for many different ways of selecting data. See the documentation for many more examples.\n\n\n2. Manipulating data\n\ncounty_df[\"Population\"].median()\n\n560502.5\n\n\n\ncounty_df[\"Population\"].mean()\n\n686951.375\n\n\nNumPy (Numerical Python) is a popular toolkit for scientific computing. Among other things, it can calculate mathematical functions, like mean, median, min, max, etc.\npandas Series can be used as arguments to most NumPy functions:\n\nimport numpy as np\n\n\n# Calculate the median population value\nnp.median(county_df[\"Population\"])\n\n560502.5\n\n\n\n# Calculate the median population value\nnp.mean(county_df[\"Population\"])\n\n686951.375\n\n\n\n\n\n\n\n\nNote: DataCamp + NumPy\n\n\n\nWe won’t go through the specifics of NumPy in detail in this course. You will see it pop up throughout the course, particularly when we start talking about two-dimensional raster image data. NumPy specializes in multi-dimensional arrays, the same format of multi-band raster data.\nIn the meantime, if you want to know more about NumPy or just build your Python knowledge, check out the DataCamp course on NumPy. To get free access to DataCamp, see the instructions on the course website.\n\n\n\n\n3. Applying a function\nFor more complex single-column transformations, you can use Series.apply. It accepts a function that is applied to each value in the Series.\nAn apply operation allows you to call an arbitrary function to each value in a column. It is a core pandas concept and can be very powerful for complex calculations.\nAs an example, we can find which counties have a population greater than one million:\n\n# Define our function\ndef get_large_counties(population):\n    return population &gt; 1e6\n\n\nlarge_county_sel = county_df[\"Population\"].apply(get_large_counties)\n\n\n# This is a Series where the value is the returned value from the above function\nlarge_county_sel\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Population, dtype: bool\n\n\n\n# Add the new computed column to our original data frame\ncounty_df[\"Large Counties\"] = large_county_sel\n\n\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\n\n\n1\nChester\n545823.0\nFalse\n\n\n2\nDelaware\n575182.0\nFalse\n\n\n3\nMontgomery\n864683.0\nFalse\n\n\n4\nPhiladelphia\n1567258.0\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\n\n\n6\nCamden\n524907.0\nFalse\n\n\n7\nGloucester\n306601.0\nFalse\n\n\n\n\n\n\n\nWe could have also used a Python lambda function for our function. These are inline functions that start with a special Python keyword lambda, followed by the function argument, a colon, and then the contents of the function.\n\n\n\n\n\n\nTip\n\n\n\nSee this DataCamp tutorial for more info on lambda functions.\n\n\nAs an example, let’s add a small counties column:\n\nsmall_counties_func = lambda population: population &lt; 1e6\n\n# we can also use lambda (unnamed, inline) functions\ncounty_df[\"Small Counties\"] = county_df[\"Population\"].apply(small_counties_func)\n\n# Print out\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n4. Data selection\nWe can select the “large” counties by passing the boolean values to the .loc[] function of a DataFrame:\n\ncounty_df[\"Large Counties\"]\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Large Counties, dtype: bool\n\n\n\ncounty_df.loc[ county_df[\"Large Counties\"] ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBe sure to use square brackets when calling .loc.\n.loc[] ✓\n.loc() ✘\n\n\nBut: there is a faster, more concise, to achieve this!\n\ncounty_df[\"Population\"] &gt; 1e6\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: Population, dtype: bool\n\n\n\n# We can pass the selection directory the .loc[] —&gt; no need to define a new variable\n\ncounty_df.loc[county_df[\"Population\"] &gt; 1e6]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n\n\n\n\n\nQuestion: What about getting rows where “Large Counties” is False?\nAnswer: Use the Python tilde operator to do a logicial not operation:\n\n# Reverse the large counties boolean selection\ncounty_df.loc[ ~large_county_sel ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n# or equivalently:\n# NOTE: you need to put the whole expression in () and then apply the tilde!\n\ncounty_df.loc[ ~(county_df[\"Population\"] &gt; 1e6) ]\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nAh! An even faster way!\nWe can use the pandas query function.\nThe query() function will return a subset of your dataframe based on a string version of the boolean expression.\n\ncounty_df.query(\"Population &lt; 1e6\")\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nWhat else can .loc[] do? So much!\nOne of the more common uses is use the DataFrame’s index labels to select particular rows. What is the index?\nEach row has a label that idenitifes it. By default the label is an integer ranging from 0 (first row) to 1 less than the number of rows (last row). The label is displayed in bold before each row when you look at a DataFrame:\n\ncounty_df\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBucks\n645054.0\nFalse\nTrue\n\n\n1\nChester\n545823.0\nFalse\nTrue\n\n\n2\nDelaware\n575182.0\nFalse\nTrue\n\n\n3\nMontgomery\n864683.0\nFalse\nTrue\n\n\n4\nPhiladelphia\n1567258.0\nTrue\nFalse\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\nWe can pass a label to the .loc[] function to select a particular row. For example, to get the Philadelphia row, we could use the “4” label:\n\ncounty_df.loc[4]\n\nCounty Name       Philadelphia\nPopulation           1567258.0\nLarge Counties            True\nSmall Counties           False\nName: 4, dtype: object\n\n\nOr maybe we want the suburb counties in NJ. In this case, we can pass a list of multiple labels (5, 6, 7)”\n\nnj_suburbs = county_df.loc[ [5, 6, 7] ]\n\nnj_suburbs\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n5\nBurlington\n466103.0\nFalse\nTrue\n\n\n6\nCamden\n524907.0\nFalse\nTrue\n\n\n7\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n::: {.callout-important}\nNote that this is different than the .iloc[] function, which can also be used to select rows from a DataFrame. However, it uses the integer value of the row, regardless of what the labels in the index are. For example, the first row of a dataframe can always be accessed using:\ndf.iloc[0]\n For more details on the iloc[] function, see the documentation on indexing by position.\nIn the case of the nj_suburbs dataframe, we can get the first row, which has a label of “5”, using:\n\nnj_suburbs.iloc[0]\n\nCounty Name       Burlington\nPopulation          466103.0\nLarge Counties         False\nSmall Counties          True\nName: 5, dtype: object\n\n\nWe can also reset the index labels so they range from 0 to the length of the dataframe, using the reset_index() function. For example\n\nnj_suburbs.reset_index(drop=True)\n\n\n\n\n\n\n\n\nCounty Name\nPopulation\nLarge Counties\nSmall Counties\n\n\n\n\n0\nBurlington\n466103.0\nFalse\nTrue\n\n\n1\nCamden\n524907.0\nFalse\nTrue\n\n\n2\nGloucester\n306601.0\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nNeed more help?\nHow to remember the specifics of all of these functions?\nThe documentation is your best friend! The Pandas user guide\nYou can also use the question mark operator in the notebook!\n\n# Use the question mark\npd.DataFrame.loc?\n\n\nType:        property\nString form: &lt;property object at 0x12f6f1d50&gt;\nDocstring:  \nAccess a group of rows and columns by label(s) or a boolean array.\n``.loc[]`` is primarily label based, but may also be used with a\nboolean array.\nAllowed inputs are:\n- A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n  interpreted as a *label* of the index, and **never** as an\n  integer position along the index).\n- A list or array of labels, e.g. ``['a', 'b', 'c']``.\n- A slice object with labels, e.g. ``'a':'f'``.\n  .. warning:: Note that contrary to usual python slices, **both** the\n      start and the stop are included\n- A boolean array of the same length as the axis being sliced,\n  e.g. ``[True, False, True]``.\n- An alignable boolean Series. The index of the key will be aligned before\n  masking.\n- An alignable Index. The Index of the returned selection will be the input.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above)\nSee more at :ref:`Selection by Label &lt;indexing.label&gt;`.\nRaises\n------\nKeyError\n    If any items are not found.\nIndexingError\n    If an indexed key is passed and its index is unalignable to the frame index.\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column label pair.\nDataFrame.iloc : Access group of rows and columns by integer position(s).\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n    Series/DataFrame.\nSeries.loc : Access group of values using labels.\nExamples\n--------\n**Getting values**\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\nSingle label. Note this returns the row as a Series.\n&gt;&gt;&gt; df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\nList of labels. Note using ``[[]]`` returns a DataFrame.\n&gt;&gt;&gt; df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\nSingle label for row and column\n&gt;&gt;&gt; df.loc['cobra', 'shield']\n2\nSlice with labels for row and single label for column. As mentioned\nabove, note that both the start and stop of the slice are included.\n&gt;&gt;&gt; df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\nBoolean list with the same length as the row axis\n&gt;&gt;&gt; df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\nAlignable boolean Series:\n&gt;&gt;&gt; df.loc[pd.Series([False, True, False],\n...        index=['viper', 'sidewinder', 'cobra'])]\n            max_speed  shield\nsidewinder          7       8\nIndex (same behavior as ``df.reindex``)\n&gt;&gt;&gt; df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\nConditional that returns a boolean Series\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 6]\n            max_speed  shield\nsidewinder          7       8\nConditional that returns a boolean Series with column labels specified\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 6, ['max_speed']]\n            max_speed\nsidewinder          7\nCallable that returns a boolean Series\n&gt;&gt;&gt; df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n**Setting values**\nSet value for all items matching the list of labels\n&gt;&gt;&gt; df.loc[['viper', 'sidewinder'], ['shield']] = 50\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\nSet value for an entire row\n&gt;&gt;&gt; df.loc['cobra'] = 10\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\nSet value for an entire column\n&gt;&gt;&gt; df.loc[:, 'max_speed'] = 30\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\nSet value for rows matching callable condition\n&gt;&gt;&gt; df.loc[df['shield'] &gt; 35] = 0\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n**Getting values on a DataFrame with an index that has integer labels**\nAnother example using integers for the index\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n&gt;&gt;&gt; df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\nSlice with integer labels for rows. As mentioned above, note that both\nthe start and stop of the slice are included.\n&gt;&gt;&gt; df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n**Getting values with a MultiIndex**\nA number of examples using a DataFrame with a MultiIndex\n&gt;&gt;&gt; tuples = [\n...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...    ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n&gt;&gt;&gt; index = pd.MultiIndex.from_tuples(tuples)\n&gt;&gt;&gt; values = [[12, 2], [0, 4], [10, 20],\n...         [1, 4], [7, 1], [16, 36]]\n&gt;&gt;&gt; df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n&gt;&gt;&gt; df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\nSingle label. Note this returns a DataFrame with a single index.\n&gt;&gt;&gt; df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\nSingle index tuple. Note this returns a Series.\n&gt;&gt;&gt; df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\nSingle label for row and column. Similar to passing in a tuple, this\nreturns a Series.\n&gt;&gt;&gt; df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\n&gt;&gt;&gt; df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\nSingle tuple for the index with a single label for the column\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'), 'shield']\n2\nSlice from index tuple to single label\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\nSlice from index tuple to index tuple\n&gt;&gt;&gt; df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1\nPlease see the :ref:`user guide&lt;advanced.advanced_hierarchical&gt;`\nfor more details and explanations of advanced indexing.\n\n\n\n\n# Use the question mark\npd.DataFrame.iloc?\n\n\nType:        property\nString form: &lt;property object at 0x12f6f1080&gt;\nDocstring:  \nPurely integer-location based indexing for selection by position.\n``.iloc[]`` is primarily integer position based (from ``0`` to\n``length-1`` of the axis), but may also be used with a boolean\narray.\nAllowed inputs are:\n- An integer, e.g. ``5``.\n- A list or array of integers, e.g. ``[4, 3, 0]``.\n- A slice object with ints, e.g. ``1:7``.\n- A boolean array.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above).\n  This is useful in method chains, when you don't have a reference to the\n  calling object, but would like to base your selection on some value.\n- A tuple of row and column indexes. The tuple elements consist of one of the\n  above inputs, e.g. ``(0, 1)``.\n``.iloc`` will raise ``IndexError`` if a requested indexer is\nout-of-bounds, except *slice* indexers which allow out-of-bounds\nindexing (this conforms with python/numpy *slice* semantics).\nSee more at :ref:`Selection by Position &lt;indexing.integer&gt;`.\nSee Also\n--------\nDataFrame.iat : Fast integer location scalar accessor.\nDataFrame.loc : Purely label-location based indexer for selection by label.\nSeries.iloc : Purely integer-location based indexing for\n               selection by position.\nExamples\n--------\n&gt;&gt;&gt; mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n&gt;&gt;&gt; df = pd.DataFrame(mydict)\n&gt;&gt;&gt; df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n**Indexing just the rows**\nWith a scalar integer.\n&gt;&gt;&gt; type(df.iloc[0])\n&lt;class 'pandas.core.series.Series'&gt;\n&gt;&gt;&gt; df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64\nWith a list of integers.\n&gt;&gt;&gt; df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n&gt;&gt;&gt; type(df.iloc[[0]])\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&gt;&gt;&gt; df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400\nWith a `slice` object.\n&gt;&gt;&gt; df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\nWith a boolean mask the same length as the index.\n&gt;&gt;&gt; df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\nWith a callable, useful in method chains. The `x` passed\nto the ``lambda`` is the DataFrame being sliced. This selects\nthe rows whose index label even.\n&gt;&gt;&gt; df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n**Indexing both axes**\nYou can mix the indexer types for the index and columns. Use ``:`` to\nselect the entire axis.\nWith scalar integers.\n&gt;&gt;&gt; df.iloc[0, 1]\n2\nWith lists of integers.\n&gt;&gt;&gt; df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000\nWith `slice` objects.\n&gt;&gt;&gt; df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000\nWith a boolean array whose length matches the columns.\n&gt;&gt;&gt; df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000\nWith a callable function that expects the Series or DataFrame.\n&gt;&gt;&gt; df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#a-more-interesting-example-the-donut-effect",
    "href": "content/week-1/lecture-1B.html#a-more-interesting-example-the-donut-effect",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "A more interesting example: The Donut Effect",
    "text": "A more interesting example: The Donut Effect\nThe pandemic and growth in work from home led to a phenomenon known as the “Donut Effect”. With more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\n\n1. Population change\nI’ve stored more data on population for Philadelphia and its suburbs in a comma-separated value (CSV) file in the data folder: data/phila-metro-counties-population.csv.\nThis data comes from the Census Population Estimates series for counties in the Philadelphia metropolitan region, which includes Philadelphia County, four suburb counties in PA, and 3 suburb counties in NJ. Later in the course, you’ll learn how to download data from the Census directly using Python, but for now, I’ve downloaded and formatted the data already.\nLet’s use this data to see if the donut effect occurred in Philadelphia and its suburbs.\nGoal: Calculate the percent population change from 2020 to 2022 in Philadelphia and the total percent change in the suburb counties and compare!\nFirst, use the pd.read_csv() function from pandas. The first argument to pd.read_csv() is the file path to load.\n\n# Load the data from the CSV file\npop_df = pd.read_csv(\"./data/phila-metro-counties-population.csv\")\n\n\n# How many rows are in the dataframe? --&gt; use the len() operator\nlen(pop_df)\n\n8\n\n\nPeak at the first 5 rows:\n\npop_df.head(n=5)\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n0\nBucks\nPA\n646112.0\n645054.0\n\n\n1\nChester\nPA\n534783.0\n545823.0\n\n\n2\nDelaware\nPA\n576323.0\n575182.0\n\n\n3\nMontgomery\nPA\n856938.0\n864683.0\n\n\n4\nPhiladelphia\nPA\n1600600.0\n1567258.0\n\n\n\n\n\n\n\n\nExercise: Calculate the percent change in population for Philadelphia\nLet’s select the data for Philadelphia to get its population change.\nSteps:\n\nSelect the data for Philadelphia\nSelect the 2020 and 2022 population columns (see the tip below about the squeeze() function)\nCalculate the percent change\n\n\n\n\n\n\n\nTip: The squeeze() function\n\n\n\nIf we select the data for Philadelphia, it will be DataFrame with only a single row. So if we access a column, it will return a Series of length one, rather than just a number.\nWe can use the .squeeze() function to get rid of this extra dimension of length one. It does just one it sounds like: if you have a DataFrame with only one row, it will “squeeze” the row dimension by removing it, returning just a Series object:\n\n\nThe selector for Philadelphia:\n\npop_df['county_name'] == 'Philadelphia'\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\nName: county_name, dtype: bool\n\n\nNow, let’s use the .loc[] function to select the data for Philadelphia\n\nphilly_pop = pop_df.loc[ pop_df['county_name'] == 'Philadelphia' ]\n\nphilly_pop\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n4\nPhiladelphia\nPA\n1600600.0\n1567258.0\n\n\n\n\n\n\n\n\nphilly_pop = pop_df.loc[pop_df[\"county_name\"] == \"Philadelphia\"].squeeze()\n\nphilly_pop\n\ncounty_name    Philadelphia\nstate_name               PA\npop_2020          1600600.0\npop_2022          1567258.0\nName: 4, dtype: object\n\n\nNow we can calculate the percent change:\n\nphilly_pop_change = 100 * (philly_pop['pop_2022'] / philly_pop['pop_2020'] - 1)\n\nphilly_pop_change\n\n-2.0830938398100685\n\n\nA little more than 2% decline in population from 2020 to 2022!\nA note about string formatting in Python\nWe can format this into a string using an “f-string”, strings that have an “f” before them.\nLook for curly braces in these strings. The syntax is: {variable_name : format string}. For example:\n\nprint(f\"Philadelphia's percent change in population from 2020 to 2022: {philly_pop_change:.1f}%\")\n\nPhiladelphia's percent change in population from 2020 to 2022: -2.1%\n\n\nIn this case, we told Python to format the philly_pop_change as a floating point number with one decimal using .1f.\n\n\n\n\n\n\nTip\n\n\n\nFor more practice with f-strings, see this DataCamp tutorial.\n\n\nNext up: let’s calculate the total population change across all the suburb counties. First select every county except for Philadelphia, using the != (not equals) operator.\n\n# Now do the selection!\nsuburb_pop = pop_df.loc[ pop_df[\"county_name\"] != 'Philadelphia' ]  # select the valid rows\n\nsuburb_pop\n\n\n\n\n\n\n\n\ncounty_name\nstate_name\npop_2020\npop_2022\n\n\n\n\n0\nBucks\nPA\n646112.0\n645054.0\n\n\n1\nChester\nPA\n534783.0\n545823.0\n\n\n2\nDelaware\nPA\n576323.0\n575182.0\n\n\n3\nMontgomery\nPA\n856938.0\n864683.0\n\n\n5\nBurlington\nNJ\n461648.0\n466103.0\n\n\n6\nCamden\nNJ\n523074.0\n524907.0\n\n\n7\nGloucester\nNJ\n302554.0\n306601.0\n\n\n\n\n\n\n\nTo calculate the total percent change, we can sum up the population in 2020 and 2022 and calculate the percent change.\nIn this case, we can use the built-in .sum() function for a column Series to sum up the values in a column:\n\nsuburb_pop_change = 100 * (suburb_pop[\"pop_2022\"].sum() / suburb_pop[\"pop_2020\"].sum() - 1)\n\nsuburb_pop_change\n\n0.6900286869026662\n\n\n\nprint(f\"Percent change in population from 2020 to 2022 in the Philadelphia suburbs: {suburb_pop_change:.1f}%\")\n\nPercent change in population from 2020 to 2022 in the Philadelphia suburbs: 0.7%\n\n\nTakeaway\nA slight 0.7% increase in population change in the suburb counties from 2020 to 2022, compared to a 2.1% decline in Philadelphia.\nThis tracks with the idea of a “donut effect” in Philadelphia during the pandemic!\n\n\n\n2. Home & rental prices\nLet’s investigate changes in home and rental prices in Philadelphia and its suburbs during the pandemic. If there was a strong “donut effect”, we would expect to see higher price appreciation in the suburbs compared to Philadelphia itself.\nFor this part, we will use Zillow data, specifically data by county for the Zillow Observed Rent Index (ZORI) and Zillow Home Value Index (ZHVI).\nI’ve already downloaded the data and stored it in the data/ folder. Let’s read it into pandas using the pd.read_csv() function.\n\nzhvi = pd.read_csv(\"data/County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\nzori = pd.read_csv(\"data/County_zori_sm_month.csv\")\n\nPeek at the first few rows of the rent data:\n\nzori.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nMetro\nStateCodeFIPS\nMunicipalCodeFIPS\n2015-01-31\n...\n2022-10-31\n2022-11-30\n2022-12-31\n2023-01-31\n2023-02-28\n2023-03-31\n2023-04-30\n2023-05-31\n2023-06-30\n2023-07-31\n\n\n\n\n0\n3101\n0\nLos Angeles County\ncounty\nCA\nCA\nLos Angeles-Long Beach-Anaheim, CA\n6\n37\n1859.636263\n...\n2884.685434\n2878.934549\n2875.026052\n2871.083085\n2877.814065\n2884.805738\n2898.603156\n2902.637331\n2911.922927\n2918.754134\n\n\n1\n139\n1\nCook County\ncounty\nIL\nIL\nChicago-Naperville-Elgin, IL-IN-WI\n17\n31\n1509.519799\n...\n1953.033934\n1948.209459\n1951.135298\n1957.004902\n1971.312435\n1985.335169\n2003.414964\n2029.555166\n2052.142638\n2065.170019\n\n\n2\n1090\n2\nHarris County\ncounty\nTX\nTX\nHouston-The Woodlands-Sugar Land, TX\n48\n201\n1244.972009\n...\n1605.890305\n1604.357134\n1603.265557\n1612.486921\n1618.611045\n1628.098030\n1630.157224\n1638.558882\n1648.540280\n1655.318245\n\n\n3\n2402\n3\nMaricopa County\ncounty\nAZ\nAZ\nPhoenix-Mesa-Chandler, AZ\n4\n13\n1001.214534\n...\n1937.026071\n1927.513924\n1918.344571\n1913.116142\n1917.415652\n1930.233484\n1939.647281\n1945.676380\n1945.808056\n1948.880852\n\n\n4\n2841\n4\nSan Diego County\ncounty\nCA\nCA\nSan Diego-Chula Vista-Carlsbad, CA\n6\n73\n1797.996849\n...\n3094.460551\n3074.724631\n3061.535358\n3052.480138\n3057.236678\n3079.509599\n3115.522418\n3154.972888\n3185.039118\n3205.105049\n\n\n\n\n5 rows × 112 columns\n\n\n\nAnd do the same for the home price data:\n\nzhvi.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nMetro\nStateCodeFIPS\nMunicipalCodeFIPS\n2000-01-31\n...\n2022-10-31\n2022-11-30\n2022-12-31\n2023-01-31\n2023-02-28\n2023-03-31\n2023-04-30\n2023-05-31\n2023-06-30\n2023-07-31\n\n\n\n\n0\n3101\n0\nLos Angeles County\ncounty\nCA\nCA\nLos Angeles-Long Beach-Anaheim, CA\n6\n37\n205982.244116\n...\n841135.925058\n838631.538883\n835366.307179\n828060.815041\n818172.457895\n809992.297565\n806193.000835\n808305.144458\n815021.466146\n826958.461096\n\n\n1\n139\n1\nCook County\ncounty\nIL\nIL\nChicago-Naperville-Elgin, IL-IN-WI\n17\n31\n136354.916313\n...\n284950.857552\n284143.669431\n283375.276172\n284008.891628\n284769.843988\n286109.760510\n286873.595748\n287996.202478\n290010.594940\n292281.611623\n\n\n2\n1090\n2\nHarris County\ncounty\nTX\nTX\nHouston-The Woodlands-Sugar Land, TX\n48\n201\n107050.239739\n...\n283073.624205\n282597.185986\n281442.805088\n279727.962356\n278170.043854\n277582.191393\n277509.897059\n277852.705574\n278500.865874\n279400.674485\n\n\n3\n2402\n3\nMaricopa County\ncounty\nAZ\nAZ\nPhoenix-Mesa-Chandler, AZ\n4\n13\n146824.674809\n...\n478373.220270\n472003.290822\n465224.903046\n459750.984210\n455757.981512\n454203.508557\n453192.923509\n453262.846630\n454625.973499\n457065.330187\n\n\n4\n2841\n4\nSan Diego County\ncounty\nCA\nCA\nSan Diego-Chula Vista-Carlsbad, CA\n6\n73\n217214.415906\n...\n881918.327354\n873416.861727\n866066.237738\n858962.865592\n854294.612077\n853873.557956\n856932.860092\n862757.750601\n871180.725221\n881746.559143\n\n\n\n\n5 rows × 292 columns\n\n\n\n\nConvert from “wide” to “tidy” format\nCurrently, our data is in wide format: each observation has its own column. This usually results in many columns but few rows.\nUsually it’s better to have data in tidy (also known as long) format. Tidy datasets are arranged such that each variable is a column and each observation is a row.\nIn our case, we want to have a column called ZORI and one called ZHVI and a row for each month that the indices were measured.\npandas provides the melt() function for converting from wide formats to tidy formats. melt() doesn’t aggregate or summarize the data. It transforms it into a different shape, but it contains the exact same information as before.\nImagine you have 6 rows of data (each row is a unique county) with 10 columns of home values (each column is a different month). That is wide data and is the format usually seen in spreadsheets or tables in a report.\nIf you melt() that wide data, you would get a table with 60 rows and 3 columns. Each row would contain the county name, the month, and the home value that county and month. This tidy-formatted data contains the same info as the wide data, but in a different form.\nThis animation shows the transformation from wide to long / long to wide. You can ignore gather() and spread() - those are the R versions of the pandas functions.\n\nThe syntax for pd.melt():\n\n“id_vars”: A list of column names that are the identifier variables, the fields that uniquely identify each observation.\n“value_vars”: A list of column names that get “unpivoted” and become the values in the new dataframe.\n“value_name”: The name to use for the new value column created; by default, this is “value”\n“var_name”: The name of the column that holds the unpivoted column names (the “key” above); by default, this is “variable”\n\nNow let’s look at our dataframes.\nFor “id_vars”, we want to use ‘RegionName’ (county name) and ‘StateName’. That uniquely identifies each row.\nBut what about “value_vars”?\nThe columns holding the values we want to unpivot are the monthly date strings. So…we need to figure out how to select all of the column names that look like dates.\n\nzhvi.columns\n\nIndex(['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName',\n       'State', 'Metro', 'StateCodeFIPS', 'MunicipalCodeFIPS', '2000-01-31',\n       ...\n       '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31', '2023-02-28',\n       '2023-03-31', '2023-04-30', '2023-05-31', '2023-06-30', '2023-07-31'],\n      dtype='object', length=292)\n\n\nThe filter() function to the rescue!\nThis function will take a function that returns True/False and list of values and return a new list with values that only return True in the function.\n\n\n\n\n\n\nTip\n\n\n\nSee this tutorial for more info on the built-in filter() function.\n\n\nIn our case, let’s write a function that tests if a string column name starts with ‘20’ — this should give us all of the column names that look like dates.\n\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\nlooks_like_a_date('2000-01-31')\n\nTrue\n\n\n\n## Now do the filter:\n## First argument is the function\n## Second argument is the list of values we want to filter\n\nfilter(looks_like_a_date, zhvi.columns)\n\n&lt;filter at 0x17fdb8040&gt;\n\n\nUgh, this doesn’t look like what we wanted!\nThis is what’s called an “iterable” in Python (something that is ready to be iterated over). In order to actually get the values, we need to explicitly pass this to a list() function.\n\nlist(filter(looks_like_a_date, zhvi.columns))\n\n['2000-01-31',\n '2000-02-29',\n '2000-03-31',\n '2000-04-30',\n '2000-05-31',\n '2000-06-30',\n '2000-07-31',\n '2000-08-31',\n '2000-09-30',\n '2000-10-31',\n '2000-11-30',\n '2000-12-31',\n '2001-01-31',\n '2001-02-28',\n '2001-03-31',\n '2001-04-30',\n '2001-05-31',\n '2001-06-30',\n '2001-07-31',\n '2001-08-31',\n '2001-09-30',\n '2001-10-31',\n '2001-11-30',\n '2001-12-31',\n '2002-01-31',\n '2002-02-28',\n '2002-03-31',\n '2002-04-30',\n '2002-05-31',\n '2002-06-30',\n '2002-07-31',\n '2002-08-31',\n '2002-09-30',\n '2002-10-31',\n '2002-11-30',\n '2002-12-31',\n '2003-01-31',\n '2003-02-28',\n '2003-03-31',\n '2003-04-30',\n '2003-05-31',\n '2003-06-30',\n '2003-07-31',\n '2003-08-31',\n '2003-09-30',\n '2003-10-31',\n '2003-11-30',\n '2003-12-31',\n '2004-01-31',\n '2004-02-29',\n '2004-03-31',\n '2004-04-30',\n '2004-05-31',\n '2004-06-30',\n '2004-07-31',\n '2004-08-31',\n '2004-09-30',\n '2004-10-31',\n '2004-11-30',\n '2004-12-31',\n '2005-01-31',\n '2005-02-28',\n '2005-03-31',\n '2005-04-30',\n '2005-05-31',\n '2005-06-30',\n '2005-07-31',\n '2005-08-31',\n '2005-09-30',\n '2005-10-31',\n '2005-11-30',\n '2005-12-31',\n '2006-01-31',\n '2006-02-28',\n '2006-03-31',\n '2006-04-30',\n '2006-05-31',\n '2006-06-30',\n '2006-07-31',\n '2006-08-31',\n '2006-09-30',\n '2006-10-31',\n '2006-11-30',\n '2006-12-31',\n '2007-01-31',\n '2007-02-28',\n '2007-03-31',\n '2007-04-30',\n '2007-05-31',\n '2007-06-30',\n '2007-07-31',\n '2007-08-31',\n '2007-09-30',\n '2007-10-31',\n '2007-11-30',\n '2007-12-31',\n '2008-01-31',\n '2008-02-29',\n '2008-03-31',\n '2008-04-30',\n '2008-05-31',\n '2008-06-30',\n '2008-07-31',\n '2008-08-31',\n '2008-09-30',\n '2008-10-31',\n '2008-11-30',\n '2008-12-31',\n '2009-01-31',\n '2009-02-28',\n '2009-03-31',\n '2009-04-30',\n '2009-05-31',\n '2009-06-30',\n '2009-07-31',\n '2009-08-31',\n '2009-09-30',\n '2009-10-31',\n '2009-11-30',\n '2009-12-31',\n '2010-01-31',\n '2010-02-28',\n '2010-03-31',\n '2010-04-30',\n '2010-05-31',\n '2010-06-30',\n '2010-07-31',\n '2010-08-31',\n '2010-09-30',\n '2010-10-31',\n '2010-11-30',\n '2010-12-31',\n '2011-01-31',\n '2011-02-28',\n '2011-03-31',\n '2011-04-30',\n '2011-05-31',\n '2011-06-30',\n '2011-07-31',\n '2011-08-31',\n '2011-09-30',\n '2011-10-31',\n '2011-11-30',\n '2011-12-31',\n '2012-01-31',\n '2012-02-29',\n '2012-03-31',\n '2012-04-30',\n '2012-05-31',\n '2012-06-30',\n '2012-07-31',\n '2012-08-31',\n '2012-09-30',\n '2012-10-31',\n '2012-11-30',\n '2012-12-31',\n '2013-01-31',\n '2013-02-28',\n '2013-03-31',\n '2013-04-30',\n '2013-05-31',\n '2013-06-30',\n '2013-07-31',\n '2013-08-31',\n '2013-09-30',\n '2013-10-31',\n '2013-11-30',\n '2013-12-31',\n '2014-01-31',\n '2014-02-28',\n '2014-03-31',\n '2014-04-30',\n '2014-05-31',\n '2014-06-30',\n '2014-07-31',\n '2014-08-31',\n '2014-09-30',\n '2014-10-31',\n '2014-11-30',\n '2014-12-31',\n '2015-01-31',\n '2015-02-28',\n '2015-03-31',\n '2015-04-30',\n '2015-05-31',\n '2015-06-30',\n '2015-07-31',\n '2015-08-31',\n '2015-09-30',\n '2015-10-31',\n '2015-11-30',\n '2015-12-31',\n '2016-01-31',\n '2016-02-29',\n '2016-03-31',\n '2016-04-30',\n '2016-05-31',\n '2016-06-30',\n '2016-07-31',\n '2016-08-31',\n '2016-09-30',\n '2016-10-31',\n '2016-11-30',\n '2016-12-31',\n '2017-01-31',\n '2017-02-28',\n '2017-03-31',\n '2017-04-30',\n '2017-05-31',\n '2017-06-30',\n '2017-07-31',\n '2017-08-31',\n '2017-09-30',\n '2017-10-31',\n '2017-11-30',\n '2017-12-31',\n '2018-01-31',\n '2018-02-28',\n '2018-03-31',\n '2018-04-30',\n '2018-05-31',\n '2018-06-30',\n '2018-07-31',\n '2018-08-31',\n '2018-09-30',\n '2018-10-31',\n '2018-11-30',\n '2018-12-31',\n '2019-01-31',\n '2019-02-28',\n '2019-03-31',\n '2019-04-30',\n '2019-05-31',\n '2019-06-30',\n '2019-07-31',\n '2019-08-31',\n '2019-09-30',\n '2019-10-31',\n '2019-11-30',\n '2019-12-31',\n '2020-01-31',\n '2020-02-29',\n '2020-03-31',\n '2020-04-30',\n '2020-05-31',\n '2020-06-30',\n '2020-07-31',\n '2020-08-31',\n '2020-09-30',\n '2020-10-31',\n '2020-11-30',\n '2020-12-31',\n '2021-01-31',\n '2021-02-28',\n '2021-03-31',\n '2021-04-30',\n '2021-05-31',\n '2021-06-30',\n '2021-07-31',\n '2021-08-31',\n '2021-09-30',\n '2021-10-31',\n '2021-11-30',\n '2021-12-31',\n '2022-01-31',\n '2022-02-28',\n '2022-03-31',\n '2022-04-30',\n '2022-05-31',\n '2022-06-30',\n '2022-07-31',\n '2022-08-31',\n '2022-09-30',\n '2022-10-31',\n '2022-11-30',\n '2022-12-31',\n '2023-01-31',\n '2023-02-28',\n '2023-03-31',\n '2023-04-30',\n '2023-05-31',\n '2023-06-30',\n '2023-07-31']\n\n\nHooray, it worked!\nNow, it’s time to melt our datasets:\n\nzhvi_tidy = zhvi.melt(\n    id_vars=[\"RegionName\", \"StateName\"],\n    value_vars=list(filter(looks_like_a_date, zhvi.columns)), # Notice I'm filtering zhvi columns here\n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n\nzori_tidy = zori.melt(\n    id_vars=[\"RegionName\", \"StateName\"],\n    value_vars=list(filter(looks_like_a_date, zori.columns)), # Notice I'm filtering zori columns here\n    var_name=\"Date\",\n    value_name=\"ZORI\",\n)\n\nand take a look:\n\nzori_tidy.head()\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\n\n\n\n\n0\nLos Angeles County\nCA\n2015-01-31\n1859.636263\n\n\n1\nCook County\nIL\n2015-01-31\n1509.519799\n\n\n2\nHarris County\nTX\n2015-01-31\n1244.972009\n\n\n3\nMaricopa County\nAZ\n2015-01-31\n1001.214534\n\n\n4\nSan Diego County\nCA\n2015-01-31\n1797.996849\n\n\n\n\n\n\n\n\nzhvi_tidy.head()\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZHVI\n\n\n\n\n0\nLos Angeles County\nCA\n2000-01-31\n205982.244116\n\n\n1\nCook County\nIL\n2000-01-31\n136354.916313\n\n\n2\nHarris County\nTX\n2000-01-31\n107050.239739\n\n\n3\nMaricopa County\nAZ\n2000-01-31\n146824.674809\n\n\n4\nSan Diego County\nCA\n2000-01-31\n217214.415906\n\n\n\n\n\n\n\n\n\nMerge the data frames\nAnother common operation is merging, also known as joining, two datasets.\nWe can use the merge() function to merge observations that have the same Date, RegionName, and StateName values.\n\n# Left dataframe is ZORI\n# Right dataframe is ZHVI\n\nzillow_data = pd.merge(\n    zori_tidy, zhvi_tidy, on=[\"Date\", \"RegionName\", \"StateName\"], how=\"outer\"\n)\n\n\n# Let's sort the data by Date\nzillow_data = zillow_data.sort_values(\n    by=\"Date\",         # Sort by Data column\n    ascending=True,    # Ascending order\n    ignore_index=True, # Reset the index to 0 to N\n)\n\nLet’s take a peak at the end of the dataframe using the .tail() function:\n\nzillow_data.head(n=20)\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\nZHVI\n\n\n\n\n0\nDooly County\nGA\n2000-01-31\nNaN\n43205.386681\n\n\n1\nMoniteau County\nMO\n2000-01-31\nNaN\nNaN\n\n\n2\nJefferson County\nGA\n2000-01-31\nNaN\n52304.968485\n\n\n3\nHenry County\nKY\n2000-01-31\nNaN\n83564.841863\n\n\n4\nPawnee County\nOK\n2000-01-31\nNaN\n37124.230106\n\n\n5\nClarke County\nMS\n2000-01-31\nNaN\nNaN\n\n\n6\nFloyd County\nIA\n2000-01-31\nNaN\n65476.850601\n\n\n7\nNottoway County\nVA\n2000-01-31\nNaN\nNaN\n\n\n8\nDewitt County\nIL\n2000-01-31\nNaN\nNaN\n\n\n9\nGrand County\nCO\n2000-01-31\nNaN\n176407.811425\n\n\n10\nSequatchie County\nTN\n2000-01-31\nNaN\n57679.387017\n\n\n11\nBarbour County\nWV\n2000-01-31\nNaN\n41933.827293\n\n\n12\nCarroll County\nIL\n2000-01-31\nNaN\nNaN\n\n\n13\nWashington County\nAL\n2000-01-31\nNaN\nNaN\n\n\n14\nFloyd County\nVA\n2000-01-31\nNaN\nNaN\n\n\n15\nMarquette County\nWI\n2000-01-31\nNaN\nNaN\n\n\n16\nVermillion County\nIN\n2000-01-31\nNaN\nNaN\n\n\n17\nLawrence County\nIL\n2000-01-31\nNaN\n14805.795158\n\n\n18\nBrown County\nIN\n2000-01-31\nNaN\nNaN\n\n\n19\nRedwood County\nMN\n2000-01-31\nNaN\nNaN\n\n\n\n\n\n\n\nMerging is very powerful and the merge can be done in a number of ways. In this case, we did a outer merge in order to keep all parts of each dataframe: even if a county doesn’t have both a ZORI and ZHVI value, the row will be in the merged dataframe with NaN to represent missing values.\nBy contrast, the inner merge only keeps the overlapping intersection of the merge: counties must have both a ZORI and ZHVI value for a given month for the row to make it into the merged dataframe.\nThe below infographic is helpful for figuring out the differences between merge types:\n\n\n\nTrim to the counties we want\nWhen we select the counties in the Philly metro region, we need to be careful because if we select just based on county name, we run the risk of selecting counties with the same name in other states. So let’s do our selection in two parts:\n\nTrim to counties in NJ or PA\nTrim based on county name\n\n\n# Get the rows with state == 'PA' or state == 'NJ'\nin_nj_pa = zillow_data[\"StateName\"].isin([\"PA\", \"NJ\"])\n\n# Save the trimmed dataframe\nzillow_trim_tmp = zillow_data.loc[in_nj_pa]\n\n\ncounty_names = [\n    \"Bucks County\",\n    \"Chester County\",\n    \"Delaware County\",\n    \"Montgomery County\",\n    \"Burlington County\",\n    \"Camden County\",\n    \"Gloucester County\",\n    \"Philadelphia County\",\n]\n\n# Trim based on county name\ncounty_sel = zillow_data['RegionName'].isin(county_names)\n\n# Trim temp dataframe from last step\nprices_philly_metro = zillow_trim_tmp.loc[county_sel].copy()\n\n\nprices_philly_metro\n\n\n\n\n\n\n\n\nRegionName\nStateName\nDate\nZORI\nZHVI\n\n\n\n\n1939\nCamden County\nNJ\n2000-01-31\nNaN\n104472.922223\n\n\n1946\nChester County\nPA\n2000-01-31\nNaN\n181113.160285\n\n\n1948\nDelaware County\nPA\n2000-01-31\nNaN\n113558.833735\n\n\n1960\nBucks County\nPA\n2000-01-31\nNaN\n166888.007583\n\n\n2008\nBurlington County\nNJ\n2000-01-31\nNaN\n141309.912108\n\n\n...\n...\n...\n...\n...\n...\n\n\n870781\nCamden County\nNJ\n2023-07-31\n1840.157047\n305607.084299\n\n\n870875\nGloucester County\nNJ\n2023-07-31\n2156.799027\n333625.235271\n\n\n870881\nBucks County\nPA\n2023-07-31\n2165.660642\n464753.874925\n\n\n870987\nMontgomery County\nPA\n2023-07-31\n2058.804292\n443222.467804\n\n\n871037\nPhiladelphia County\nPA\n2023-07-31\n1747.206167\n221021.754706\n\n\n\n\n2264 rows × 5 columns\n\n\n\nLet’s verify it worked…how many unique values in the “RegionName” column?\n\nprices_philly_metro['RegionName'].nunique()\n\n8\n\n\n\n\nGroup by: split-apply-combine\npandas is especially useful for grouping and aggregating data via the groupby() function.\nFrom the pandas documentation, groupby means: - Splitting the data into groups based on some criteria. - Applying a function to each group independently. - Combining the results into a data structure.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the documentation for more info.\n\n\nWe will take advantage of the pandas “group by” to calculate the percent increase in price values from March 2020 to July 2023 (the latest available data) for each county.\nTo do this, we’ll do the following:\n\nGroup by county name (the “RegionName” column)\nApply a function that does the following for each county’s data:\n\nSelect the March 2020 data\nSelect the July 2023 data\nCalculates the percent change for ZHVI and ZORI\n\n\nFirst, do the groupby operation:\n\ngrouped = prices_philly_metro.groupby(\"RegionName\")\n\ngrouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x17fdb8fa0&gt;\n\n\nThe groupby() function returns a DataFrameGroupBy object. This object knows what the groups are, but we still need to apply() a function to this object in order to get a DataFrame back!\nLet’s set up our function:\n\ndef calculate_percent_increase(group_df):\n    \"\"\"\n    Calculate the percent increase from 2020-03-31 to 2023-07-31.\n    \n    Note that `group_df` is the DataFrame for each group, in this\n    case, the data for each county.\n    \"\"\"\n    # Create selections for the march 2020 and july 2023 data\n    march_sel = group_df[\"Date\"] == \"2020-03-31\"\n    july_sel = group_df[\"Date\"] == \"2023-07-31\"\n    \n    # Get the data for each month (only 1 row, so squeeze it!)\n    march_2020 = group_df.loc[march_sel].squeeze()\n    july_2023 = group_df.loc[july_sel].squeeze()\n\n    # Columns to calculate percent change for\n    columns = [\"ZORI\", \"ZHVI\"]\n    \n    # Return the percent change for both columns\n    return 100 * (july_2023[columns] / march_2020[columns] - 1)\n\nNow, let’s apply this function to our group object:\n\nresult = grouped.apply(calculate_percent_increase)\n\nresult\n\n\n\n\n\n\n\n\nZORI\nZHVI\n\n\nRegionName\n\n\n\n\n\n\nBucks County\n28.943525\n33.028304\n\n\nBurlington County\n35.053677\n37.403941\n\n\nCamden County\n35.580024\n51.834765\n\n\nChester County\n27.913963\n39.692418\n\n\nDelaware County\n29.269362\n41.453732\n\n\nGloucester County\n36.979221\n44.089155\n\n\nMontgomery County\n27.045037\n39.111692\n\n\nPhiladelphia County\n15.020128\n20.048094\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the index (shown in bold in the notebook output) is the column that we grouped over. That is always the case when we use groupby() in pandas. The resulting DataFrame’s index is the groupby columns.\n\n\nSort by ZHVI and take a look:\n\nresult.sort_values(by=\"ZHVI\", ascending=True)\n\n\n\n\n\n\n\n\nZORI\nZHVI\n\n\nRegionName\n\n\n\n\n\n\nPhiladelphia County\n15.020128\n20.048094\n\n\nBucks County\n28.943525\n33.028304\n\n\nBurlington County\n35.053677\n37.403941\n\n\nMontgomery County\n27.045037\n39.111692\n\n\nChester County\n27.913963\n39.692418\n\n\nDelaware County\n29.269362\n41.453732\n\n\nGloucester County\n36.979221\n44.089155\n\n\nCamden County\n35.580024\n51.834765\n\n\n\n\n\n\n\nTakeaway\nPhiladelphia has had much lower price appreciation for home values and rent values since the pandemic started compared to its suburbs.\nThis is evidence in favor of the “donut effect”, with higher demand in the suburbs driving prices higher!\nNote: There are a number of built-in, convenience functions that we can use on the groupby object, such as .mean(), .sum(), etc.\nFor example, if we wanted to calculate the average ZHVI and ZORI values since March 2020 for every county, we could do the following:\n\n# First trim to dates from March 2020 onwards, then group by county name\ngroup_example = prices_philly_metro.query(\"Date &gt;= '2020-03-31'\").groupby(\"RegionName\")\n\n# Select the columns we want, and then use the built-in mean function\navg_prices = group_example[['ZHVI', 'ZORI']].mean()\n\navg_prices.sort_values(\"ZHVI\")\n\n\n\n\n\n\n\n\nZHVI\nZORI\n\n\nRegionName\n\n\n\n\n\n\nPhiladelphia County\n208756.603528\n1620.775839\n\n\nCamden County\n255742.478151\n1612.282080\n\n\nGloucester County\n283831.654654\n1868.459836\n\n\nDelaware County\n285825.916694\n1532.245821\n\n\nBurlington County\n315162.461555\n1924.282700\n\n\nMontgomery County\n382385.207204\n1831.782312\n\n\nBucks County\n413498.955059\n1907.576110\n\n\nChester County\n440770.335977\n1880.804478\n\n\n\n\n\n\n\nNote: Philadelphia also had the lowest average prices (in addition to the lowest growth in prices) since the pandemic started.\n\n\nBonus: a matplotlib preview\nThe week 2 lectures will dive into (probably too much) detail about matplotlib and data visualization in Python. For now, let’s do a quick and dirty plot for illustration purposes only. The matplotlib details here aren’t important\n\n# The import statement matplotlib\nfrom matplotlib import pyplot as plt\n\nWe’ll plot the ZORI/ZHVI values for each county from March 2020 through July 2023. One way to do this is to use the same groupby operation we used earlier.\nAs shown below, if you iterate over the groupby object, you will get two things back each time:\n\nThe value of the thing you grouped over\nThe dataframe holding the data for that group.\n\nWe will use this trick to group by “RegionName” and when we iterate over the groups, we can easily plot the data for each group on the same axes. It will look like this:\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Make our plots, each time using group_df\n    ...\nHere we go…plot the ZORI values.\nTo better see the growth since March 2020, we will normalize the y-axis to 1 in March 2020:\n\n# Create the figure and axes\nfig, ax = plt.subplots()\n\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Trim to where Date &gt; March 2020\n    # Sort by Date in ascending order\n    group_df = group_df.sort_values(\"Date\").query(\"Date &gt;= '2020-03-01'\")\n\n    # Date vs. ZORI\n    x = group_df[\"Date\"]\n    y = group_df[\"ZORI\"]\n    \n    # Trime\n    ax.plot(x, y / y.iloc[0], label=countyName)\n\n# Format and add a legend\nax.set_title(\"ZORI Values, Normalized to 1 in March 2020\")\nax.set_xticks([\"2020-03-31\", \"2023-07-31\"])\nax.legend(fontsize=10);\n\n\n\n\nPlot the ZHVI values, normalized to 1 in March 2020:\n\n# Create the figure and axes\nfig, ax = plt.subplots()\n\n# Iterate over the data, grouped by county name\nfor countyName, group_df in prices_philly_metro.groupby(\"RegionName\"):\n    \n    # Trim to where Date &gt; March 2020\n    # Sort by Date in ascending order\n    group_df = group_df.sort_values(\"Date\").query(\"Date &gt;= '2020-03-01'\")\n\n    # Date vs. ZHVI\n    x = group_df[\"Date\"]\n    y = group_df[\"ZHVI\"]\n    \n    # Trime\n    ax.plot(x, y / y.iloc[0], label=countyName)\n\n# Format and add a legend\nax.set_title(\"ZHVI Values, Normalized to 1 in March 2020\")\nax.set_xticks([\"2020-03-31\", \"2023-07-31\"])\nax.legend(fontsize=10);\n\n\n\n\nTakeaway: You can clearly see that price growth in Philadelphia significantly lags the growth in its suburb counties."
  },
  {
    "objectID": "content/week-1/lecture-1B.html#first-homework-assignment",
    "href": "content/week-1/lecture-1B.html#first-homework-assignment",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "First Homework Assignment",
    "text": "First Homework Assignment\nAvailable on GitHub:\nhttps://github.com/MUSA-550-Fall-2023/assignment-1\nTwo parts:\n\nDownload and install Python locally on your computer. Instructions in the assignment README!\nExplore the “Donut Effect” using Zillow ZHVI data by ZIP code in Philadelphia, and submit your Jupyter notebook.\n\n\n\n\n\n\n\nImportant\n\n\n\nDue date: Wednesday 9/20 by the end of the day (11:59 PM)"
  },
  {
    "objectID": "content/week-1/lecture-1B.html#thats-it",
    "href": "content/week-1/lecture-1B.html#thats-it",
    "title": "Week 1B: Exploratory Data Science in Python",
    "section": "That’s it!",
    "text": "That’s it!\n\nNext week: Data Visualization Fundamentals\nOffice hours:\n\nNick:\nTeresa:\nSign up for 15-minute time slots on Canvas (Zoom info in calendar invite)\n\nPost any questions on Ed Discussion!\nEmail questions/concerns to nhand@design.upenn.edu"
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Overview",
    "section": "",
    "text": "There are six homework assignments due throughout the semester and one final project that is due at the end of the finals period. For grading purposes, the assignment with the lowest grade will not count towards your final grade."
  },
  {
    "objectID": "assignment/index.html#schedule",
    "href": "assignment/index.html#schedule",
    "title": "Overview",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\nNote\n\n\n\nThe schedule is tentative and could change in the future."
  },
  {
    "objectID": "assignment/index.html#guidelines",
    "href": "assignment/index.html#guidelines",
    "title": "Overview",
    "section": "Guidelines",
    "text": "Guidelines\nWe will be using Jupyter notebook files for assignments. Below are a few guidelines to make this process as smooth as possible.\n1. Notebook files should be a polished, finished product.\n\nPlease be sure that your name (as well as the names of anyone you worked with) is listed at the top of the notebook file you submit.\nPlease remove any extra or unneccesary code.\nWhen possible, use markdown cells for comments or add comments directly to your Python code to help explain the steps in your analysis.\nSection headers in markdown can also be helpful to mark different sections of the analysis.\n\n2. Notebook files should be reproducible and executable.\n\nEnsure that your assignments can be executed from top to bottom and produce the desired result. This is how our course TAs will grade each assignment.\nIf your analysis loads a dataset, make sure you either A) Add the dataset to the repository, or B) document the original source of the data so it can be downloaded during grading.\nMake sure you are using relative file paths when loading your data so that the TA can download your repository and the file structure will still work. For more information, check out the guide on file paths."
  },
  {
    "objectID": "assignment/index.html#submission",
    "href": "assignment/index.html#submission",
    "title": "Overview",
    "section": "Submission",
    "text": "Submission\nWe’ll be using GitHub Classroom to submit homework assignments. For each assignment, be on the look out for the submission url, which will always start with the following: https://classroom.github.com/.\nWhen you click on this submission link for each assignment, you will be prompted to log in to GitHub, and then a unique, private repository will be created on the course’s GitHub page. You will create a new private repository for each assignment. Only you and the instructors will be able to view these repositories.\nWe will be using Jupyter notebook files for assignments, and you should add your notebook to your newly created assignment repository before the deadline. You can add files to the repository through the browser (github.com) interface or using the command line locally on your machine. For more help, see these instructions."
  },
  {
    "objectID": "assignment/final-project.html",
    "href": "assignment/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is to replicate the pipeline approach on a dataset (or datasets) of your choosing.\nThe final deliverable will be a web-based data visualization and accompanying description including a summary of the results and the methods used in each step of the process (collection, analysis and visualization)."
  },
  {
    "objectID": "assignment/final-project.html#due-dates",
    "href": "assignment/final-project.html#due-dates",
    "title": "Final Project",
    "section": "Due dates",
    "text": "Due dates\nNote: Due dates are tentative and subject to change\nWritten proposal due date: Monday, December 4th (or earlier)\nProject due date: Wednesday, December 20th at 11:59 PM\nFinal projects must receive prior approval in the form of a written proposal."
  },
  {
    "objectID": "assignment/assignment-1.html",
    "href": "assignment/assignment-1.html",
    "title": "HW #1",
    "section": "",
    "text": "go\nThis week’s assignment will be broken into two parts:"
  },
  {
    "objectID": "assignment/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "href": "assignment/assignment-1.html#part-1-installing-python-locally-and-launching-a-jupyter-notebook",
    "title": "HW #1",
    "section": "Part 1: Installing Python locally and launching a Jupyter notebook",
    "text": "Part 1: Installing Python locally and launching a Jupyter notebook\nFollow the initial installation guide on the course website for instructions on how to set up Python locally and launch JupyterLab\nPlease see the list of recommended readings for tutorials and background reading to get familiar with Python, mamba/conda, and the Jupyter notebook.\nProblems? Post your question to Ed Discussion!\n\nWorking Locally\nIf you’ve successfully followed the installation guide, you should be able to launch JupyterLab by running jupyter lab (after activating the course environment) from the command line. This should launch the JupyterLab dashboard. Now you can launch the assignment notebook and get started on the homework!\nThe notebook will execute code from the current working directory (the folder that the notebook was launched from). This folder is usually your home folder on your laptop. If you are using relative file paths to load the data, the path should be relative to this working directory. From within the Jupyter notebook, you can find out the current working directory by running the following command in a cell:\npwd\nIf you’ve downloaded the assignment-1 repository to your computer, it usually makes sense to launch the Jupyter notebook from this folder instead of the default folder. You can change the start-up folder by first navigating to your assignment folder in the command line: see instructions here."
  },
  {
    "objectID": "assignment/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "href": "assignment/assignment-1.html#part-2-exploring-the-donut-effect-for-philadelphia-zip-codes",
    "title": "HW #1",
    "section": "Part 2: Exploring the Donut Effect for Philadelphia ZIP codes",
    "text": "Part 2: Exploring the Donut Effect for Philadelphia ZIP codes\nIn part #2, you will explore the Donut Effect for home values in different parts of Philadelphia. This part will be submitted as a Jupyter notebook (a .ipynb file). There is already a starter notebook in this repository to help guide you through this part.\n\nSubmission\nNote: Be sure to read the documentation for assignment guidelines and submission.\nYou will submit your assignment through GitHub. For each assignment, I will provide a GitHub link that can be used to create a new repository. Each student will have their own private repository on GitHub where the assignment can be submitted. Only the student and instructors will have access to the private repository.\nThe invitation link for this week is:\nhttps://classroom.github.com/a/MkVE9Lb0\nIf you do not have a GitHub account yet, you should be prompted to make an account. After clicking on this link, GitHub will create a new private repo with permissions such that only you and the instructors can view the commits.\nYour assignment should be added to this newly crearted GitHub repository before the deadline. You can add files to the repository through the web (github.com) interface or using the command line on your laptop.\nImportant: Files should be committed to the newly created private repository (after following the above link) and not to your forked version of the assignment-1 repository.\nYour Jupyter notebook should be submitted to your private repository by the deadline, 11:59 PM on 9/20."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to the course syllabus for MUSA 550, Geospatial Data Science in Python, taught at the University of Pennsylvania in fall 2023."
  },
  {
    "objectID": "syllabus.html#overview",
    "href": "syllabus.html#overview",
    "title": "Syllabus",
    "section": "Overview",
    "text": "Overview\nThis course will provide students with the knowledge and tools to turn data into meaningful insights, with a focus on real-world case studies in the urban planning and public policy realm. Focusing on the latest Python software tools, the course will outline the “pipeline” approach to data science. It will teach students the tools to gather, visualize, and analyze datasets, providing the skills to effectively explore large datasets and transform results into understandable and compelling narratives. The course is organized into five main sections:\n\nExploratory Data Science: Students will be introduced to the main tools needed to get started analyzing and visualizing data using Python.\nIntroduction to Geospatial Data Science: Building on the previous set of tools, this module will teach students how to work with geospatial datasets using a range of modern Python toolkits.\nData Ingestion & Big Data: Students will learn how to collect new data through web scraping and APIs, as well as how to work effectively with the large datasets often encountered in real-world applications.\nFrom Exploration to Storytelling: With a solid foundation, students will learn the latest tools to present their analysis results using web-based formats to transform their insights into interactive stories.\nGeospatial Data Science in the Wild: Armed with the necessary data science tools, the final module introduces a range of advanced analytic and machine learning techniques using a number of innovative examples from modern researchers."
  },
  {
    "objectID": "syllabus.html#logistics",
    "href": "syllabus.html#logistics",
    "title": "Syllabus",
    "section": "Logistics",
    "text": "Logistics\nThere are two sections for this course, 401 and 402. Info for these sections is below.\n\nLecture\nThe course will be conducted in weekly sessions devoted to lectures, interactive demonstrations, and in-class labs.\n\nSection 401\n\nMonday & Wednesday, 8:30 AM to 10:00 AM\nWilliams Hall, Room 202\n\n\n\nSection 402\n\nThursday, 1:45 PM to 4:45 PM\nMeyerson Hall, Room B2\n\n\n\n\nContact Info\n\nSection 401\n\nInstructor: Nick Hand, nhand@design.upenn.edu\nTeaching Assistant: Teresa Chang, thchang@design.upenn.edu\n\n\n\nSection 402\n\nInstructor: Eric Delmelle, ericdel@design.upenn.edu\nTeaching Assistant: Jinze Wang, wangjz@seas.upenn.edu\n\n\n\n\nOffice Hours\nOffice hours will be by appointment via Zoom — you should be able to sign up for 1 (or more) 15-minute time slot via the Canvas calendar.\n\nSection 401\nNick:\nTBD\nTeresa:\nTBD\n\n\nSection 402\nEric:\nTBD\nJinze:\nTBD\n\n\n\nCourse Websites\n\nMain website: https://musa-550-fall-2023.github.io\nGitHub: https://www.github.com/MUSA-550-Fall-2023\nCanvas: https://canvas.upenn.edu/courses/1740535\nEd Discussion https://edstem.org/us/courses/42616/discussion/\n\nThe course’s main website will be the main source of information, including the course schedule, weekly content, and guides/resources.\nThe course’s GitHub page will have repositories for each week’s lectures as well as assignments. Students will also submit their assignments through GitHub.\nWe will use Canvas signing up for office hours and tracking grades.\nEd Discussion is a Q&A forum that allows students to ask questions related to lecture materials and assignments.\n\n\nAssignments\nThere are six homework assignments and one required final project at the end of the semester. While you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nFor the final project, students will replicate the pipeline approach on a dataset (or datasets) of their choosing. Students will be required to use several of the analysis techniques taught in the class and produce a web-based data visualization that effectively communicates the empirical results to a non-technical audience. The final product should also include a description of the methods used in each step of the data science process (collection, analysis, and visualization).\nFor more details on the final project, see the GitHub repository.\n\n\nGrading\nThe grading breakdown is as follows: 55% for homework; 45% for final project, 5% for participation. Your participation grade will be determined by your activity on Ed Discussion — both asking, answering, and reading questions.\nWhile you are required to submit all six assignments, the assignment with the lowest grade will not count towards your final grade.\nThere’s no penalty for late assignments. I would highly recommend staying caught up on lectures and assignments as much as possible, but if you need to turn something in a few days late, that’s fine — there’s no penalty. If you turn in something late, you’ll be missing out on valuable feedback, but that’s the only practical penalty, there’s no extra penalty to your grade.\n\n\nSoftware\nThis course relies on use of Python and various related packages and for geospatial topics. All software is open-source and freely available. The course will require a working installation of Python on your local computer. See the Installation Setup Guide for instructions on how to setup your computer for use in this course."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Syllabus",
    "section": "Policies",
    "text": "Policies\nMUSA 550 is a fast-paced course that covers a lot of topics in a short amount of time. I know that it can be overwhelming and frustrating, particularly as you are trying to learn Python syntax and the topics in the course at the same time. But I firmly believe that all students can succeed in this class.\nYou’ll get the most out of the course if you stay up to date on the lectures and assignments. If you fall behind, I know there can be a desire to copy code from the Internet or others to help you complete assignments. Ultimately, this will be detrimental to your progress as an analytics wizard. My goal for this course is for everyone to learn and engage with the material to the best of their ability.\n\nIf you find yourself falling behind or struggling with Python issues, please ask for help by:\n\nPost a question on Canvas — the fix for your problem might be quick and other students are probably experiencing similar issues.\nCome to office hours and discuss issues or larger conceptual questions you are having.\nTake advantage of the free resources to help fine-tune your Python skills.\n\n\nAnd if you are still struggling, reach out and let me know and we’ll figure out a strategy to make things work!\n\nCommunication Policies\n\nPlease add the following text into the subject line of emails to us: [MUSA550]. This will help us make sure we don’t miss your email!\nWe will use the Ed Discussion Q&A forum for questions related to lecture material and assignments.\nTo prevent code copying, please do not post long, complete code examples to Ed Discussion.\nAnonymous posting is enabled on Ed Discussion — if you have a question that requires a full code example, please use the anonymous feature to post the question.\nWe will also use Ed Discussion for announcements — please make sure your notifications are turned on and you check the website frequently. This will be the primary method of communication for course-wide announcements.\nIf you have larger-scale or conceptual questions on assignments or lecture material, please set up a time to discuss during office hours.\n\n\n\nGroup Work\nStudents are allowed (and encouraged!) to collaborate when working through lecture materials or assignments. If you work closely with other students, please list the members of your group at the top of your assignment.\n\n\nSpecial Accommodations\nThere are a number of ongoing situations in the world that may take precedence over the course work. If you are experiencing any difficulties outside the course, please contact me and accommodations can be made. Similarly, if you are having any difficulties with the course schedule, attending lectures, or similar, please let us know.\n\n\nAcademic Integrity\nStudents are expected to be familiar with and comply with Penn’s Code of Academic Integrity, which is available in the Pennbook, or online at https://catalog.upenn.edu/pennbook/code-of-academic-integrity."
  },
  {
    "objectID": "resource/mamba.html",
    "href": "resource/mamba.html",
    "title": "Using mamba",
    "section": "",
    "text": "In this guide, we’ll outline some of the key concepts and common uses to get you up and running with mamba. A conda cheatsheet is also available under the “Cheatsheets” section in the left sidebar."
  },
  {
    "objectID": "resource/mamba.html#key-concepts",
    "href": "resource/mamba.html#key-concepts",
    "title": "Using mamba",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nCommands\nWe’ll be using the mamba tool through its command-line interface rather than a typical graphical user interface (GUI) application. If you are unfamiliar with the command line, this will take some getting used to but once you get the hang of it, it will make working with mamba and Python much easier.\nThe mamba command is the main interface for using the mamba tool for managing your Python packages. From the command line, you simply run:\nconda command [optional arguments will go here]\nwhere “command” is the name of the command you want to run. Commands exist to install new packages, create new environments, and much more.\n\n\nStarting and running mamba\nWe will run mamba from the command line but the specifics of this will depend on your operating system.\n\nWindows\nOpen the Start menu, search for and open the “Miniforge Prompt”. This application provides a command line interface where the mamba tool is properly load, initialized, and ready to be used. Note that you cannot use the default “Command Prompt” application to use mamba because it doesn’t know how to load mamba properly.\nMacOS\nThe Terminal app should be used on MacOS to use mamba. You can also use any Terminal emulator (such as iTerm2). Simply open the Terminal application and the mamba command should be ready to use.\n\n\n\nChannels\n“Channels” are the locations where packages are located. Channels are typically remote and hosted in the cloud. When you specify a channel, mamba will search the remote database for the right package and download it to your local computer.\nBy default, conda usually downloads packages from the defaults channel, which hosts thousands of packages and is managed by the makers of the Anaconda distribution. A full list of packages is available here.\nThe “mambaforge” distribution that we installed in this course is pre-configured to use a community-managed channel known as “conda-forge” instead of the “defaults” channel. Conda forge includes many of the packages on the “defaults” channel but also popular packages that are widely-used but not quite essential enough for the “defaults” channel. A list of maintained packages is available here.\nFor less well known packages, there is a higher likelihood the package will be hosted on conda forge. For that reason, we will prefer downloading and installing packages from conda forge in this course.\n\n\nEnvironments\nThe mamba tool not only lets you download and install packages, but you can group those packages together into environments. By default, the “mambaforge” Python distribution creates an environment named base. We will create a new environment specifically for this course that will hold all of the packages needed for the entire semester.\nEnvironments become particularly useful when working with lots of packages, packages that have a lot of dependencies, or packages that are difficult to install. When environments become too large, it can be difficult to install a new package that satisfies all of the existing package dependencies. For that reason, we will create a fresh, new environment to install the packages we need to use during this course.\n\n\nConda/mamba vs. pip\nThe other widely used method for installing packages is via the pip command. The commands are similar in a lot of ways but with some key differences. The pip command installs packages from the Python Package Index and is designed to install Python-only packages.\nThe main advantage of conda/mamba is that it is cross-platform and can handle dependencies that are written in C (or other languages) and will automatically handle the compiling process during installation. Many of the packages we use in this course have complex dependencies written in C, and mamba will make installation of these packages much easier.\nIn this course, we’ll be using mamba to install packages. Generally speaking, if you already are using mamba to manage environments, it’s best to try to install packages with mamba and if the package is not available, then try using pip.\nSee this article for more information about conda and pip."
  },
  {
    "objectID": "resource/mamba.html#common-uses",
    "href": "resource/mamba.html#common-uses",
    "title": "Using mamba",
    "section": "Common Uses",
    "text": "Common Uses\nManaging environments and installing packages will be done by executing the mamba command in the command line. Below are some of the most common commands that we will use in this class.\n\n\n\n\n\n\nImportant\n\n\n\nAll of the examples below should be run in the Terminal app (MacOS) or Miniforge Prompt (Windows). See the Starting and running mamba section above for more detail.\n\n\n\nGetting help with the mamba command\nThe mamba command has a built-in help function. From the command line, run,\nmamba --help\nwhich will print out info about individual commands:\nusage: mamba [-h] [-V] command ...\n\nconda is a tool for managing and deploying applications, environments and packages.\n\nOptions:\n\npositional arguments:\n  command\n    clean             Remove unused packages and caches.\n    compare           Compare packages between conda environments.\n    config            Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .condarc file\n                      (/Users/nhand/.condarc) by default. Use the --show-sources flag to display all identified configuration locations on your computer.\n    create            Create a new conda environment from a list of specified packages.\n    info              Display information about current conda install.\n    init              Initialize conda for shell interaction.\n    install           Installs a list of packages into a specified conda environment.\n    list              List installed packages in a conda environment.\n    package           Low-level conda package utility. (EXPERIMENTAL)\n    remove (uninstall)\n                      Remove a list of packages from a specified conda environment. Use `--all` flag to remove all packages and the environment itself.\n    rename            Renames an existing environment.\n    run               Run an executable in a conda environment.\n    search            Search for packages and display associated information.The input is a MatchSpec, a query language for conda packages. See examples\n                      below.\n    update (upgrade)  Updates conda packages to the latest compatible version.\n    notices           Retrieves latest channel notifications.\n    repoquery         Query repositories using mamba.\n\noptional arguments:\n  -h, --help          Show this help message and exit.\n  -V, --version       Show the conda version number and exit.\nTo find out more info about a specific sub-command, you can run:\nmamba command --help\nFor example, for more info about the arguments (both required and optional) needed to install packages, use: mamba install --help.\n\n\nListing the available environments\nThe default environment when first installing mamba is called 'base'. You can list the currently installed Python environments by running the following command from the command line:\nmamba env list\nThe currently active environment will have a '*' next to it. You should see the 'base' environment as well as any other environments you have created.\n\n\nActivating your environment\nEnvironments must first be “activated” before the packages are available to use. To activate the environment for this course, you can run the following from the command line:\nmamba activate musa-550-fall-2023\nNow, all of the packages in this environment will be available when we run Python.\n\n\nFinding the active environment\nTo see the active environment, list the available environments. The active environment will be listed with a ‘*’ next to its name.\nFrom the command line, run:\nmamba env list\n\n\nListing the installed packages\nIf you have already activated the musa-550-fall-2023 environment, you can list all of the installed packages.\nFrom the command line:\nmamba list\n\n\nActivating the base environment\nTo activate the 'base' default environment, run from the command line:\nmamba activate base\n\n\n\n\n\n\nNote\n\n\n\nYou should always use the ‘musa-550-fall-2023’ environment to do the analysis in this course. Make sure it is the activated environment when using Python.\n\n\n\n\nDeleting an environment\nNote that you cannot create a new environment with the same name as an existing environment. If your environment becomes corrupted or you run into issues, it is often easiest to delete the environment and start over. To do, you can run the following commands from the command line:\nmamba deactivate\nmamba env remove --name musa-550-fall-2023\n\n\nUpdating an existing environment\nThe environment we are using throughout the course might be need to be updated during the course. For example, we might want to update to include a newly released version of a package.\nYou can update your local environment via the following command. From the command line:\nmamba env update pennmusa/musa-550-fall-2023\nThis command will ensure that the ‘musa-550-fall-2023’ environment on your local computer matches the environment specified by the “environment.yml” file stored in the cloud for the course.\n\n\nInstalling specific packages\nYou shouldn’t need to install any individual packages into the ‘musa-550-fall-2023’ environment. But for reference, you could install specific packages into the active environment using from the command line:\nmamba install package_name"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Helpful resources",
    "section": "",
    "text": "This section includes a number of extra resources, cheatsheets, and guides related to software installation, Python, and other relevant topics."
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing Python and inital set-up",
    "section": "",
    "text": "MUSA 550 relies on freely available software from the Python  open-source ecosystem. This guide will walk you through how to set up your computer for the course, including downloading and installing Python as well as the various packages you will need throughout the semester.\nBy the end of this guide, you’ll have Python installed, be able to launch a Jupyter notebook (the interface for running Python code), and will be writing your first Python code. Let’s get started!"
  },
  {
    "objectID": "resource/install.html#step-1.-installing-python",
    "href": "resource/install.html#step-1.-installing-python",
    "title": "Installing Python and inital set-up",
    "section": "Step 1. Installing Python",
    "text": "Step 1. Installing Python\nThere are a number of different tools and ways to install Python for new users. In this course, we’ll be using a package manager called mamba to install Python and manage dependencies. In the Python ecosystem, package managers are especially useful, as they greatly simplify the installation process and ensure all of your dependencies will function properly.\nmamba is a drop-in replacement (meaning it has the same functionality) for a very popular Python package manager called conda. We’ll use mamba instead of conda because it has signficiantly better performance. In my experience, it’s not uncommon for it to sometimes take more than hour to install a list of Python packages with conda but only a few seconds for mamba to do the same.\n\n\n\n\n\n\nTip\n\n\n\nAnything you can do with the conda tool, you can do with the mamba tool. Outside this class, you’ll likely hear about conda more often, since it’s the more popular tool at the moment.\nAs you will see later, we will often refer to the conda documentation to learn about the key concepts behind conda/mamba and its main functionality.\n\n\nThe mamba tool allows you to easily install Python packages on your laptop using environments. An environment allows you to install packages for specific purposes and keep those packages isolated from any other Python packages that might be installed on your laptop. This is very useful, since different versions of packages often don’t work nicely together. We will create an environment for use during this class that includes all of the Python packages you will need in the course.\nIn this course, we will use the “mambaforge” distribution of Python, which includes Python, mamba, and a few other essential packages and dependencies. It also comes pre-configured with “conda-forge”, a popular, community-maintained server that makes the most popular Python packages available for download for free.\n\n\n\n\n\n\nNote\n\n\n\nThe mambaforge distribution is the mamba version of the popular “Miniconda” distribution, which is a free, minimal Python installation that just includes conda. Miniconda is a lightweight version of the full Anaconda distribution. The differences between the “mini” and “full” versions are outlined here.\nThe major difference is that the Anaconda distribution will install more than 1,500 of the most common scientific Python packages (many more than we need in this course) and will take up about 3 GB of disk space. Mambaforge/Miniconda will only install core Python dependencies (as well as mamba/conda) and will only take up a much smaller amount of disk space.\n\n\nThe following page contains the installation files for the mambaforge distribution:\nhttps://github.com/conda-forge/miniforge#mambaforge\nSelect the appropriate file for your computer’s operating system and click to download the file. The file should be named something like Mambaforge-Linux-*, Mambaforge-MacOSX-*, or Mambaforge-Windows-*.\nThe rest of the installation instructions will vary based on your operating system:\n\nWindows\n\nIn the file browser, double-click the .exe file that you downloaded.\nFollow the instructions on the screen. If you are unsure about any setting, accept the defaults. You can change them later.\n\nMacOS\n\nOpen the Terminal application.\nChange to the folder to the directory where the .sh installer file was downloaded (this is usually the “Downloads” folder) by running the following command in the Terminal app:\nbash Mambaforge-MacOSX-x86_64.sh\nor if you have a Mac with the new M2 chips, use:\nbash Mambaforge-MacOSX-arm64.sh\nFollow the instructions on the screen. If you are asked about if you “wish the installer to initialize Mambaforge by running conda init?”, type “yes”. This will ensure that mamba is an available command when you open up Terminal.\nIf you are unsure about any setting, accept the defaults. You can change them later.\nTo make the changes take effect, close and then re-open your terminal window."
  },
  {
    "objectID": "resource/install.html#verify-that-your-python-installation-is-working",
    "href": "resource/install.html#verify-that-your-python-installation-is-working",
    "title": "Installing Python and inital set-up",
    "section": "2. Verify that your Python installation is working",
    "text": "2. Verify that your Python installation is working\nTo verify that the install worked, we will run mamba from the command line. The specifics of this will depend on your operating system:\n\nWindows\nOpen the Start menu, search for and open the “Miniforge Prompt”. This application provides a command line interface where the mamba tool is properly loaded, initialized, and ready to be used.\nNote that you cannot use the default “Command Prompt” application to use mamba because it doesn’t know how to load mamba properly.\nMacOS\nThe Terminal app should be used on MacOS to use mamba. You can also use any Terminal emulator (such as iTerm2). Simply open the Terminal application and the mamb command should be ready to use.\n\nNow, let’s test your installation. From the command line, run: mamba list. A list of installed packages should be printed to the screen if the installation was successful.\nAfter you’ve sucessfully installed the mambaforge distribution, you will have Python 3.10 installed with a default environment called “base”."
  },
  {
    "objectID": "resource/install.html#create-your-first-python-environment",
    "href": "resource/install.html#create-your-first-python-environment",
    "title": "Installing Python and inital set-up",
    "section": "3. Create your first Python environment",
    "text": "3. Create your first Python environment\nThe mamba/conda tool allows us to easily install new Python packages and keep track of which ones we’ve already installed. I’ve put together a list of the packages we’ll need in this course (a group of packages is known as an environment in mamba-speak). Note that you’ll be using the command line (either the Miniforge Prompt in Windows or Terminal app in MacOS) to run mamba and create your environment.\nThroughout this course, we will maintain an environment called “musa-550-fall-2023” to install and manage all of the packages needed throughout the semester.\nThe packages in an environment are specified in a file typically called “environment.yml”. The environment file for this course is stored in the course-materials repository on Github and a copy is also stored in the cloud on anaconda.org.\nIt is recommended to create the ‘musa-550-fall-2023’ environment on your local computer using the environment file stored in the cloud on anaconda.org. The instructions to do so are as follows:\n\nFirst, we need to make sure the anaconda-client package is installed locally. This will ensure that mamba can interface with anaconda.org. From the command line (Miniforge Prompt on Windows or Terminal on MacOS), run:\nmamba install anaconda-client -n base\nThis will install the anaconda-client package into the default “base” environment.\nThen, create the musa-550-fall-2023 environment by running:\nmamba env create pennmusa/musa-550-fall-2023\nAfter this command finishes, all of the packages we need for the course should be installed. To verify this, you can run mamba env list from the command line to see the installed environments. If everything worked, you should now see the 'musa-550-fall-2023' environment listed.\n\n\n\n\n\n\n\nNote\n\n\n\nThis semester, we will be using Python version 3.10. More information about the different versions of Python is available here."
  },
  {
    "objectID": "resource/install.html#activate-the-courses-environment",
    "href": "resource/install.html#activate-the-courses-environment",
    "title": "Installing Python and inital set-up",
    "section": "4. Activate the course’s environment",
    "text": "4. Activate the course’s environment\nOnce you’ve created your new environment and installed the Python packages for the course, you need to tell mamba to activate it (more mamba-speak) so that you can actually use the packages when you are writing Python code.\nTo activate the environment for this course, you can run the following from the command line (Miniforge Prompt on Windows or Terminal on MacOS):\nmamba activate musa-550-fall-2023\nNow, all of the packages in this environment will be available when we run Python.\n\n\n\n\n\n\nImportant\n\n\n\nIf you forget to activate the course’s environment, you will be using the default “base” environment. This has some of the packages we will need, but many will be missing. If you are trying to import Python packages and get a “ModuleNotFoundError” error, the active environoment is likely the issue!"
  },
  {
    "objectID": "resource/install.html#launching-a-jupyter-notebook",
    "href": "resource/install.html#launching-a-jupyter-notebook",
    "title": "Installing Python and inital set-up",
    "section": "5. Launching a Jupyter notebook",
    "text": "5. Launching a Jupyter notebook\nThroughout the course, we will write, edit, and execute Python code in files called Jupyter notebooks. These files have an .ipynb extension. Notebooks are documents that combine live runnable code with narrative text, images, and interactive visualizations.\nAn application called JupyterLab is the recommended way to work with these notebook files. JupyterLab is a browser-based interface that allows users to execute Python in notebook files. It can also handle all sorts of additional file formats and even has a built-in command-line feature. The JupyterLab guide provides much more information about the features of JupyterLab — for the moment, we will just focus on launching our first notebook file.\n\n\n\n\n\n\nNote\n\n\n\nThere are other interfaces for working with Jupyter notebooks. The original Jupyter notebook application has since been replaced by the more powerful JupyterLab application. Popular code editors such as VS Code have also added support for Jupyter notebooks, although it is important to note that VS Code does not support all features of notebooks. For this reason, JupyterLab remains the recommended notebook interface.\n\n\nThe recommended approach for starting a notebook is to use the Miniforge Prompt on Windows or the Terminal app on MacOS. To do so, we simply need to activate our 'musa-550-fall-2023' environment and then start JupyterLab, which is included in the course’s environment.\nFrom the command line, first activate the environment:\nmamba activate musa-550-fall-2023\nand then launch JupyterLab\njupyter lab\nThis will create the local Jupyter server and should launch the JupyterLab dashboard in a browser. If it does not open in a browser, copy the link that is output by the command into your favorite browser. Typically, the server will be running at http://localhost:8888. The dashboard should like look something like this:\n\nOn the left, you’ll see a file browser for the files in the folder where you ran the jupyter lab command. On the right you will see the “Launcher”, which allows you to easily create various types of new files. Click on the “Python 3” button under the “Notebook” section and you’ll create your first notebook. Alternatively, you can use the File -&gt; New -&gt; Notebook option from the menu bar. The new notebook, entitled “Untitled.ipynb”, is created within the same directory.\nNow, let’s type the following into the first cell:\nprint(\"Hello, World!\")\nClick the ⏵ button in the menu bar of the notebook and you will run your first Python code in a notebook!\n\n\n\n\n\n\n\nTip\n\n\n\nThe Jupyter notebook section of the JupyterLab walks you through each step of working with notebook files in JupyterLab. Check it out here, or find more information in the JupyterLab guide."
  }
]